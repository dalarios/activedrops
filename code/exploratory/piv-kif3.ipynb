{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File management\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import norm\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "\n",
    "# Utilities\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, cpu_count\n",
    "mp.set_start_method('fork', force=True)\n",
    "from ipywidgets import interact, FloatSlider, Layout, interactive\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import cv2\n",
    "from natsort import natsorted\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions: ['kif3']\n",
      "Subconditions: ['Rep1', 'Rep2', 'Rep3']\n"
     ]
    }
   ],
   "source": [
    "def prepare_conditions(data_path, num_reps):\n",
    "    # List conditions while ignoring 'output_data'\n",
    "    conditions = natsorted([\n",
    "        f for f in os.listdir(data_path) \n",
    "        if os.path.isdir(os.path.join(data_path, f)) and f != 'output_data'\n",
    "    ])\n",
    "    \n",
    "    # Generate subconditions list based on num_reps\n",
    "    subconditions = [f\"Rep{x}\" for x in range(1, num_reps + 1)]\n",
    "    \n",
    "    return conditions, subconditions\n",
    "\n",
    "# Example usage\n",
    "calibration_curve_paths = sorted(glob.glob(\"../../../../Thomson Lab Dropbox/SURF_activedrops/Edgar/calibration_curve/***ugml.tif\"))\n",
    "\n",
    "data_path = \"../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/\"\n",
    "conditions, subconditions = prepare_conditions(data_path, 3)\n",
    "\n",
    "print(\"Conditions:\", conditions)\n",
    "print(\"Subconditions:\", subconditions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorgTiffsToOriginal(data_path, conditions, subconditions):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions (list): List of conditions.\n",
    "        subconditions (list): List of subconditions.\n",
    "        \n",
    "    This function renames the subconditions as PosX and moves the raw data to the \"original\" folder.\n",
    "    \"\"\"\n",
    "    for condition in conditions:\n",
    "        # Get the actual subconditions in the directory\n",
    "        actual_subconditions = [name for name in os.listdir(os.path.join(data_path, condition)) if os.path.isdir(os.path.join(data_path, condition, name))]\n",
    "        \n",
    "        # Rename the actual subconditions to match the subconditions in your list\n",
    "        for i, actual_subcondition in enumerate(sorted(actual_subconditions)):\n",
    "            os.rename(os.path.join(data_path, condition, actual_subcondition), os.path.join(data_path, condition, subconditions[i]))\n",
    "        \n",
    "        for subcondition in subconditions:\n",
    "            # Construct the path to the subcondition directory\n",
    "            subcondition_path = os.path.join(data_path, condition, subcondition)\n",
    "            \n",
    "            # Create the path for the \"original\" directory within the subcondition directory\n",
    "            original_dir_path = os.path.join(subcondition_path, \"original\")\n",
    "            \n",
    "            # Always create the \"original\" directory\n",
    "            os.makedirs(original_dir_path, exist_ok=True)\n",
    "            \n",
    "            # Iterate over all files in the subcondition directory\n",
    "            for filename in os.listdir(subcondition_path):\n",
    "                # Check if the file is a .tif file\n",
    "                if filename.endswith(\".tif\"):\n",
    "                    # Construct the full path to the file\n",
    "                    file_path = os.path.join(subcondition_path, filename)\n",
    "                    \n",
    "                    # Construct the path to move the file to\n",
    "                    destination_path = os.path.join(original_dir_path, filename)\n",
    "                    \n",
    "                    # Move the file to the \"original\" directory\n",
    "                    shutil.move(file_path, destination_path)\n",
    "            print(f\"Moved .tif files from {subcondition_path} to {original_dir_path}\")\n",
    "\n",
    "\n",
    "def ensure_output_dir(output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "def calculate_mean_intensity(path):\n",
    "    \"\"\"Calculate mean intensity of an image.\"\"\"\n",
    "    return io.imread(path).mean()\n",
    "\n",
    "\n",
    "def calculate_protein_concentration(mean_intensity, intercept, slope):\n",
    "    \"\"\"Calculate protein concentration in ng/ul and nM.\"\"\"\n",
    "    conc_ng_ul = (mean_intensity - intercept) / slope\n",
    "    return conc_ng_ul\n",
    "\n",
    "\n",
    "def calculate_protein_concentration_nM(conc_ng_ul, mw_kda):\n",
    "    \"\"\"Convert protein concentration from ng/ul to nM.\"\"\"\n",
    "    conc_nM = (conc_ng_ul * 1e-3) / (mw_kda * 1e3) * 1e9\n",
    "    return conc_nM\n",
    "\n",
    "\n",
    "def calculate_number_of_protein_molecules(protein_mass, mw_kda):\n",
    "    \"\"\"Calculate number of protein molecules.\"\"\"\n",
    "    return (protein_mass * 6e14) / (mw_kda * 1e3)\n",
    "\n",
    "\n",
    "def convert_time_units(time_values_s):\n",
    "    \"\"\"Convert time values from seconds to minutes and hours.\"\"\"\n",
    "    time_values_min = time_values_s / 60\n",
    "    time_values_h = time_values_s / 3600\n",
    "    return time_values_s, time_values_min, time_values_h\n",
    "\n",
    "\n",
    "def process_image(args):\n",
    "    image_file, output_directory_path, channel, slope, intercept, vmax, time_interval, i, show_scalebar, min_frame, skip_frames, condition, subcondition = args\n",
    "    # Read the image into a numpy array\n",
    "    intensity_matrix = io.imread(image_file)\n",
    "\n",
    "    if channel == \"cy5\":\n",
    "        matrix_to_plot = intensity_matrix\n",
    "        # Use raw intensity for cy5 channel\n",
    "        label = 'Fluorescence Intensity'\n",
    "    else:\n",
    "        # Convert intensity values to protein concentration using the calibration curve\n",
    "        matrix_to_plot = calculate_protein_concentration(intensity_matrix, slope, intercept)\n",
    "        matrix_to_plot = matrix_to_plot / 27000 * 1E6\n",
    "        label = 'Protein concentration (nM)'\n",
    "\n",
    "    # Plot the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    im = ax.imshow(matrix_to_plot, cmap='gray', interpolation='nearest', vmin=0, vmax=vmax)\n",
    "\n",
    "    if show_scalebar:\n",
    "        plt.colorbar(im, ax=ax, label=label)\n",
    "    plt.title(f\"Time (min): {(i - min_frame) * time_interval * skip_frames / 60:.2f} \\nTime (h): {(i - min_frame) * time_interval * skip_frames / 3600:.2f} \\n{condition} - {subcondition} - {channel}\", fontsize=14)\n",
    "    plt.xlabel('x [µm]')\n",
    "    plt.ylabel('y [µm]')\n",
    "    plt.grid(True, color='#d3d3d3', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "    # Save the heatmap\n",
    "    heatmap_filename = f\"heatmap_frame_{i}.png\"\n",
    "    heatmap_path = os.path.join(output_directory_path, heatmap_filename)\n",
    "    plt.savefig(heatmap_path, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def fluorescence_heatmap(data_path, conditions, subconditions, channel, time_interval_list, min_frame, max_frame, vmax, skip_frames=1, calibration_curve_paths=None, show_scalebar=True):\n",
    "    \"\"\"\n",
    "    Reads each image as a matrix, creates, and saves a heatmap representing the normalized pixel-wise fluorescence intensity.\n",
    "\n",
    "    Args:\n",
    "    - data_path (str): Base directory where the images are stored.\n",
    "    - conditions (list): List of conditions defining subdirectories within the data path.\n",
    "    - subconditions (list): List of subconditions defining further subdirectories.\n",
    "    - channel (str): Channel specifying the fluorescence ('cy5' or 'gfp').\n",
    "    - time_interval_list (list): List of time intervals in seconds between frames for each condition.\n",
    "    - min_frame (int): Minimum frame number to start processing from.\n",
    "    - max_frame (int): Maximum frame number to stop processing at.\n",
    "    - vmax (float): Maximum value for color scale in the heatmap.\n",
    "    - skip_frames (int): Interval to skip frames (default is 1, meaning process every frame).\n",
    "    - calibration_curve_paths (list): List of file paths for the calibration curve images.\n",
    "    - show_scalebar (bool): Whether to show the color scale bar in the heatmap.\n",
    "    \"\"\"\n",
    "    output_data_dir = os.path.join(data_path, \"output_data\", \"movies\")\n",
    "    ensure_output_dir(output_data_dir)\n",
    "\n",
    "    def setup_calibration_curve(channel, calibration_curve_paths):\n",
    "        if channel != \"cy5\":\n",
    "            # Calibration curve data and fit\n",
    "            sample_concentration_values = [0, 2, 5, 10, 20, 40, 80, 160, 320]\n",
    "\n",
    "            if calibration_curve_paths is None or len(calibration_curve_paths) != len(sample_concentration_values):\n",
    "                raise ValueError(f\"Mismatch in lengths: {len(calibration_curve_paths)} calibration images, {len(sample_concentration_values)} sample concentrations\")\n",
    "\n",
    "            with mp.Pool(mp.cpu_count()) as pool:\n",
    "                mean_intensity_calibration = pool.map(calculate_mean_intensity, calibration_curve_paths)\n",
    "            return np.polyfit(sample_concentration_values, mean_intensity_calibration, 1)\n",
    "        return None, None\n",
    "\n",
    "    for idx, condition in enumerate(conditions):\n",
    "        time_interval = time_interval_list[idx]\n",
    "\n",
    "        for subcondition in subconditions:\n",
    "            # Determine the directory paths based on the channel\n",
    "            input_directory_path = os.path.join(data_path, condition, subcondition, \"original\")\n",
    "            output_directory_path = os.path.join(output_data_dir, f\"{condition}_{subcondition}_heatmaps_{channel}\")\n",
    "\n",
    "            # Create the output directory if it doesn't exist, or clear it if it does\n",
    "            if os.path.exists(output_directory_path):\n",
    "                shutil.rmtree(output_directory_path)\n",
    "            os.makedirs(output_directory_path, exist_ok=True)\n",
    "\n",
    "            # Get all .tif files in the folder\n",
    "            image_files = sorted(glob.glob(os.path.join(input_directory_path, f\"*{channel}*.tif\")))[min_frame:max_frame:skip_frames]\n",
    "\n",
    "            # Setup calibration curve for non-cy5 channels\n",
    "            slope, intercept = setup_calibration_curve(channel, calibration_curve_paths)\n",
    "\n",
    "            # Prepare arguments for multiprocessing\n",
    "            args = [(image_file, output_directory_path, channel, slope, intercept, vmax, time_interval, i, show_scalebar, min_frame, skip_frames, condition, subcondition) for i, image_file in enumerate(image_files, start=min_frame)]\n",
    "\n",
    "            # Use multiprocessing to process images\n",
    "            with mp.Pool(mp.cpu_count()) as pool:\n",
    "                list(tqdm(pool.imap(process_image, args), total=len(args), desc=f\"Processing {condition} - {subcondition}\"))\n",
    "\n",
    "\n",
    "def prepare_conditions(data_path, num_reps):\n",
    "    # List conditions while ignoring 'output_data'\n",
    "    conditions = natsorted([\n",
    "        f for f in os.listdir(data_path) \n",
    "        if os.path.isdir(os.path.join(data_path, f)) and f != 'output_data'\n",
    "    ])\n",
    "    \n",
    "    # Generate subconditions list based on num_reps\n",
    "    subconditions = [f\"Rep{x}\" for x in range(1, num_reps + 1)]\n",
    "    \n",
    "    return conditions, subconditions\n",
    "\n",
    "\n",
    "def process_video_creation(args):\n",
    "    condition, subcondition, images_dir, out_path, frame_rate, max_frame = args\n",
    "\n",
    "    image_files = natsorted(glob.glob(os.path.join(images_dir, \"*.png\")))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found for subcondition {subcondition}.\")\n",
    "        return\n",
    "\n",
    "    # Limit the number of files if max_frame is specified\n",
    "    image_files = image_files[:max_frame] if max_frame is not None else image_files\n",
    "\n",
    "    # Get the resolution of the first image (assuming all images are the same size)\n",
    "    first_image = cv2.imread(image_files[0])\n",
    "    video_resolution = (first_image.shape[1], first_image.shape[0])  # Width x Height\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    out = cv2.VideoWriter(out_path, fourcc, frame_rate, video_resolution)\n",
    "\n",
    "    for file in tqdm(image_files, desc=f\"Creating video for {condition}\", leave=False):\n",
    "        img = cv2.imread(file)\n",
    "        out.write(img)  # Write the image as a frame in the video\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Video saved to {out_path}\")\n",
    "\n",
    "\n",
    "def create_movies(data_path, conditions, subconditions, channel, frame_rate=30, max_frame=None):\n",
    "    \"\"\"\n",
    "    Creates video files from heatmaps stored in the specified directory.\n",
    "\n",
    "    Args:\n",
    "    - data_path (str): Base path where the heatmaps are stored.\n",
    "    - conditions (list): List of conditions defining subdirectories within the data path.\n",
    "    - subconditions (list): List of subconditions defining further subdirectories.\n",
    "    - channel (str): The specific channel being processed ('cy5' or 'gfp').\n",
    "    - frame_rate (int): Frame rate for the output video. Defaults to 30.\n",
    "    - max_frame (int, optional): Maximum number of frames to be included in the video. If None, all frames are included.\n",
    "    \"\"\"\n",
    "    output_data_dir = os.path.join(data_path, \"output_data\", \"movies\")\n",
    "\n",
    "    # Prepare arguments for multiprocessing\n",
    "    args_list = []\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            images_dir = os.path.join(output_data_dir, f\"{condition}_{subcondition}_heatmaps_{channel}\")\n",
    "            video_filename = f\"{condition}_{subcondition}_{channel}.avi\"\n",
    "            out_path = os.path.join(output_data_dir, video_filename)\n",
    "            args_list.append((condition, subcondition, images_dir, out_path, frame_rate, max_frame))\n",
    "\n",
    "    # Use multiprocessing to process video creation for subconditions of a single condition\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        list(tqdm(pool.imap(process_video_creation, args_list), total=len(args_list), desc=\"Creating videos\", leave=True))\n",
    "\n",
    "\n",
    "def process_intensity(path, slope, intercept, mw_kda):\n",
    "    mean_intensity = calculate_mean_intensity(path)\n",
    "    protein_concentration_ng_ul = calculate_protein_concentration(mean_intensity, intercept, slope)\n",
    "    protein_concentration_nM = calculate_protein_concentration_nM(protein_concentration_ng_ul, mw_kda)\n",
    "    return mean_intensity, protein_concentration_ng_ul, protein_concentration_nM\n",
    "\n",
    "\n",
    "def quantify_tiffiles(data_path, conditions, subconditions, calibration_curve_paths, mw_kda_list, droplet_volume_list, time_interval_s_list):\n",
    "    \"\"\"Process images to calculate protein concentration and generate plots.\"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # Sort the calibration curve paths\n",
    "    calibration_curve_paths = sorted(calibration_curve_paths)\n",
    "\n",
    "    # Calibration curve data and fit\n",
    "    sample_concentration_values = [0, 2, 5, 10, 20, 40, 80, 160, 320]\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        mean_intensity_calibration = pool.map(calculate_mean_intensity, calibration_curve_paths)\n",
    "    slope, intercept = np.polyfit(sample_concentration_values, mean_intensity_calibration, 1)\n",
    "\n",
    "    for idx, condition in enumerate(conditions):\n",
    "        # Get condition-specific parameters\n",
    "        mw_kda = mw_kda_list[idx]\n",
    "        droplet_volume = droplet_volume_list[idx]\n",
    "        time_interval_s = time_interval_s_list[idx]\n",
    "\n",
    "        for subcondition in subconditions:\n",
    "            # Construct paths based on condition and subcondition\n",
    "            pattern = os.path.join(data_path, condition, subcondition, \"original\", \"img_*********_gfp-4x_000.tif\")\n",
    "            paths = sorted(glob.glob(pattern))\n",
    "\n",
    "            if not paths:\n",
    "                print(f\"No image files found for condition {condition}, subcondition {subcondition}.\")\n",
    "                continue\n",
    "\n",
    "            # Calculate mean intensity, protein concentrations in ng/ul, and nM in parallel\n",
    "            with mp.Pool(mp.cpu_count()) as pool:\n",
    "                results = list(tqdm(pool.imap(lambda path: process_intensity(path, slope, intercept, mw_kda), paths), total=len(paths), desc=f\"Processing {condition} - {subcondition}\"))\n",
    "\n",
    "            mean_intensity_list, protein_concentration_list, protein_concentration_nM_list = zip(*results)\n",
    "\n",
    "            # Normalize intensities and concentrations\n",
    "            min_intensity = min(mean_intensity_list)\n",
    "            mean_intensity_list = np.array(mean_intensity_list) - min_intensity\n",
    "            protein_concentration_list = np.array(protein_concentration_list) - min(protein_concentration_list)\n",
    "            protein_concentration_nM_list = np.array(protein_concentration_nM_list) - min(protein_concentration_nM_list)\n",
    "\n",
    "            # Time values\n",
    "            time_values_s = np.arange(len(mean_intensity_list)) * time_interval_s\n",
    "            time_values_s, time_values_min, time_values_h = convert_time_units(time_values_s)\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                \"Condition\": condition,\n",
    "                \"Subcondition\": subcondition,\n",
    "                \"Time_s\": time_values_s,\n",
    "                \"Time_min\": time_values_min,\n",
    "                \"Time_h\": time_values_h,\n",
    "                \"Mean Intensity\": mean_intensity_list,\n",
    "                \"Protein Concentration_ng_ul\": protein_concentration_list,\n",
    "                \"Protein Concentration_nM\": protein_concentration_nM_list\n",
    "            })\n",
    "\n",
    "            # Calculate number of protein molecules\n",
    "            protein_mass_list = df[\"Protein Concentration_ng_ul\"] * droplet_volume\n",
    "            df[\"Number of Protein Molecules\"] = [calculate_number_of_protein_molecules(mass, mw_kda) for mass in protein_mass_list]\n",
    "\n",
    "            # Calculate rate of change of protein molecules\n",
    "            t_vals = np.linspace(0, (len(df) - 1) * time_interval_s, len(df))\n",
    "            dp_dt = gaussian_filter1d(np.gradient(df[\"Number of Protein Molecules\"], t_vals), sigma=2)\n",
    "            df[\"Rate of Change of Number of Protein Molecules (PM/s)\"] = dp_dt\n",
    "\n",
    "            # Append the data for this condition and subcondition to the list\n",
    "            all_data.append(df)\n",
    "\n",
    "    # Combine all data into a single DataFrame\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Calculate mean for each condition across subconditions\n",
    "    mean_df = combined_df.groupby([\"Condition\", \"Time_s\", \"Time_min\", \"Time_h\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    # Set the output directory within the data path\n",
    "    output_dir = os.path.join(data_path, \"output_data\")\n",
    "    ensure_output_dir(output_dir)\n",
    "\n",
    "    # Save combined results to CSV\n",
    "    combined_csv_path = os.path.join(output_dir, \"combined_experiment.csv\")\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "    # Save mean results to CSV\n",
    "    mean_csv_path = os.path.join(output_dir, \"mean_experiment.csv\")\n",
    "    mean_df.to_csv(mean_csv_path, index=False)\n",
    "\n",
    "    # Plotting\n",
    "    plot_results(combined_df, mean_df, output_dir, sample_concentration_values, mean_intensity_calibration, slope, intercept)\n",
    "\n",
    "    return combined_csv_path, mean_csv_path\n",
    "\n",
    "\n",
    "def plot_results(df, mean_df, output_dir, sample_concentration_values, mean_intensity_calibration, slope, intercept):\n",
    "    \"\"\"Generate plots based on the processed data.\"\"\"\n",
    "    # Create subdirectories for plots\n",
    "    single_plot_dir = os.path.join(output_dir, \"experiment_plots\", \"single_plots\")\n",
    "    combined_plot_dir = os.path.join(output_dir, \"experiment_plots\", \"combined_plots\")\n",
    "    mean_plot_dir = os.path.join(output_dir, \"experiment_plots\", \"mean_plots\")\n",
    "    ensure_output_dir(single_plot_dir)\n",
    "    ensure_output_dir(combined_plot_dir)\n",
    "    ensure_output_dir(mean_plot_dir)\n",
    "\n",
    "    # Plot calibration curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sample_concentration_values, mean_intensity_calibration, 'o', label='Data points', linewidth=0.75, markersize=5)\n",
    "    plt.plot(sample_concentration_values, slope * np.array(sample_concentration_values) + intercept, 'r-', label=f'Fit: y = {slope:.2f}x + {intercept:.2f}', linewidth=0.75)\n",
    "    plt.title('Mean Intensity vs Protein Concentration')\n",
    "    plt.xlabel('Protein Concentration (ug/ml)')\n",
    "    plt.ylabel('Mean Intensity')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(single_plot_dir, 'mean_intensity_vs_protein_concentration.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot calibration curve (log scale)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sample_concentration_values, mean_intensity_calibration, 'o', label='Data points', linewidth=0.75, markersize=5)\n",
    "    plt.plot(sample_concentration_values, slope * np.array(sample_concentration_values) + intercept, 'r-', label=f'Fit: y = {slope:.2f}x + {intercept:.2f}', linewidth=0.75)\n",
    "    plt.title('Mean Intensity vs Protein Concentration (Log Scale)')\n",
    "    plt.xlabel('Protein Concentration (ug/ml)')\n",
    "    plt.ylabel('Mean Intensity')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(single_plot_dir, 'mean_intensity_vs_protein_concentration_log.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Time units and concentration units to plot\n",
    "    time_units = [(\"Time_s\", \"Time_s\"), (\"Time_min\", \"Time_min\"), (\"Time_h\", \"Time_h\")]\n",
    "    protein_concentration_units = [\n",
    "        (\"Protein Concentration_ng_ul\", \"Protein Concentration_ng_ul\"),\n",
    "        (\"Protein Concentration_nM\", \"Protein Concentration_nM\"),\n",
    "        (\"Number of Protein Molecules\", \"Number of Protein Molecules\")\n",
    "    ]\n",
    "\n",
    "    # Plot protein concentration over time for each time and concentration unit\n",
    "    for time_unit, time_label in time_units:\n",
    "        for conc_unit, conc_label in protein_concentration_units:\n",
    "            # Individual plots for each condition and subcondition\n",
    "            for condition in df[\"Condition\"].unique():\n",
    "                # Create a directory for each condition's single plots\n",
    "                condition_single_plot_dir = os.path.join(single_plot_dir, f\"{condition}_single_plots\")\n",
    "                ensure_output_dir(condition_single_plot_dir)\n",
    "\n",
    "                for subcondition in df[df[\"Condition\"] == condition][\"Subcondition\"].unique():\n",
    "                    condition_data = df[(df[\"Condition\"] == condition) & (df[\"Subcondition\"] == subcondition)]\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.plot(condition_data[time_unit], condition_data[conc_unit], 'o-', label=f'{condition} {subcondition}', linewidth=0.75, markersize=5)\n",
    "                    plt.title(f'{conc_label} vs {time_label} for {condition} {subcondition}')\n",
    "                    plt.xlabel(time_label)\n",
    "                    plt.ylabel(conc_label)\n",
    "                    plt.legend()\n",
    "                    plt.grid(True)\n",
    "                    plt.savefig(os.path.join(condition_single_plot_dir, f'{condition}_{subcondition}_{conc_label}_vs_{time_label}.png'))\n",
    "                    plt.close()\n",
    "\n",
    "            # Combined plots for all conditions and subconditions\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for condition in df[\"Condition\"].unique():\n",
    "                for subcondition in df[df[\"Condition\"] == condition][\"Subcondition\"].unique():\n",
    "                    condition_data = df[(df[\"Condition\"] == condition) & (df[\"Subcondition\"] == subcondition)]\n",
    "                    plt.plot(condition_data[time_unit], condition_data[conc_unit], 'o-', label=f'{condition} {subcondition}', linewidth=0.75, markersize=5)\n",
    "            plt.title(f'Combined {conc_label} vs {time_label} for all conditions')\n",
    "            plt.xlabel(time_label)\n",
    "            plt.ylabel(conc_label)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(combined_plot_dir, f'combined_{conc_label}_vs_{time_label}.png'))\n",
    "            plt.close()\n",
    "\n",
    "            # Combined plots for all conditions and subconditions (log scale)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for condition in df[\"Condition\"].unique():\n",
    "                for subcondition in df[df[\"Condition\"] == condition][\"Subcondition\"].unique():\n",
    "                    condition_data = df[(df[\"Condition\"] == condition) & (df[\"Subcondition\"] == subcondition)]\n",
    "                    plt.plot(condition_data[time_unit], condition_data[conc_unit], 'o-', label=f'{condition} {subcondition}', linewidth=0.75, markersize=5)\n",
    "            plt.title(f'Combined {conc_label} vs {time_label} for all conditions (Log Scale)')\n",
    "            plt.xlabel(time_label)\n",
    "            plt.ylabel(conc_label)\n",
    "            plt.yscale('log')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(combined_plot_dir, f'combined_{conc_label}_vs_{time_label}_log.png'))\n",
    "            plt.close()\n",
    "\n",
    "            # Mean plots for each condition\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for condition in mean_df[\"Condition\"].unique():\n",
    "                condition_mean_data = mean_df[mean_df[\"Condition\"] == condition]\n",
    "                plt.plot(condition_mean_data[time_unit], condition_mean_data[conc_unit], 'o-', label=f'{condition} Mean', linewidth=0.75, markersize=5)\n",
    "            plt.title(f'Mean {conc_label} vs {time_label} for each condition')\n",
    "            plt.xlabel(time_label)\n",
    "            plt.ylabel(conc_label)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(mean_plot_dir, f'mean_{conc_label}_vs_{time_label}.png'))\n",
    "            plt.close()\n",
    "\n",
    "            # Mean plots for each condition (log scale)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for condition in mean_df[\"Condition\"].unique():\n",
    "                condition_mean_data = mean_df[mean_df[\"Condition\"] == condition]\n",
    "                plt.plot(condition_mean_data[time_unit], condition_mean_data[conc_unit], 'o-', label=f'{condition} Mean', linewidth=0.75, markersize=5)\n",
    "            plt.title(f'Mean {conc_label} vs {time_label} for each condition (Log Scale)')\n",
    "            plt.xlabel(time_label)\n",
    "            plt.ylabel(conc_label)\n",
    "            plt.yscale('log')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(mean_plot_dir, f'mean_{conc_label}_vs_{time_label}_log.png'))\n",
    "            plt.close()\n",
    "\n",
    "    # Plot rate of change of protein molecules\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for condition in df[\"Condition\"].unique():\n",
    "        for subcondition in df[df[\"Condition\"] == condition][\"Subcondition\"].unique():\n",
    "            condition_data = df[(df[\"Condition\"] == condition) & (df[\"Subcondition\"] == subcondition)]\n",
    "            plt.plot(condition_data[\"Time_h\"], condition_data[\"Rate of Change of Number of Protein Molecules (PM/s)\"], 'o', label=f'{condition} {subcondition}', linewidth=0.75, markersize=5)\n",
    "    plt.title('Rate of Change of Number of Protein Molecules vs Time_h')\n",
    "    plt.xlabel('Time_h')\n",
    "    plt.ylabel('Rate of Change (PM/s)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(combined_plot_dir, 'rate_of_change_of_protein_molecules_vs_time_h.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot rate of change of protein molecules (log scale)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for condition in df[\"Condition\"].unique():\n",
    "        for subcondition in df[df[\"Condition\"] == condition][\"Subcondition\"].unique():\n",
    "            condition_data = df[(df[\"Condition\"] == condition) & (df[\"Subcondition\"] == subcondition)]\n",
    "            plt.plot(condition_data[\"Time_h\"], condition_data[\"Rate of Change of Number of Protein Molecules (PM/s)\"], 'o', label=f'{condition} {subcondition}', linewidth=0.75, markersize=5)\n",
    "    plt.title('Rate of Change of Number of Protein Molecules vs Time_h (Log Scale)')\n",
    "    plt.xlabel('Time_h')\n",
    "    plt.ylabel('Rate of Change (PM/s)')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(combined_plot_dir, 'rate_of_change_of_protein_molecules_vs_time_h_log.png'))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing kif3 - Rep1: 100%|██████████| 9/9 [00:02<00:00,  4.49it/s]\n",
      "Processing kif3 - Rep2: 100%|██████████| 9/9 [00:01<00:00,  5.07it/s]\n",
      "Processing kif3 - Rep3: 100%|██████████| 9/9 [00:01<00:00,  5.03it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/output_data/movies/kif3_Rep1_cy5.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/output_data/movies/kif3_Rep2_cy5.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/output_data/movies/kif3_Rep3_cy5.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating videos: 100%|██████████| 3/3 [00:02<00:00,  1.46it/s]\n",
      "Processing kif3 - Rep1: 100%|██████████| 180/180 [00:28<00:00,  6.38it/s]\n",
      "Processing kif3 - Rep2: 100%|██████████| 180/180 [00:27<00:00,  6.52it/s]\n",
      "Processing kif3 - Rep3: 100%|██████████| 180/180 [00:27<00:00,  6.50it/s]\n",
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/output_data/movies/kif3_Rep2_gfp.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/output_data/movies/kif3_Rep3_gfp.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/output_data/movies/kif3_Rep1_gfp.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating videos: 100%|██████████| 3/3 [00:36<00:00, 12.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "channel = \"cy5\"\n",
    "time_interval_list = [120] * len(conditions)  # time intervals in seconds between frames for each condition\n",
    "min_frame = 0\n",
    "max_frame = None\n",
    "vmax = None  # Set vmax based on your data's expected concentration range\n",
    "skip_frames = 4\n",
    "\n",
    "# Call the function\n",
    "fluorescence_heatmap(data_path, conditions, subconditions, channel, time_interval_list, min_frame, max_frame, vmax, skip_frames, calibration_curve_paths, show_scalebar=False)\n",
    "\n",
    "frame_rate = 15  # frames per second\n",
    "\n",
    "create_movies(data_path, conditions, subconditions, channel, frame_rate=frame_rate)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "channel = \"gfp\"\n",
    "time_interval_list = [120] * len(conditions)  # time intervals in seconds between frames for each condition\n",
    "min_frame = 0\n",
    "max_frame = None\n",
    "vmax = None  # Set vmax based on your data's expected concentration range\n",
    "\n",
    "# Call the function\n",
    "fluorescence_heatmap(data_path, conditions, subconditions, channel, time_interval_list, min_frame, max_frame, vmax, skip_frames, calibration_curve_paths, show_scalebar=False)\n",
    "\n",
    "frame_rate = 15  # frames per second\n",
    "\n",
    "create_movies(data_path, conditions, subconditions, channel, frame_rate=frame_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert a single image (helper function for multiprocessing)\n",
    "def process_single_image(args):\n",
    "    file_name, output_dir, brightness_factor, contrast_factor, num_digits, i = args\n",
    "    image = Image.open(file_name).convert(\"L\")\n",
    "    image_resized = image.resize((2048, 2048), Image.LANCZOS)\n",
    "\n",
    "    enhancer = ImageEnhance.Brightness(image_resized)\n",
    "    image_brightened = enhancer.enhance(brightness_factor)\n",
    "    enhancer = ImageEnhance.Contrast(image_brightened)\n",
    "    image_contrasted = enhancer.enhance(contrast_factor)\n",
    "\n",
    "    padded_index = str(i + 1).zfill(num_digits)\n",
    "    base_file_name = f'converted_image_{padded_index}.tif'\n",
    "    processed_image_path = os.path.join(output_dir, base_file_name)\n",
    "    image_contrasted.save(processed_image_path, format='TIFF', compression='tiff_lzw')\n",
    "\n",
    "# Convert PIVlab images to the right size using multiprocessing\n",
    "def convert_images(data_path, conditions, subconditions, max_frame, brightness_factor=1, contrast_factor=1, skip_frames=1):\n",
    "    \"\"\"\n",
    "    Converts, resizes, and adjusts the brightness and contrast of images for multiple conditions and \n",
    "    subconditions, then saves the processed images in new directories.\n",
    "\n",
    "    Args:\n",
    "    - data_path (str): Base directory where the original PIV movie images are stored.\n",
    "    - conditions (list of str): List of conditions defining subdirectories within the data path.\n",
    "    - subconditions (list of str): List of subconditions defining sub-subdirectories within each condition directory.\n",
    "    - max_frame (int, optional): Maximum number of images to process. If None, all images in the directory are processed.\n",
    "    - brightness_factor (float, optional): Factor to adjust the brightness of the images. Defaults to 1 (no change).\n",
    "    - contrast_factor (float, optional): Factor to adjust the contrast of the images. Defaults to 1 (no change).\n",
    "    - skip_frames (int, optional): Number of frames to skip between processing. Defaults to 1 (no skipping).\n",
    "    \"\"\"\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            input_dir = os.path.join(data_path, condition, subcondition, \"piv_movie\")\n",
    "            output_dir = os.path.join(data_path, condition, subcondition, \"piv_movie_converted\")\n",
    "\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            input_files = natsorted(glob.glob(os.path.join(input_dir, '*.jpg')))\n",
    "\n",
    "            if max_frame:\n",
    "                input_files = input_files[:max_frame]\n",
    "\n",
    "            # Apply frame skipping\n",
    "            input_files = input_files[::skip_frames]\n",
    "\n",
    "            output_files = natsorted(glob.glob(os.path.join(output_dir, '*.tif')))\n",
    "            if len(input_files) <= len(output_files):\n",
    "                print(f\"Conversion might already be completed or partial for {output_dir}. Continuing...\")\n",
    "                # Optional: Add logic to check and continue incomplete work.\n",
    "\n",
    "            num_digits = len(str(len(input_files)))\n",
    "\n",
    "            # Prepare arguments for parallel processing\n",
    "            args = [(file_name, output_dir, brightness_factor, contrast_factor, num_digits, i)\n",
    "                    for i, file_name in enumerate(input_files)]\n",
    "\n",
    "            # Use all available cores\n",
    "            with Pool(cpu_count()) as pool:\n",
    "                pool.map(process_single_image, args)\n",
    "\n",
    "\n",
    "# helper function to plot autocorrelation\n",
    "def plot_autocorrelation_values(data_path, condition, subcondition, frame_id, lambda_tau, results, fitted_values, intervector_distance_microns):\n",
    "    output_directory_dfs = os.path.join(data_path, condition, subcondition, \"autocorrelation_plots\")\n",
    "    os.makedirs(output_directory_dfs, exist_ok=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    x_values = np.arange(len(results)) * intervector_distance_microns * 1E6\n",
    "\n",
    "    plt.plot(x_values, results, label='Autocorrelation Values', marker='o', linestyle='-', markersize=5)\n",
    "    plt.plot(x_values, fitted_values, label='Fitted Exponential Decay', linestyle='--', color='red')\n",
    "    plt.axvline(x=lambda_tau, color='green', linestyle='-.', label=f'Correlation Length = {lambda_tau:.2f} µm')\n",
    "\n",
    "    plt.xlabel('Scaled Lag (µm)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.title(f'Autocorrelation Function and Fitted Exponential Decay (Frame {frame_id})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    # plt.ylim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = os.path.join(output_directory_dfs, f'autocorrelation_frame_{frame_id}.jpg')\n",
    "    plt.savefig(filename, dpi=200, format='jpg')\n",
    "    plt.close()\n",
    "\n",
    "# helper function to calculate correlation length\n",
    "def correlation_length(data_frame):\n",
    "    # Reshaping the data frame to a 2D grid and normalizing\n",
    "    v = data_frame.pivot(index='y [m]', columns='x [m]', values=\"velocity magnitude [m/s]\").values\n",
    "    v -= np.mean(v)  # Centering the data\n",
    "\n",
    "    # FFT to find the power spectrum and compute the autocorrelation\n",
    "    fft_v = np.fft.fft2(v)\n",
    "    autocorr = np.fft.ifft2(fft_v * np.conj(fft_v))\n",
    "    autocorr = np.real(autocorr) / np.max(np.real(autocorr))  # Normalize the autocorrelation\n",
    "\n",
    "    # Preparing to extract the autocorrelation values along the diagonal\n",
    "    r_values = min(v.shape) // 2\n",
    "    results = np.zeros(r_values)\n",
    "    for r in range(r_values):\n",
    "        # Properly average over symmetric pairs around the center\n",
    "        autocorrelation_value = (autocorr[r, r] + autocorr[-r, -r]) / 2\n",
    "        results[r] = autocorrelation_value\n",
    "\n",
    "    # Normalize the results to start from 1\n",
    "    results /= results[0]\n",
    "\n",
    "    # Exponential decay fitting to extract the correlation length\n",
    "    def exponential_decay(x, A, B, C):\n",
    "        return A * np.exp(-x / B) + C\n",
    "\n",
    "    # Fit parameters and handling potential issues with initial parameter guesses\n",
    "    try:\n",
    "        params, _ = curve_fit(exponential_decay, np.arange(len(results)), results, p0=(1, 10, 0), maxfev=5000)\n",
    "    except RuntimeError:\n",
    "        # Handle cases where the curve fit does not converge\n",
    "        params = [np.nan, np.nan, np.nan]  # Use NaN to indicate the fit failed\n",
    "\n",
    "    A, B, C = params\n",
    "    fitted_values = exponential_decay(np.arange(r_values), *params)\n",
    "\n",
    "    # Calculate the correlation length\n",
    "    intervector_distance_microns = ((data_frame[\"y [m]\"].max() - data_frame[\"y [m]\"].min()) / v.shape[0])\n",
    "    if B > 0 and A != C:  # Ensure valid values for logarithmic calculation\n",
    "        lambda_tau = -B * np.log((0.3 - C) / A) * intervector_distance_microns\n",
    "    else:\n",
    "        lambda_tau = np.nan  # Return NaN if parameters are not suitable for calculation\n",
    "\n",
    "    return lambda_tau, results, fitted_values, intervector_distance_microns\n",
    "\n",
    "\n",
    "# load PIV data from PIVlab into dataframes\n",
    "def load_piv_data(data_path, condition, subcondition, min_frame=0, max_frame=None, skip_frames=1):\n",
    "    \"\"\"\n",
    "    Processes Particle Image Velocimetry (PIV) data to create a DataFrame that combines mean values, \n",
    "    power calculations, and pivot matrices for each feature.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the directory containing PIV data files.\n",
    "        condition (str): Condition label for the data set.\n",
    "        subcondition (str): Subcondition label for the data set.\n",
    "        min_frame (int, optional): Minimum frame index to start processing (inclusive).\n",
    "        max_frame (int, optional): Maximum frame index to stop processing (exclusive).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame where each row corresponds to a frame, combining mean values, \n",
    "        power calculations, and pivot matrices for each feature.\n",
    "    \"\"\"\n",
    "\n",
    "    input_piv_data = os.path.join(data_path, condition, subcondition, \"piv_data\", \"PIVlab_****.txt\")\n",
    "    \n",
    "    # Using a for loop instead of list comprehension\n",
    "    dfs = []\n",
    "    for file in sorted(glob.glob(input_piv_data))[min_frame:max_frame:skip_frames]:\n",
    "        df = pd.read_csv(file, skiprows=2).fillna(0).rename(columns={\n",
    "            \"magnitude [m/s]\": \"velocity magnitude [m/s]\",\n",
    "            \"simple shear [1/s]\": \"shear [1/s]\",\n",
    "            \"simple strain [1/s]\": \"strain [1/s]\",\n",
    "            \"Vector type [-]\": \"data type [-]\"\n",
    "        })\n",
    "        dfs.append(df)\n",
    "\n",
    "    return dfs\n",
    "\n",
    "# store pivlab output as dataframes\n",
    "def generate_dataframes_from_piv_data(data_path, condition, subcondition, min_frame=0, max_frame=None, skip_frames=1, plot_autocorrelation=True):\n",
    "    \"\"\"\n",
    "    Generates a time series pivot DataFrame from input data.\n",
    "\n",
    "    Parameters:\n",
    "    data_path (str): Path to the input data file.\n",
    "    condition (str): Primary condition for data filtering.\n",
    "    subcondition (str): Secondary condition for further data filtering.\n",
    "    min_frame (int, optional): Minimum frame to consider in the analysis. Defaults to 0.\n",
    "    max_frame (int, optional): Maximum frame to consider in the analysis. If None, considers all frames. Defaults to None.\n",
    "    plot_autocorrelation (bool, optional): Flag to plot autocorrelation. Defaults to True.\n",
    "    time_interval (int, optional): Time interval between frames, in seconds. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two pandas DataFrames. The first is the mean values DataFrame and the second is the pivot matrices DataFrame.\n",
    "    \"\"\"\n",
    "    # Creating output directories\n",
    "    output_directory_dfs = os.path.join(data_path, condition, subcondition, \"dataframes_PIV\")\n",
    "    os.makedirs(output_directory_dfs, exist_ok=True)\n",
    "\n",
    "    # Load PIV data\n",
    "    data_frames = load_piv_data(data_path, condition, subcondition, min_frame, max_frame, skip_frames)\n",
    "\n",
    "\n",
    "    # Calculating mean values with valid vectors only\n",
    "    mean_values = []\n",
    "    for frame_id, data_frame in enumerate(data_frames):\n",
    "        lambda_tau, results, fitted_values, intervector_distance_microns = correlation_length(data_frame)\n",
    "        if plot_autocorrelation:\n",
    "            plot_autocorrelation_values(data_path, condition, subcondition, frame_id, lambda_tau * 1E6, results, fitted_values, intervector_distance_microns)\n",
    "        data_frame[\"correlation length [m]\"] = lambda_tau\n",
    "        data_frame = data_frame[data_frame[\"data type [-]\"] == 1]\n",
    "        mean_values.append(data_frame.mean(axis=0))\n",
    "\n",
    "    # Creating mean DataFrame\n",
    "    mean_data_frame = pd.DataFrame(mean_values)\n",
    "    mean_data_frame.reset_index(drop=False, inplace=True)\n",
    "    mean_data_frame.rename(columns={'index': 'frame'}, inplace=True)\n",
    "\n",
    "    # Calculate power and add to DataFrame\n",
    "    volume = 2.5E-9  # µl --> m^3\n",
    "    viscosity = 1E-3  # mPa*S\n",
    "    mean_data_frame[\"power [W]\"] = volume * viscosity * (mean_data_frame[\"velocity magnitude [m/s]\"]/mean_data_frame[\"correlation length [m]\"])**2\n",
    "\n",
    "    # Renaming time column\n",
    "    # mean_data_frame.rename(columns={'frame': 'time [min]'}, inplace=True)\n",
    "\n",
    "    # Remove unnecessary columns for the pivot matrices\n",
    "    # mean_data_frame = mean_data_frame.iloc[:, 5:]\n",
    "\n",
    "    # Scale time appropriately\n",
    "    mean_data_frame[\"frame\"] = np.arange(len(mean_data_frame)) \n",
    "\n",
    "\n",
    "    # Creating pivot matrices for each feature\n",
    "    features = data_frames[0].columns[:-1]\n",
    "    pivot_matrices = {feature: [] for feature in features}\n",
    "\n",
    "    for data_frame in data_frames:\n",
    "        temporary_dictionary = {feature: data_frame.pivot(index='y [m]', columns='x [m]', values=feature).values for feature in features}\n",
    "        for feature in features:\n",
    "            pivot_matrices[feature].append(temporary_dictionary[feature])\n",
    "\n",
    "    pivot_data_frame = pd.DataFrame(pivot_matrices)\n",
    "\n",
    "    # Adjusting column names in mean_data_frame\n",
    "    mean_data_frame.columns = [f\"{column}_mean\" if column != \"frame\" else column for column in mean_data_frame.columns]\n",
    "    \n",
    "    # Adding time column to pivot_data_frame\n",
    "    pivot_data_frame[\"frame\"] = mean_data_frame[\"frame\"].values\n",
    "    \n",
    "    # Save DataFrames to CSV\n",
    "    mean_df_output_path = os.path.join(output_directory_dfs, \"mean_values.csv\")\n",
    "    mean_data_frame.to_csv(mean_df_output_path, index=False)\n",
    "\n",
    "    pivot_df_output_path = os.path.join(output_directory_dfs, \"features_matrices.csv\")\n",
    "    pivot_data_frame.to_csv(pivot_df_output_path, index=False)\n",
    "\n",
    "    # return mean_data_frame, pivot_data_frame, average_values\n",
    "    return mean_data_frame, pivot_data_frame\n",
    "\n",
    "# plot the pivlab output as heatmaps\n",
    "def generate_heatmaps_from_dataframes(df, data_path, condition, subcondition, feature_limits, time_interval=3):\n",
    "    \"\"\"\n",
    "    Generates and saves heatmaps for each feature specified in the feature_limits dictionary.\n",
    "    Each heatmap is overlaid on a corresponding image and saved to a structured directory.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing the data to plot. Each column represents a feature,\n",
    "                      and each row represents a frame.\n",
    "    - data_path (str): Base path for reading source images and saving heatmaps.\n",
    "    - condition (str): Condition name, used for directory structuring.\n",
    "    - subcondition (str): Subcondition name, further specifying the directory structure.\n",
    "    - feature_limits (dict): A dictionary where keys are feature names (column names in df) and\n",
    "                             values are tuples (vmin, vmax) representing the limits for the heatmap.\n",
    "    - time_interval (int, optional): Time interval between frames, used for time annotation in the plot title. \n",
    "                                     Default is 3.\n",
    "\n",
    "    The function creates a directory structure under 'data_path' for each feature to store the heatmaps.\n",
    "    The structure is: data_path/condition/subcondition/heatmaps_PIV/feature_name/.\n",
    "\n",
    "    Heatmaps are generated for each frame (row in df) and saved as JPEG images.\n",
    "    \"\"\"\n",
    "    \n",
    "    for feature, limits in feature_limits.items():\n",
    "        vmin, vmax = limits\n",
    "\n",
    "        for j in range(len(df)):\n",
    "            vals = df.iloc[j, df.columns.get_loc(feature)]\n",
    "\n",
    "            output_directory_heatmaps = os.path.join(data_path, condition, subcondition, \"heatmaps_PIV\", f\"{feature.split()[0]}\", f\"{feature.split()[0]}_heatmap_{j}.jpg\")\n",
    "            image_files_pattern = f\"{data_path}/{condition}/{subcondition}/piv_movie_converted/converted_image_****.tif\"\n",
    "            image_files = sorted(glob.glob(image_files_pattern))[j]\n",
    "            image = Image.open(image_files)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(image, cmap=None, extent=[-2762/2, 2762/2, -2762/2, 2762/2]) # piv image\n",
    "            im = plt.imshow(vals, cmap='inferno', origin='upper', alpha=0.7, extent=[-2762/2, 2762/2, -2762/2, 2762/2], vmin=vmin, vmax=vmax) # heatmap\n",
    "            plt.xlabel('x [um]')\n",
    "            plt.ylabel('y [um]')\n",
    "            cbar = plt.colorbar(im)\n",
    "            cbar.set_label(feature)\n",
    "            time = df.iloc[j, -1]\n",
    "            plt.title(f\"PIV - {feature}  ||  time: {int(time * time_interval/60)} min\")\n",
    "\n",
    "            os.makedirs(os.path.dirname(output_directory_heatmaps), exist_ok=True)\n",
    "            plt.savefig(output_directory_heatmaps, format='jpg', dpi=250)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# create a movie from the processed images -- general function\n",
    "def create_movies_PIV(data_path, condition, subcondition, channel, movie_type, frame_rate, feature_limits=None, max_frame=None):\n",
    "    \"\"\"\n",
    "    Creates video files from processed and annotated images stored in a specified directory.\n",
    "\n",
    "    Args:\n",
    "    - data_path (str): Base path where the annotated images are stored.\n",
    "    - condition (str): Condition under which the annotated images are stored.\n",
    "    - subcondition (str): Subcondition under which the annotated images are stored.\n",
    "    - channel (str): The specific channel being processed ('cy5' or 'gfp').\n",
    "    - movie_type (str): Type of movie to create ('single', 'grid', or 'PIV').\n",
    "    - feature_limits (dict, optional): Dictionary specifying the limits for each feature (only for 'PIV' movie type).\n",
    "    - frame_rate (int, optional): Frame rate for the output video. Defaults to 120.\n",
    "    - max_frame (int, optional): Maximum number of frames to be included in the video. If None, all frames are included.\n",
    "    \"\"\"\n",
    "\n",
    "    plots_dir = f\"{data_path}/{condition}/{subcondition}/heatmaps_PIV/\"\n",
    "    for feature in feature_limits.keys():\n",
    "        feature_name_for_file = feature.split()[0]\n",
    "        heatmap_dir = os.path.join(data_path, condition, subcondition, \"heatmaps_PIV\", f\"{feature.split()[0]}\", f\"{feature.split()[0]}_heatmap_****.jpg\")\n",
    "        image_files = natsorted(glob.glob(heatmap_dir))\n",
    "\n",
    "        if not image_files:\n",
    "            continue\n",
    "\n",
    "        # Limit the number of files if max_frame is specified\n",
    "        image_files = image_files[:max_frame] if max_frame is not None else image_files\n",
    "\n",
    "        # Get the resolution of the first image (assuming all images are the same size)\n",
    "        first_image = cv2.imread(image_files[0])\n",
    "        video_resolution = (first_image.shape[1], first_image.shape[0])  # Width x Height\n",
    "\n",
    "        # Define the codec and create VideoWriter object\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "        out = cv2.VideoWriter(f'{plots_dir}{feature_name_for_file}.avi', fourcc, frame_rate, video_resolution)\n",
    "\n",
    "        for file in image_files:\n",
    "            img = cv2.imread(file)\n",
    "            out.write(img)  # Write the image as is, without resizing\n",
    "\n",
    "        out.release()\n",
    "        return\n",
    "\n",
    "    if not image_files:\n",
    "        print(\"No images found for video creation.\")\n",
    "        return\n",
    "\n",
    "    # Limit the number of files if max_frame is specified\n",
    "    image_files = image_files[:max_frame] if max_frame is not None else image_files\n",
    "\n",
    "    # Get the resolution of the first image (assuming all images are the same size)\n",
    "    first_image = cv2.imread(image_files[0])\n",
    "    video_resolution = (first_image.shape[1], first_image.shape[0])  # Width x Height\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    out = cv2.VideoWriter(out_path, fourcc, frame_rate, video_resolution)\n",
    "\n",
    "    for file_path in image_files:\n",
    "        img = cv2.imread(file_path)\n",
    "        out.write(img)  # Write the image frame to the video\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Video saved to {out_path}\")\n",
    "\n",
    "# turn heatmaps into movies \n",
    "def process_piv_data(data_path, conditions, subconditions, feature_limits, time_intervals, min_frame=0, max_frame=None, skip_frames=1, plot_autocorrelation=True, frame_rate=120, heatmaps=True):\n",
    "    \"\"\"Process PIV data for all conditions and subconditions, then average and save results.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Base directory for PIV data and output.\n",
    "        conditions (list): List of conditions.\n",
    "        subconditions (list): List of subconditions.\n",
    "        feature_limits (dict): Dictionary of feature limits.\n",
    "        time_intervals (list): List of time intervals matching the conditions.\n",
    "        min_frame (int, optional): Minimum frame number to process. Defaults to 0.\n",
    "        max_frame (int, optional): Maximum frame number to process. Defaults to None.\n",
    "        skip_frames (int, optional): Number of frames to skip between processed frames. Defaults to 1.\n",
    "        plot_autocorrelation (bool, optional): Whether to plot autocorrelation. Defaults to True.\n",
    "        frame_rate (int, optional): Frame rate for the movies. Defaults to 120.\n",
    "    \"\"\"\n",
    "    for i, condition in enumerate(conditions):\n",
    "        time_interval = time_intervals[i] * skip_frames\n",
    "        results = []\n",
    "        for subcondition in subconditions:\n",
    "            m, p = generate_dataframes_from_piv_data(data_path, condition, subcondition, min_frame, max_frame, skip_frames, plot_autocorrelation)\n",
    "            results.append(m)\n",
    "\n",
    "            if heatmaps == True:\n",
    "                generate_heatmaps_from_dataframes(p, data_path, condition, subcondition, feature_limits, time_interval)\n",
    "                create_movies_PIV(data_path, condition, subcondition, channel=None, movie_type='PIV', feature_limits=feature_limits, frame_rate=frame_rate, max_frame=max_frame)\n",
    "\n",
    "        # Averaging and saving the results for the current condition\n",
    "        save_path = os.path.join(data_path, condition, 'averaged')\n",
    "        average_df = sum(results) / len(results)\n",
    "        \n",
    "        os.makedirs(save_path, exist_ok=True)  # Ensure the directory exists\n",
    "        average_df.to_csv(os.path.join(save_path, f\"{condition}_average.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "v = 2E-6\n",
    "velocity_limits = (0, v)\n",
    "other_limits = (-0.0005, 0.0005)\n",
    "\n",
    "\n",
    "# velocity_limits = (None, None)\n",
    "# other_limits = (None, None)\n",
    "\n",
    "\n",
    "feature_limits = {\n",
    "    'u [m/s]': (-v, v), \n",
    "    'v [m/s]': (-v, v), \n",
    "    'data type [-]': (None, None),\n",
    "    'velocity magnitude [m/s]': velocity_limits,\n",
    "    'vorticity [1/s]': other_limits,\n",
    "    'divergence [1/s]': other_limits,\n",
    "    'dcev [1]': (0, 250),\n",
    "    'shear [1/s]': other_limits,\n",
    "    'strain [1/s]': other_limits,\n",
    "    'vector direction [degrees]': (-180, 180),\n",
    "}\n",
    "\n",
    "skip_frames = 64\n",
    "time_interval_list = [120] * len(conditions)  # time intervals in seconds between frames for each condition\n",
    "\n",
    "\n",
    "# Convert images to the right size\n",
    "convert_images(data_path, conditions, subconditions, max_frame=None, brightness_factor=1, contrast_factor=1, skip_frames=skip_frames)\n",
    "# Process PIV data\n",
    "process_piv_data(data_path, conditions, subconditions, feature_limits, time_interval_list, min_frame=0, max_frame=None, skip_frames=skip_frames, plot_autocorrelation=True, frame_rate=1, heatmaps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted directory: ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/kif3/Rep1/piv_movie_converted\n",
      "Deleted directory: ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/kif3/Rep1/autocorrelation_plots\n",
      "Deleted directory: ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/kif3/Rep1/heatmaps_PIV\n",
      "Deleted directory: ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/kif3/Rep1/dataframes_PIV\n",
      "Deleted directory: ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/kif3/Rep2/piv_movie_converted\n",
      "Deleted directory: ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/kif3/Rep2/autocorrelation_plots\n",
      "Deleted directory: ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/kif3/Rep2/heatmaps_PIV\n",
      "Deleted directory: ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/kif3/Rep2/dataframes_PIV\n",
      "Deleted directory: ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/kif3/Rep3/piv_movie_converted\n",
      "Deleted directory: ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/kif3/Rep3/autocorrelation_plots\n",
      "Deleted directory: ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/kif3/Rep3/heatmaps_PIV\n",
      "Deleted directory: ../../../../Thomson Lab Dropbox/David Larios/activedrops/1&2-Mechanism&Phases/Kif3-3reps-3sint-piv/kif3/Rep3/dataframes_PIV\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_generated_outputs(data_path, conditions, subconditions, delete_images=True, delete_autocorrelations=True, delete_heatmaps=True, delete_movies=True, delete_dataframes=True):\n",
    "    \"\"\"\n",
    "    Deletes generated output files and directories for the specified conditions and subconditions.\n",
    "\n",
    "    Args:\n",
    "    - data_path (str): Base directory for the outputs.\n",
    "    - conditions (list): List of conditions.\n",
    "    - subconditions (list): List of subconditions.\n",
    "    - delete_images (bool): Flag to delete converted images. Defaults to True.\n",
    "    - delete_autocorrelations (bool): Flag to delete autocorrelation plots. Defaults to True.\n",
    "    - delete_heatmaps (bool): Flag to delete heatmaps. Defaults to True.\n",
    "    - delete_movies (bool): Flag to delete movies. Defaults to True.\n",
    "    - delete_dataframes (bool): Flag to delete dataframes. Defaults to True.\n",
    "    \"\"\"\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            if delete_images:\n",
    "                converted_images_dir = os.path.join(data_path, condition, subcondition, \"piv_movie_converted\")\n",
    "                if os.path.exists(converted_images_dir):\n",
    "                    shutil.rmtree(converted_images_dir)\n",
    "                    print(f\"Deleted directory: {converted_images_dir}\")\n",
    "\n",
    "            if delete_autocorrelations:\n",
    "                autocorrelation_dir = os.path.join(data_path, condition, subcondition, \"autocorrelation_plots\")\n",
    "                if os.path.exists(autocorrelation_dir):\n",
    "                    shutil.rmtree(autocorrelation_dir)\n",
    "                    print(f\"Deleted directory: {autocorrelation_dir}\")\n",
    "\n",
    "            if delete_heatmaps:\n",
    "                heatmaps_dir = os.path.join(data_path, condition, subcondition, \"heatmaps_PIV\")\n",
    "                if os.path.exists(heatmaps_dir):\n",
    "                    shutil.rmtree(heatmaps_dir)\n",
    "                    print(f\"Deleted directory: {heatmaps_dir}\")\n",
    "\n",
    "            if delete_movies:\n",
    "                single_movies_dir = os.path.join(data_path, f\"single_movies_{subcondition}\")\n",
    "                if os.path.exists(single_movies_dir):\n",
    "                    shutil.rmtree(single_movies_dir)\n",
    "                    print(f\"Deleted directory: {single_movies_dir}\")\n",
    "\n",
    "                grid_movies_dir = os.path.join(data_path, f\"grid_heatmaps_{subcondition}\")\n",
    "                if os.path.exists(grid_movies_dir):\n",
    "                    shutil.rmtree(grid_movies_dir)\n",
    "                    print(f\"Deleted directory: {grid_movies_dir}\")\n",
    "\n",
    "                plots_dir = os.path.join(data_path, condition, subcondition, \"heatmaps_PIV\")\n",
    "                for root, dirs, files in os.walk(plots_dir):\n",
    "                    for file in files:\n",
    "                        if file.endswith(\".avi\"):\n",
    "                            os.remove(os.path.join(root, file))\n",
    "                            print(f\"Deleted file: {os.path.join(root, file)}\")\n",
    "\n",
    "            if delete_dataframes:\n",
    "                dataframes_dir = os.path.join(data_path, condition, subcondition, \"dataframes_PIV\")\n",
    "                if os.path.exists(dataframes_dir):\n",
    "                    shutil.rmtree(dataframes_dir)\n",
    "                    print(f\"Deleted directory: {dataframes_dir}\")\n",
    "\n",
    "# Example usage:\n",
    "delete_generated_outputs(data_path, conditions, subconditions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
