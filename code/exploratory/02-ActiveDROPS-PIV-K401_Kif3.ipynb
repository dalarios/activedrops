{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import relevant libraries\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('../../activedrops')\n",
    "\n",
    "## Including ourselves\n",
    "import pivdrops\n",
    "pivdrops.set_plotting_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K401/Kif3-mVenus-Avitag ActiveDROPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of conditions and volumes\n",
    "conditions = ['k401', 'kif3']\n",
    "replicates = ['rep1']\n",
    "\n",
    "\n",
    "# Base directory for data and plots\n",
    "base_data_dir = \"../../data/02-ActiveDROPSfig2-K401_Kif3\"\n",
    "base_plots_dir = \"../../data/02-ActiveDROPSfig2-K401_Kif3\"\n",
    "\n",
    "# Iterate over each condition and volume\n",
    "for condition in conditions:\n",
    "    for replicate in replicates:\n",
    "        # Construct the input and output directory paths\n",
    "        input_dir = f\"{base_data_dir}/{condition}/{replicate}/piv_data/PIVlab_****.txt\"\n",
    "        output_dir = f\"{base_plots_dir}/{condition}/{replicate}/plots/\"\n",
    "\n",
    "        # Process the PIV files\n",
    "        dataframes = pivdrops.process_piv_files(input_dir, max_frame=None)\n",
    "\n",
    "        # Generate time series\n",
    "        df = pivdrops.piv_time_series(dataframes, time_interval_seconds=3)\n",
    "\n",
    "        # List of feature names to plot\n",
    "        features_to_plot = ['velocity', 'power', 'distance', 'work']\n",
    "\n",
    "        # Plot the time series\n",
    "        for feature in features_to_plot:\n",
    "            pivdrops.plot_time_series(df, feature, output_dir=output_dir, sigma=0.5)\n",
    "\n",
    "        pivdrops.generate_heatmaps(dataframes, 'magnitude [um/s]', vmin=0, vmax=10, output_dir_base=output_dir) \n",
    "        pivdrops.generate_heatmaps(dataframes, 'vorticity [1/s]', vmin=-0.03, vmax=0.03, output_dir_base=output_dir)\n",
    "        pivdrops.generate_heatmaps(dataframes, 'divergence [1/s]', vmin=-0.03, vmax=0.03, output_dir_base=output_dir) \n",
    "        pivdrops.generate_heatmaps(dataframes, 'dcev [1]', vmin=0, vmax=250, output_dir_base=output_dir) \n",
    "        pivdrops.generate_heatmaps(dataframes, 'simple shear [1/s]', vmin=-0.03, vmax=0.03, output_dir_base=output_dir) \n",
    "        pivdrops.generate_heatmaps(dataframes, 'simple strain [1/s]', vmin=-0.03, vmax=0.03, output_dir_base=output_dir) \n",
    "        pivdrops.generate_heatmaps(dataframes, 'vector direction [degrees]', vmin=-180, vmax=180, output_dir_base=output_dir) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append('../../activedrops')\n",
    "import pivdrops\n",
    "pivdrops.set_plotting_style()\n",
    "from natsort import natsorted  # Import for natural sorting\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "from PIL import Image, ImageEnhance, ImageOps  # Added ImageOps here\n",
    "\n",
    "\n",
    "def sorted_alphanumeric(data):\n",
    "    \"\"\"\n",
    "    Helper function to sort data in human-readable alphanumeric order.\n",
    "    \"\"\"\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(data, key=alphanum_key)\n",
    "\n",
    "def make_movies_from_features(base_directory, fps):\n",
    "    # Find all subdirectories in the base directory\n",
    "    subdirectories = [d for d in os.listdir(base_directory) if os.path.isdir(os.path.join(base_directory, d))]\n",
    "\n",
    "    for feature in subdirectories:\n",
    "        feature_directory = os.path.join(base_directory, feature)\n",
    "        output_filename = os.path.join(base_directory, f\"{feature}.avi\")\n",
    "\n",
    "        # Get all the .jpg files from the feature directory\n",
    "        images = [img for img in os.listdir(feature_directory) if img.endswith(\".jpg\")]\n",
    "\n",
    "        # Skip if no images are found\n",
    "        if not images:\n",
    "            print(f\"No images found in {feature_directory}, skipping movie creation.\")\n",
    "            continue\n",
    "\n",
    "        # Sort the images in alphanumeric order\n",
    "        images = sorted_alphanumeric(images)\n",
    "\n",
    "        # Read the first image to get the width and height\n",
    "        frame = cv2.imread(os.path.join(feature_directory, images[0]))\n",
    "        height, width, layers = frame.shape\n",
    "\n",
    "        # Define the codec and create VideoWriter object\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(output_filename, fourcc, fps, (width, height))\n",
    "\n",
    "        # Loop through all the images and add them to the video\n",
    "        for image in images:\n",
    "            frame = cv2.imread(os.path.join(feature_directory, image))\n",
    "            out.write(frame)\n",
    "\n",
    "        # Release everything when the job is finished\n",
    "        out.release()\n",
    "\n",
    "def save_dataframes(dataframes, output_dir, prefix='df'):\n",
    "    \"\"\"Saves a list of DataFrames to the specified directory.\"\"\"\n",
    "    dataframes_dir = os.path.join(output_dir, 'dataframes')\n",
    "    os.makedirs(dataframes_dir, exist_ok=True)\n",
    "\n",
    "    for i, df in enumerate(dataframes):\n",
    "        df_path = os.path.join(dataframes_dir, f'{prefix}_{i}.csv')\n",
    "        df.to_csv(df_path, index=False)\n",
    "\n",
    "def load_dataframes(output_dir, prefix='df'):\n",
    "    \"\"\"Loads DataFrames from the specified directory.\"\"\"\n",
    "    dataframes_dir = os.path.join(output_dir, 'dataframes')\n",
    "    if not os.path.exists(dataframes_dir):\n",
    "        return None\n",
    "\n",
    "    df_files = sorted(glob.glob(os.path.join(dataframes_dir, f'{prefix}_*.csv')))\n",
    "    if not df_files:\n",
    "        return None\n",
    "\n",
    "    return [pd.read_csv(df_file) for df_file in df_files]\n",
    "\n",
    "def convert_images(input_dir, output_dir, max_frame=None, brightness_factor=1, contrast_factor=1):\n",
    "    \"\"\"Converts and adjusts images from input_dir and saves them in output_dir.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    input_files = natsorted(glob.glob(os.path.join(input_dir, '*.jpg')))\n",
    "\n",
    "    if max_frame is not None:\n",
    "        input_files = input_files[:max_frame]\n",
    "\n",
    "    output_files = natsorted(glob.glob(os.path.join(output_dir, '*.tif')))\n",
    "\n",
    "    if len(input_files) == len(output_files):\n",
    "        print(f\"Conversion already completed for {output_dir}. Skipping...\")\n",
    "        return\n",
    "\n",
    "    num_digits = len(str(len(input_files)))\n",
    "\n",
    "    for i, file_name in enumerate(input_files):\n",
    "        image = Image.open(file_name).convert(\"L\")\n",
    "        image_resized = image.resize((2048, 2048), Image.LANCZOS)\n",
    "\n",
    "        # Adjust brightness\n",
    "        enhancer = ImageEnhance.Brightness(image_resized)\n",
    "        image_brightened = enhancer.enhance(brightness_factor)\n",
    "\n",
    "        # Adjust contrast\n",
    "        enhancer = ImageEnhance.Contrast(image_brightened)\n",
    "        image_contrasted = enhancer.enhance(contrast_factor)\n",
    "\n",
    "        padded_index = str(i + 1).zfill(num_digits)\n",
    "        base_file_name = f'converted_image_{padded_index}.tif'\n",
    "        processed_image_path = os.path.join(output_dir, base_file_name)\n",
    "        image_contrasted.save(processed_image_path, format='TIFF', compression='tiff_lzw')\n",
    "\n",
    "\n",
    "def combine_timeseries_dataframes(base_data_dir, conditions, subconditions):\n",
    "    \"\"\"\n",
    "    Combines time series dataframes from multiple conditions and subconditions into a single dataframe.\n",
    "\n",
    "    This function iterates through specified conditions and subconditions, locating corresponding time series dataframes in a structured directory hierarchy. Each found dataframe is read and a new column 'Condition' is added to it, concatenating the condition and subcondition names. These dataframes are then concatenated into a single dataframe.\n",
    "\n",
    "    The function assumes a specific file structure and naming convention, where time series dataframes are stored in 'plots/dataframes' subdirectory of each condition and subcondition directory, and are named 'timeseries_df_0.csv'.\n",
    "\n",
    "    Args:\n",
    "        base_data_dir (str): The base directory path where the condition and subcondition directories are located.\n",
    "        conditions (list of str): A list of condition names. Each condition corresponds to a directory under the base data directory.\n",
    "        subconditions (list of str): A list of subcondition names. Each subcondition corresponds to a subdirectory under each condition directory.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A combined dataframe containing all the time series data, with an additional 'Condition' column indicating the source condition and subcondition of each row.\n",
    "    \"\"\"\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            file_path = os.path.join(base_data_dir, condition, subcondition, 'plots', 'dataframes', 'timeseries_df_0.csv')\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['Condition'] = f'{condition} {subcondition}'\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def process_piv_workflow(\n",
    "        conditions, \n",
    "        subconditions, \n",
    "        base_data_dir, \n",
    "        feature_limits, \n",
    "        time_interval, \n",
    "        max_frame=None, \n",
    "        volume_ul=2, \n",
    "        brightness_factor=0.8, \n",
    "        contrast_factor=0.8\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Main workflow to process PIV data.\n",
    "    \n",
    "    Args:\n",
    "    - conditions (list of str): List of conditions.\n",
    "    - subconditions (list of str): List of subconditions.\n",
    "    - base_data_dir (str): Base directory for the data.\n",
    "    - time_intervals (list of int): List of time intervals in seconds, corresponding to each condition.\n",
    "    - max_frame (int, optional): Maximum number of frames to process.\n",
    "    - volume_ul (int): Volume in microliters.\n",
    "    - brightness_factor (float): Factor for brightness adjustment.\n",
    "    - contrast_factor (float): Factor for contrast adjustment.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    for i, condition in enumerate(conditions):\n",
    "        for subcondition in subconditions:\n",
    "            # Define directories\n",
    "            input_image_dir = os.path.join(base_data_dir, condition, subcondition, \"piv_movie\")\n",
    "            output_image_dir = os.path.join(base_data_dir, condition, subcondition, \"piv_movie_8bit_2048x2048\")\n",
    "            output_dir = os.path.join(base_data_dir, condition, subcondition, \"plots\")\n",
    "\n",
    "            # Convert images if not already done\n",
    "            convert_images(input_image_dir, output_image_dir, max_frame, brightness_factor, contrast_factor)\n",
    "\n",
    "            # Process PIV files\n",
    "            input_dir = os.path.join(base_data_dir, condition, subcondition, \"piv_data\", \"PIVlab_****.txt\")\n",
    "\n",
    "            # Process PIV files if not already done\n",
    "            if not os.path.exists(os.path.join(output_dir, 'dataframes')):\n",
    "                dataframes = pivdrops.process_piv_files(input_dir, volume=volume_ul, max_frame=max_frame)\n",
    "                save_dataframes(dataframes, output_dir)\n",
    "\n",
    "            # Load all dataframes\n",
    "            dataframes_dir = os.path.join(output_dir, 'dataframes')\n",
    "            dataframe_files = os.listdir(dataframes_dir)\n",
    "            dataframes = [pd.read_csv(os.path.join(dataframes_dir, f)) for f in dataframe_files[:max_frame]]\n",
    "\n",
    "            # Process time series data if not already done\n",
    "            timeseries_df_path = os.path.join(output_dir, 'dataframes', 'timeseries_df_0.csv')\n",
    "            \n",
    "            # Process time series data with the specific time interval\n",
    "            if not os.path.exists(timeseries_df_path):\n",
    "                df = pivdrops.piv_time_series(dataframes, time_interval_seconds=time_interval)\n",
    "                save_dataframes([df], output_dir, prefix='timeseries_df')\n",
    "            else:\n",
    "                df = pd.read_csv(timeseries_df_path)\n",
    "\n",
    "            # Generate heatmaps for each feature\n",
    "            for feature, (vmin, vmax) in feature_limits.items():\n",
    "                pivdrops.generate_heatmaps(dataframes, feature, vmin=vmin, vmax=vmax, output_dir_base=output_dir, image_path=output_image_dir)\n",
    "\n",
    "\n",
    "            # Make movies\n",
    "            make_movies_from_features(output_dir, fps=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_limits = {\n",
    "    'magnitude [um/s]': (0, 0.1),\n",
    "    # 'vorticity [1/s]': (-0.001, 0.001),\n",
    "    # 'divergence [1/s]': (-0.001, 0.001),\n",
    "    # 'dcev [1]': (0, 250),\n",
    "    # 'simple shear [1/s]': (-0.001, 0.001),\n",
    "    # 'simple strain [1/s]': (-0.001, 0.001),\n",
    "    # 'vector direction [degrees]': (-180, 180),\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "conditions = ['k401']\n",
    "subconditions = ['rep1']\n",
    "base_data_dir = \"../../data/02-ActiveDROPSfig2-K401_Kif3/\"\n",
    "process_piv_workflow(conditions, subconditions, base_data_dir, feature_limits, time_interval=180, max_frame=120, volume_ul=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_limits = {\n",
    "    'magnitude [um/s]': (0, 4),\n",
    "    'vorticity [1/s]': (-0.001, 0.001),\n",
    "    'divergence [1/s]': (-0.001, 0.001),\n",
    "    'dcev [1]': (0, 250),\n",
    "    'simple shear [1/s]': (-0.001, 0.001),\n",
    "    'simple strain [1/s]': (-0.001, 0.001),\n",
    "    'vector direction [degrees]': (-180, 180),\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "conditions = ['kif3']\n",
    "subconditions = ['rep1']\n",
    "base_data_dir = \"../../data/02-ActiveDROPSfig2-K401_Kif3/\"\n",
    "process_piv_workflow(conditions, subconditions, base_data_dir, feature_limits, time_interval=3, max_frame=None, volume_ul=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_timeseries_dataframes(base_data_dir, conditions, subconditions):\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            file_path = os.path.join(base_data_dir, condition, subcondition, 'plots', 'dataframes', 'timeseries_df_0.csv')\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['Condition'] = f'{condition} {subcondition}'\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "conditions = ['k401', 'kif3']\n",
    "subconditions = ['rep1']\n",
    "base_data_dir = \"../../data/02-ActiveDROPSfig2-K401_Kif3/\"\n",
    "df = combine_timeseries_dataframes(base_data_dir, conditions, subconditions)\n",
    "\n",
    "for feature in ['velocity', 'power', 'distance', 'work']:\n",
    "    pivdrops.plot_combined_time_series(df, feature, sigma=1, output_dir=base_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make it a package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('../../activedrops')\n",
    "import pivdrops\n",
    "pivdrops.set_plotting_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_limits = {\n",
    "    'magnitude [um/s]': (0, 0.1),\n",
    "    # 'vorticity [1/s]': (-0.001, 0.001),\n",
    "    # 'divergence [1/s]': (-0.001, 0.001),\n",
    "    # 'dcev [1]': (0, 250),\n",
    "    # 'simple shear [1/s]': (-0.001, 0.001),\n",
    "    # 'simple strain [1/s]': (-0.001, 0.001),\n",
    "    # 'vector direction [degrees]': (-180, 180),\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "conditions = ['k401']\n",
    "subconditions = ['rep1']\n",
    "base_data_dir = \"../../data/02-ActiveDROPSfig2-K401_Kif3/\"\n",
    "pivdrops.process_piv_workflow(conditions, subconditions, base_data_dir, feature_limits, time_interval=180, max_frame=120, volume_ul=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
