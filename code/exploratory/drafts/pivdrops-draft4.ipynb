{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "# Import data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import image processing libraries\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Import plotting and visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import colorcet as cc\n",
    "from scipy.optimize import curve_fit\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "# Additional utilities\n",
    "from natsort import natsorted  # For natural sorting\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Default RP plotting style\n",
    "def set_plotting_style():\n",
    "    \"\"\"\n",
    "    Formats plotting environment to that used in Physical Biology of the Cell,\n",
    "    2nd edition. To format all plots within a script, simply execute\n",
    "    `mwc_induction_utils.set_plotting_style() in the preamble.\n",
    "    \"\"\"\n",
    "    rc = {'lines.linewidth': 1.25,\n",
    "          'axes.labelsize': 8,\n",
    "          'axes.titlesize': 9,\n",
    "          'axes.facecolor': '#E3DCD0',\n",
    "          'xtick.labelsize': 7,\n",
    "          'ytick.labelsize': 7,\n",
    "        #   'font.family': 'Lucida Sans Unicode',\n",
    "          'grid.linestyle': '-',\n",
    "          'grid.linewidth': 0.1,\n",
    "          'grid.color': '#ffffff',\n",
    "          'legend.fontsize': 9}\n",
    "    plt.rc('text.latex', preamble=r'\\usepackage{sfmath}')\n",
    "    plt.rc('xtick.major', pad=-1)\n",
    "    plt.rc('ytick.major', pad=-1)\n",
    "    plt.rc('mathtext', fontset='stixsans', sf='sansserif')\n",
    "    plt.rc('figure', figsize=[3.5, 2.5])\n",
    "    plt.rc('svg', fonttype='none')\n",
    "    plt.rc('legend', title_fontsize='8', frameon=True, \n",
    "           facecolor='#E3DCD0', framealpha=1)\n",
    "    sns.set_style('darkgrid', rc=rc)\n",
    "    sns.set_palette(\"colorblind\", color_codes=True)\n",
    "    sns.set_context('notebook', rc=rc)\n",
    "\n",
    "set_plotting_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autocorrelation_values(lambda_tau, results, fitted_values, filename=None):\n",
    "    \"\"\"\n",
    "    Plots the autocorrelation values and the fitted exponential decay with scaled x-axis.\n",
    "    \n",
    "    Parameters:\n",
    "    - lambda_tau (float): Correlation length.\n",
    "    - results (array): Array of autocorrelation values.\n",
    "    - fitted_values (array): Array of fitted values.\n",
    "    - filename (str, optional): If provided, the plot will be saved to this filename.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    intervector_distance_microns = 21.745\n",
    "\n",
    "    # Scale x-axis by 20\n",
    "    x_values = np.arange(len(results)) * intervector_distance_microns  # Generate scaled x-coordinates by intervector distance\n",
    "\n",
    "    # Plot autocorrelation values and fitted exponential decay with scaled x-axis\n",
    "    plt.plot(x_values, results, label='Autocorrelation Values', marker='o', linestyle='-', markersize=5)\n",
    "    plt.plot(x_values, fitted_values, label='Fitted Exponential Decay', linestyle='--', color='red')\n",
    "    plt.axvline(x=lambda_tau, color='green', linestyle='-.', label=f'Correlation Length = {lambda_tau:.2f} µm')\n",
    "\n",
    "    # Adding labels, title, and legend\n",
    "    plt.xlabel('Scaled Lag (µm)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.title('Autocorrelation Function and Fitted Exponential Decay')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.ylim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # If filename is provided, save the plot\n",
    "    if filename:\n",
    "        directory = os.path.dirname(filename)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        plt.savefig(filename, dpi=200, format='jpg')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_length(df, plot_autocorrelation=True):\n",
    "\n",
    "    # Obtain square grid of velocity magnitudes\n",
    "    v = df.pivot(index='y [m]', columns='x [m]', values=\"velocity magnitude [m/s]\").values\n",
    "\n",
    "    # Calculate intervector distance\n",
    "    intervector_distance_microns = ((df[\"y [m]\"].max() - df[\"y [m]\"].min()) / v.shape[0])\n",
    "\n",
    "    # Calculate the autocorrelation function with Fourier transform\n",
    "    full_product = np.fft.fft2(v) * np.conj(np.fft.fft2(v))\n",
    "    inverse = np.real(np.fft.ifft2(full_product))  # Real part of the inverse Fourier transform\n",
    "    normalized_inverse = inverse / inverse[0, 0]  # Normalize the autocorrelation function\n",
    "\n",
    "    # Compute the autocorrelation for each r value\n",
    "    r_values = v.shape[0] // 2\n",
    "    results = np.zeros(r_values)\n",
    "    for r in range(r_values):\n",
    "        autocorrelation_value = (normalized_inverse[r, r] + normalized_inverse[-r, -r]) / (v.shape[0] * v.shape[1])\n",
    "        results[r] = autocorrelation_value \n",
    "\n",
    "    # Normalize the results array\n",
    "    results /= results[0]\n",
    "\n",
    "    # Fit the results to an exponential decay model\n",
    "    def exponential_decay(x, A, B, C):\n",
    "        return A * np.exp(-x / B) + C\n",
    "\n",
    "    params, _ = curve_fit(exponential_decay, np.arange(len(results)), results, maxfev=5000)\n",
    "    A, B, C = params\n",
    "    fitted_values = exponential_decay(np.arange(r_values), A, B, C)\n",
    "\n",
    "    # Compute correlation length and other parameters\n",
    "    lambda_tau = -B * np.log((0.3 - C) / A) * intervector_distance_microns \n",
    "\n",
    "    if plot_autocorrelation:\n",
    "        # Plot and save autocorrelation values\n",
    "        plot_autocorrelation_values(lambda_tau * 1e6, results, fitted_values)\n",
    "\n",
    "\n",
    "    return lambda_tau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_piv(data_path, condition, subcondition, min_frame=0, max_frame=None):\n",
    "    \"\"\"\n",
    "    Processes Particle Image Velocimetry (PIV) data to create a DataFrame that combines mean values, \n",
    "    power calculations, and pivot matrices for each feature.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the directory containing PIV data files.\n",
    "        condition (str): Condition label for the data set.\n",
    "        subcondition (str): Subcondition label for the data set.\n",
    "        min_frame (int, optional): Minimum frame index to start processing (inclusive).\n",
    "        max_frame (int, optional): Maximum frame index to stop processing (exclusive).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame where each row corresponds to a frame, combining mean values, \n",
    "        power calculations, and pivot matrices for each feature.\n",
    "    \"\"\"\n",
    "\n",
    "    input_piv_data = os.path.join(data_path, condition, subcondition, \"piv_data\", \"PIVlab_****.txt\")\n",
    "    \n",
    "    # Using a for loop instead of list comprehension\n",
    "    dfs = []\n",
    "    for file in sorted(glob.glob(input_piv_data))[min_frame:max_frame]:\n",
    "        df = pd.read_csv(file, skiprows=2).fillna(0).rename(columns={\n",
    "            \"magnitude [m/s]\": \"velocity magnitude [m/s]\",\n",
    "            \"simple shear [1/s]\": \"shear [1/s]\",\n",
    "            \"simple strain [1/s]\": \"strain [1/s]\",\n",
    "            \"Vector type [-]\": \"data type [-]\"\n",
    "        })\n",
    "        dfs.append(df)\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "\n",
    "def process_piv_data(data_path, condition, subcondition, min_frame=0, max_frame=None, plot_autocorrelation=True):\n",
    "    \"\"\"\n",
    "    Generates a time series pivot DataFrame from input data.\n",
    "\n",
    "    Parameters:\n",
    "    data_path (str): Path to the input data file.\n",
    "    condition (str): Primary condition for data filtering.\n",
    "    subcondition (str): Secondary condition for further data filtering.\n",
    "    min_frame (int, optional): Minimum frame to consider in the analysis. Defaults to 0.\n",
    "    max_frame (int, optional): Maximum frame to consider in the analysis. If None, considers all frames. Defaults to None.\n",
    "    plot_autocorrelation (bool, optional): Flag to plot autocorrelation. Defaults to True.\n",
    "    time_interval (int, optional): Time interval between frames, in seconds. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two pandas DataFrames. The first is the mean values DataFrame and the second is the pivot matrices DataFrame.\n",
    "    \"\"\"\n",
    "    # Creating output directories\n",
    "    output_directory_dfs = os.path.join(data_path, condition, subcondition, \"dataframes_PIV\")\n",
    "    os.makedirs(output_directory_dfs, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Reading data frames\n",
    "    # Note: Assuming df_piv and correlation_length are pre-defined functions\n",
    "    data_frames = df_piv(data_path, condition, subcondition, min_frame, max_frame)\n",
    "\n",
    "\n",
    "    # Calculating mean values with valid vectors only\n",
    "    mean_values = []\n",
    "    for data_frame in data_frames:\n",
    "        data_frame[\"correlation length [m]\"] = correlation_length(data_frame, plot_autocorrelation)\n",
    "        data_frame = data_frame[data_frame[\"data type [-]\"] == 1]\n",
    "        mean_values.append(data_frame.mean(axis=0))\n",
    "\n",
    "    # Creating mean DataFrame\n",
    "    mean_data_frame = pd.DataFrame(mean_values)\n",
    "    mean_data_frame.reset_index(drop=False, inplace=True)\n",
    "    mean_data_frame.rename(columns={'index': 'frame'}, inplace=True)\n",
    "\n",
    "    # Calculate power and add to DataFrame\n",
    "    volume = 2E-9  # µl --> m^3\n",
    "    viscosity = 1E-3  # mPa*S\n",
    "    mean_data_frame[\"power [W]\"] = volume * viscosity * (mean_data_frame[\"velocity magnitude [m/s]\"]/mean_data_frame[\"correlation length [m]\"])**2\n",
    "\n",
    "    # Renaming time column\n",
    "    # mean_data_frame.rename(columns={'frame': 'time [min]'}, inplace=True)\n",
    "\n",
    "    # Remove unnecessary columns for the pivot matrices\n",
    "    mean_data_frame = mean_data_frame.iloc[:, 5:]\n",
    "\n",
    "    # Scale time appropriately\n",
    "    mean_data_frame[\"frame\"] = np.arange(len(mean_data_frame)) \n",
    "\n",
    "\n",
    "\n",
    "    # Creating pivot matrices for each feature\n",
    "    features = data_frames[0].columns[:-1]\n",
    "    pivot_matrices = {feature: [] for feature in features}\n",
    "\n",
    "    for data_frame in data_frames:\n",
    "        temporary_dictionary = {feature: data_frame.pivot(index='y [m]', columns='x [m]', values=feature).values for feature in features}\n",
    "        for feature in features:\n",
    "            pivot_matrices[feature].append(temporary_dictionary[feature])\n",
    "\n",
    "    pivot_data_frame = pd.DataFrame(pivot_matrices)\n",
    "\n",
    "    # Adjusting column names in mean_data_frame\n",
    "    mean_data_frame.columns = [f\"<{column}>\" if column != \"frame\" else column for column in mean_data_frame.columns]\n",
    "    \n",
    "    # Adding time column to pivot_data_frame\n",
    "    pivot_data_frame[\"frame\"] = mean_data_frame[\"frame\"].values\n",
    "\n",
    "    # Save DataFrames to CSV\n",
    "    mean_df_output_path = os.path.join(output_directory_dfs, \"mean_values.csv\")\n",
    "    mean_data_frame.to_csv(mean_df_output_path, index=False)\n",
    "\n",
    "    pivot_df_output_path = os.path.join(output_directory_dfs, \"features_matrices.csv\")\n",
    "    pivot_data_frame.to_csv(pivot_df_output_path, index=False)\n",
    "\n",
    "    return mean_data_frame, pivot_data_frame.iloc[:, 2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;data type [-]&gt;</th>\n",
       "      <th>&lt;vorticity [1/s]&gt;</th>\n",
       "      <th>&lt;velocity magnitude [m/s]&gt;</th>\n",
       "      <th>&lt;divergence [1/s]&gt;</th>\n",
       "      <th>&lt;dcev [1]&gt;</th>\n",
       "      <th>&lt;shear [1/s]&gt;</th>\n",
       "      <th>&lt;strain [1/s]&gt;</th>\n",
       "      <th>&lt;vector direction [degrees]&gt;</th>\n",
       "      <th>&lt;correlation length [m]&gt;</th>\n",
       "      <th>&lt;power [W]&gt;</th>\n",
       "      <th>frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>5.028160</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>5.698260</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>1.551034e-17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000636</td>\n",
       "      <td>5.065683</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>0.978958</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>2.186982e-17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000238</td>\n",
       "      <td>5.322772</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-2.692348</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>1.964561e-17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000618</td>\n",
       "      <td>5.509289</td>\n",
       "      <td>-0.000267</td>\n",
       "      <td>-0.000370</td>\n",
       "      <td>-1.791994</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>1.923224e-17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000702</td>\n",
       "      <td>5.958653</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000273</td>\n",
       "      <td>-7.945343</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>2.075812e-17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   <data type [-]>  <vorticity [1/s]>  <velocity magnitude [m/s]>  \\\n",
       "0              1.0           0.000552                    0.000002   \n",
       "1              1.0           0.000493                    0.000002   \n",
       "2              1.0           0.000585                    0.000002   \n",
       "3              1.0           0.000277                    0.000002   \n",
       "4              1.0           0.000693                    0.000002   \n",
       "\n",
       "   <divergence [1/s]>  <dcev [1]>  <shear [1/s]>  <strain [1/s]>  \\\n",
       "0           -0.000032    5.028160       0.000353        0.000103   \n",
       "1           -0.000636    5.065683       0.000359       -0.000170   \n",
       "2           -0.000238    5.322772      -0.000055       -0.000091   \n",
       "3           -0.000618    5.509289      -0.000267       -0.000370   \n",
       "4           -0.000702    5.958653       0.000022       -0.000273   \n",
       "\n",
       "   <vector direction [degrees]>  <correlation length [m]>   <power [W]>  frame  \n",
       "0                      5.698260                  0.000581  1.551034e-17      0  \n",
       "1                      0.978958                  0.000548  2.186982e-17      1  \n",
       "2                     -2.692348                  0.000552  1.964561e-17      2  \n",
       "3                     -1.791994                  0.000550  1.923224e-17      3  \n",
       "4                     -7.945343                  0.000548  2.075812e-17      4  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../../data/01-k401-biotin_strep/\"\n",
    "condition = \"125nM\"\n",
    "subcondition = \"2ul\"\n",
    "\n",
    "# Usage\n",
    "mean_df, pivot_df = process_piv_data(data_path, condition, subcondition, min_frame=0, max_frame=5, plot_autocorrelation=False)\n",
    "mean_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_images(data_path, condition, subcondition, max_frame=None, brightness_factor=1, contrast_factor=1):\n",
    "    \"\"\"\n",
    "    Converts, resizes, and adjusts the brightness and contrast of images located in a specified \n",
    "    directory and saves the processed images in a new directory. The function identifies images based on \n",
    "    a specified data path, condition, and subcondition.\n",
    "\n",
    "    This function is specifically tailored for converting PIV (Particle Image Velocimetry) movie images. \n",
    "    It supports adjustments in brightness and contrast, and checks to avoid re-processing already \n",
    "    converted images.\n",
    "\n",
    "    Args:\n",
    "    - data_path (str): Base directory where the original PIV movie images are stored.\n",
    "    - condition (str): Specific condition defining a subdirectory within the data path.\n",
    "    - subcondition (str): Specific subcondition defining a sub-subdirectory within the condition directory.\n",
    "    - max_frame (int, optional): Maximum number of images to process. If None, all images in the directory are processed.\n",
    "    - brightness_factor (float, optional): Factor to adjust the brightness of the images. Defaults to 1 (no change).\n",
    "    - contrast_factor (float, optional): Factor to adjust the contrast of the images. Defaults to 1 (no change).\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct input and output directories based on provided path, condition, and subcondition\n",
    "    input_dir = f\"{data_path}{condition}/{subcondition}/piv_movie/\"\n",
    "    output_dir = f\"{data_path}{condition}/{subcondition}/piv_movie_converted/\"\n",
    "\n",
    "    # Create the output directory if it does not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Gather all JPEG images from the input directory\n",
    "    input_files = natsorted(glob.glob(os.path.join(input_dir, '****.jpg')))\n",
    "\n",
    "    # Limit the processing to max_frame if specified\n",
    "    input_files = input_files[:max_frame] if max_frame is not None else input_files\n",
    "\n",
    "    # Check if the output directory already has the converted files\n",
    "    output_files = natsorted(glob.glob(os.path.join(output_dir, '****.tif')))\n",
    "    if len(input_files) == len(output_files):\n",
    "        print(f\"Conversion already completed for {output_dir}. Skipping...\")\n",
    "        return\n",
    "\n",
    "    # Prepare for filename formatting\n",
    "    num_digits = len(str(len(input_files)))\n",
    "\n",
    "    # Process each image\n",
    "    for i, file_name in enumerate(input_files):\n",
    "        # Open and convert image to grayscale\n",
    "        image = Image.open(file_name).convert(\"L\")\n",
    "\n",
    "        # Resize image to 2048x2048 pixels\n",
    "        image_resized = image.resize((2048, 2048), Image.LANCZOS)\n",
    "\n",
    "        # Adjust brightness and contrast\n",
    "        enhancer = ImageEnhance.Brightness(image_resized)\n",
    "        image_brightened = enhancer.enhance(brightness_factor)\n",
    "        enhancer = ImageEnhance.Contrast(image_brightened)\n",
    "        image_contrasted = enhancer.enhance(contrast_factor)\n",
    "\n",
    "        # Prepare the filename and save the processed image\n",
    "        padded_index = str(i + 1).zfill(num_digits)\n",
    "        base_file_name = f'converted_image_{padded_index}.tif'\n",
    "        processed_image_path = os.path.join(output_dir, base_file_name)\n",
    "        image_contrasted.save(processed_image_path, format='TIFF', compression='tiff_lzw')\n",
    "\n",
    "\n",
    "convert_images(data_path, condition, subcondition, max_frame=5, brightness_factor=1, contrast_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piv_heatmap(df, data_path, condition, subcondition, feature_limits, time_interval=3):\n",
    "    \"\"\"\n",
    "    Generates and saves heatmaps for each feature specified in the feature_limits dictionary.\n",
    "    Each heatmap is overlaid on a corresponding image and saved to a structured directory.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing the data to plot. Each column represents a feature,\n",
    "                      and each row represents a frame.\n",
    "    - data_path (str): Base path for reading source images and saving heatmaps.\n",
    "    - condition (str): Condition name, used for directory structuring.\n",
    "    - subcondition (str): Subcondition name, further specifying the directory structure.\n",
    "    - feature_limits (dict): A dictionary where keys are feature names (column names in df) and\n",
    "                             values are tuples (vmin, vmax) representing the limits for the heatmap.\n",
    "    - time_interval (int, optional): Time interval between frames, used for time annotation in the plot title. \n",
    "                                     Default is 3.\n",
    "\n",
    "    The function creates a directory structure under 'data_path' for each feature to store the heatmaps.\n",
    "    The structure is: data_path/condition/subcondition/heatmaps_PIV/feature_name/.\n",
    "\n",
    "    Heatmaps are generated for each frame (row in df) and saved as JPEG images.\n",
    "    \"\"\"\n",
    "    \n",
    "    for feature, limits in feature_limits.items():\n",
    "        vmin, vmax = limits\n",
    "\n",
    "        for j in range(len(df)):\n",
    "            vals = df.iloc[j, df.columns.get_loc(feature)]\n",
    "\n",
    "            output_directory_heatmaps = os.path.join(data_path, condition, subcondition, \"heatmaps_PIV\", f\"{feature.split()[0]}\", f\"{feature.split()[0]}_heatmap_{j}.jpg\")\n",
    "            image_files_pattern = f\"{data_path}{condition}/{subcondition}/piv_movie_converted/converted_image_****.tif\"\n",
    "            image_files = sorted(glob.glob(image_files_pattern))[j]\n",
    "            image = Image.open(image_files)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(image, cmap=None, extent=[-2762/2, 2762/2, -2762/2, 2762/2]) # piv image\n",
    "            im = plt.imshow(vals, cmap='inferno', origin='lower', alpha=0.7, extent=[-2762/2, 2762/2, -2762/2, 2762/2], vmin=vmin, vmax=vmax) # heatmap\n",
    "            plt.xlabel('x [um]')\n",
    "            plt.ylabel('y [um]')\n",
    "            cbar = plt.colorbar(im)\n",
    "            cbar.set_label(feature)\n",
    "            time = df.iloc[j, -1]\n",
    "            plt.title(f\"PIV - {feature}  ||  time: {time * time_interval/60} min\")\n",
    "\n",
    "            os.makedirs(os.path.dirname(output_directory_heatmaps), exist_ok=True)\n",
    "            plt.savefig(output_directory_heatmaps, format='jpg', dpi=250)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "v = 10E-6\n",
    "velocity_limits = (0, v)\n",
    "other_limits = (-0.15, 0.15)\n",
    "\n",
    "# velocity_limits = (None, None)\n",
    "# other_limits = (None, None)\n",
    "\n",
    "feature_limits = {\n",
    "    'u [m/s]': (-v, v), \n",
    "    'v [m/s]': (-v, v), \n",
    "    'data type [-]': (None, None),\n",
    "    'velocity magnitude [m/s]': velocity_limits,\n",
    "    'vorticity [1/s]': other_limits,\n",
    "    'divergence [1/s]': other_limits,\n",
    "    'dcev [1]': (0, 250),\n",
    "    'shear [1/s]': other_limits,\n",
    "    'strain [1/s]': other_limits,\n",
    "    'vector direction [degrees]': (-180, 180),\n",
    "}\n",
    " \n",
    "piv_heatmap(pivot_df, data_path, condition, subcondition, feature_limits, time_interval=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap_movies(data_path, condition, subcondition, feature_limits, frame_rate=120, max_frame=None):\n",
    "    \"\"\"\n",
    "    Creates heatmap video files from heatmap images stored in a specified directory.\n",
    "\n",
    "    Args:\n",
    "    - data_path (str): Base path where the heatmap images are stored.\n",
    "    - condition (str): Condition under which the heatmap images are stored.\n",
    "    - subcondition (str): Subcondition under which the heatmap images are stored.\n",
    "    - feature_limits (dict): Dictionary specifying the limits for each feature.\n",
    "    - frame_rate (int, optional): Frame rate for the output video. Defaults to 120.\n",
    "    - max_frame (int, optional): Maximum number of frames to be included in the video. If None, all frames are included.\n",
    "\n",
    "    The function reads heatmap images from the specified directory and creates a video file for each feature.\n",
    "    \"\"\"\n",
    "\n",
    "    plots_dir = f\"{data_path}{condition}/{subcondition}/heatmaps_PIV/\"\n",
    "    for feature in feature_limits.keys():\n",
    "        feature_name_for_file = feature.split()[0]\n",
    "        heatmap_dir = os.path.join(data_path, condition, subcondition, \"heatmaps_PIV\", f\"{feature.split()[0]}\", f\"{feature.split()[0]}_heatmap_****.jpg\")\n",
    "        heatmap_files = natsorted(glob.glob(heatmap_dir))\n",
    "\n",
    "        if not heatmap_files:\n",
    "            continue\n",
    "\n",
    "        # Limit the number of files if max_frame is specified\n",
    "        heatmap_files = heatmap_files[:max_frame] if max_frame is not None else heatmap_files\n",
    "\n",
    "        # Get the resolution of the first image (assuming all images are the same size)\n",
    "        first_image = cv2.imread(heatmap_files[0])\n",
    "        video_resolution = (first_image.shape[1], first_image.shape[0])  # Width x Height\n",
    "\n",
    "        # Define the codec and create VideoWriter object\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "        out = cv2.VideoWriter(f'{plots_dir}{feature_name_for_file}.avi', fourcc, frame_rate, video_resolution)\n",
    "\n",
    "        for file in heatmap_files:\n",
    "            img = cv2.imread(file)\n",
    "            out.write(img)  # Write the image as is, without resizing\n",
    "\n",
    "        out.release()\n",
    "\n",
    "\n",
    "create_heatmap_movies(data_path, condition, subcondition, feature_limits, max_frame=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(dfs, data_paths, conditions, subconditions, time_interval=3):\n",
    "    \"\"\"\n",
    "    Plots each feature with respect to frame for multiple DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - dfs (list of DataFrame): List of DataFrames to plot.\n",
    "    - data_paths (list of str): List of base paths for saving the plots, corresponding to each DataFrame.\n",
    "    - conditions (list of str): List of condition names corresponding to each DataFrame.\n",
    "    - subconditions (list of str): List of subcondition names corresponding to each DataFrame.\n",
    "    - time_interval (int, optional): Time interval between frames, used for x-axis scaling. Default is 3.\n",
    "\n",
    "    The function creates a plot for each feature in the DataFrame(s), combining data from all provided\n",
    "    DataFrames. Plots are saved as JPEG images in the specified data_paths.\n",
    "    \"\"\"\n",
    "\n",
    "    for feature in dfs[0].columns[:-1]:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for df, data_path, condition, subcondition in zip(dfs, data_paths, conditions, subconditions):\n",
    "            output_directory_plots = os.path.join(data_path, condition, subcondition, \"plots_PIV\", f\"{feature.split()[0]}_plot.jpg\")\n",
    "            os.makedirs(os.path.dirname(output_directory_plots), exist_ok=True)\n",
    "\n",
    "            plt.plot(df[\"frame\"] * time_interval/60, df[feature], marker='o', linestyle='-', markersize=5, label=f'{condition}_{subcondition}')\n",
    "\n",
    "        plt.xlabel('Time (minutes)')\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(f\"PIV - {feature}\")\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        plt.legend()\n",
    "        plt.savefig(output_directory_plots, format='jpg', dpi=250)\n",
    "        plt.close()\n",
    "\n",
    "plot_features([mean_df], [data_path], [condition], [subcondition], time_interval=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(dfs, data_paths, conditions, subconditions, features):\n",
    "    # Perform PCA and Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Get colors from Seaborn's \"colorblind\" color palette\n",
    "    sns.set_palette(\"colorblind\", color_codes=True)\n",
    "    colors = sns.color_palette(\"colorblind\", n_colors=len(data_paths))\n",
    "\n",
    "    for group_index, (df, data_path, condition, subcondition) in enumerate(zip(dfs, data_paths, conditions, subconditions)):\n",
    "        pca = PCA(n_components=2)\n",
    "        principalComponents = pca.fit_transform(df.loc[:, features])\n",
    "        principalDf = pd.DataFrame(data=principalComponents, columns=['principal component 1', 'principal component 2'])\n",
    "\n",
    "        # Scaling alpha to increase with respect to the frame index\n",
    "        num_points = principalDf.shape[0]\n",
    "        alphas = np.linspace(0.01, 1, num_points)  # Alpha values linearly spaced from 1 to 0.01\n",
    "\n",
    "        # Plotting each line segment with increasing alpha\n",
    "        for i in range(1, num_points):\n",
    "            plt.plot(principalDf['principal component 1'][i-1:i+1], principalDf['principal component 2'][i-1:i+1], \n",
    "                     alpha=alphas[i], linestyle='-', linewidth=1, color=colors[group_index])\n",
    "\n",
    "        # Plotting the points\n",
    "        plt.scatter(principalDf['principal component 1'], principalDf['principal component 2'], \n",
    "                    alpha=0.5, label=f'{condition}_{subcondition}', s=10, color=colors[group_index])\n",
    "\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('PCA of PIV Features (All Samples)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    output_dir_pca = os.path.join(data_paths[-1], conditions[-1], subconditions[-1], \"plots_PIV\", \"PCA.jpg\")\n",
    "    os.makedirs(os.path.dirname(output_dir_pca), exist_ok=True)\n",
    "    plt.savefig(output_dir_pca, format='jpg', dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "features_pca = [\n",
    "    # \"<data type [-]>\", \n",
    "    \"<vorticity [1/s]>\",\t\n",
    "    \"<velocity magnitude [m/s]>\", \n",
    "    \"<divergence [1/s]>\", \n",
    "    \"<dcev [1]>\", \n",
    "    \"<shear [1/s]>\", \n",
    "    \"<strain [1/s]>\", \n",
    "    \"<vector direction [degrees]>\", \n",
    "    \"<correlation length [m]>\", \n",
    "    \"<power [W]>\"]\n",
    "\n",
    "plot_pca([mean_df], [data_path], [condition], [subcondition], features_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_images(data_path, condition, subcondition, max_frame=5, brightness_factor=1, contrast_factor=1)\n",
    "\n",
    "\n",
    "v = 10E-6\n",
    "velocity_limits = (0, v)\n",
    "other_limits = (-0.15, 0.15)\n",
    "\n",
    "velocity_limits = (None, None)\n",
    "other_limits = (None, None)\n",
    "\n",
    "feature_limits = {\n",
    "    'u [m/s]': (-v, v), \n",
    "    'v [m/s]': (-v, v), \n",
    "    'data type [-]': (None, None),\n",
    "    'velocity magnitude [m/s]': velocity_limits,\n",
    "    'vorticity [1/s]': other_limits,\n",
    "    'divergence [1/s]': other_limits,\n",
    "    'dcev [1]': (0, 250),\n",
    "    'shear [1/s]': other_limits,\n",
    "    'strain [1/s]': other_limits,\n",
    "    'vector direction [degrees]': (-180, 180),\n",
    "}\n",
    " \n",
    "piv_heatmap(pivot_df, data_path, condition, subcondition, feature_limits, time_interval=180)\n",
    "create_heatmap_movies(data_path, condition, subcondition, feature_limits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data/02-ActiveDROPSfig2-K401_Kif3/\"\n",
    "condition = \"k401\"\n",
    "subcondition = \"rep1\"\n",
    "\n",
    "\n",
    "k401, pivot_df = process_piv_data(data_path, condition, subcondition, min_frame=20, max_frame=None, plot_autocorrelation=False)\n",
    "\n",
    "plot_features([k401], [data_path], [condition], [subcondition], time_interval=180)\n",
    "\n",
    "features_pca = [\n",
    "    # \"<data type [-]>\", \n",
    "    \"<vorticity [1/s]>\",\t\n",
    "    \"<velocity magnitude [m/s]>\", \n",
    "    \"<divergence [1/s]>\", \n",
    "    \"<dcev [1]>\", \n",
    "    \"<shear [1/s]>\", \n",
    "    \"<strain [1/s]>\", \n",
    "    \"<vector direction [degrees]>\", \n",
    "    \"<correlation length [m]>\", \n",
    "    \"<power [W]>\"]\n",
    "\n",
    "plot_pca([k401], [data_path], [condition], [subcondition], features_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data/02-ActiveDROPSfig2-K401_Kif3/\"\n",
    "condition = \"kif3\"\n",
    "subcondition = \"rep1\"\n",
    "\n",
    "\n",
    "kif3, pivot_df = process_piv_data(data_path, condition, subcondition, min_frame=20, max_frame=None, plot_autocorrelation=False)\n",
    "\n",
    "plot_features([kif3], [data_path], [condition], [subcondition], time_interval=180)\n",
    "\n",
    "features_pca = [\n",
    "    # \"<data type [-]>\", \n",
    "    \"<vorticity [1/s]>\",\t\n",
    "    \"<velocity magnitude [m/s]>\", \n",
    "    \"<divergence [1/s]>\", \n",
    "    \"<dcev [1]>\", \n",
    "    \"<shear [1/s]>\", \n",
    "    \"<strain [1/s]>\", \n",
    "    \"<vector direction [degrees]>\", \n",
    "    \"<correlation length [m]>\", \n",
    "    \"<power [W]>\"]\n",
    "\n",
    "plot_pca([kif3], [data_path], [condition], [subcondition], features_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features([k401, kif3], [data_path, data_path], [\"k401\", \"kif3\"], [\"rep1\", \"rep1\"], time_interval=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pca = [\n",
    "    # \"<data type [-]>\", \n",
    "    \"<vorticity [1/s]>\",\t\n",
    "    \"<velocity magnitude [m/s]>\", \n",
    "    \"<divergence [1/s]>\", \n",
    "    # \"<dcev [1]>\", \n",
    "    \"<shear [1/s]>\", \n",
    "    \"<strain [1/s]>\", \n",
    "    \"<vector direction [degrees]>\", \n",
    "    \"<correlation length [m]>\", \n",
    "    \"<power [W]>\"\n",
    "    ]\n",
    "\n",
    "plot_pca([k401, kif3], [data_path, data_path], [\"k401\", \"kif3\"], [\"rep1\", \"rep1\"], features_pca)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
