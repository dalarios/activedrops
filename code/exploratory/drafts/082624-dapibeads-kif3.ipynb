{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File management\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import norm\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "\n",
    "# Utilities\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, cpu_count\n",
    "mp.set_start_method('fork', force=True)\n",
    "from ipywidgets import interact, FloatSlider, Layout, interactive\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import cv2\n",
    "from natsort import natsorted\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "def reorgTiffsToOriginal(data_path, conditions, subconditions):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions (list): List of conditions.\n",
    "        subconditions (list): List of subconditions.\n",
    "        \n",
    "    This function renames the subconditions as PosX and moves the raw data to the \"original\" folder.\n",
    "    \"\"\"\n",
    "    for condition in conditions:\n",
    "        # Get the actual subconditions in the directory\n",
    "        actual_subconditions = [name for name in os.listdir(os.path.join(data_path, condition)) if os.path.isdir(os.path.join(data_path, condition, name))]\n",
    "        \n",
    "        # Rename the actual subconditions to match the subconditions in your list\n",
    "        for i, actual_subcondition in enumerate(sorted(actual_subconditions)):\n",
    "            os.rename(os.path.join(data_path, condition, actual_subcondition), os.path.join(data_path, condition, subconditions[i]))\n",
    "        \n",
    "        for subcondition in subconditions:\n",
    "            # Construct the path to the subcondition directory\n",
    "            subcondition_path = os.path.join(data_path, condition, subcondition)\n",
    "            \n",
    "            # Create the path for the \"original\" directory within the subcondition directory\n",
    "            original_dir_path = os.path.join(subcondition_path, \"original\")\n",
    "            \n",
    "            # Always create the \"original\" directory\n",
    "            os.makedirs(original_dir_path, exist_ok=True)\n",
    "            \n",
    "            # Iterate over all files in the subcondition directory\n",
    "            for filename in os.listdir(subcondition_path):\n",
    "                # Check if the file is a .tif file\n",
    "                if filename.endswith(\".tif\"):\n",
    "                    # Construct the full path to the file\n",
    "                    file_path = os.path.join(subcondition_path, filename)\n",
    "                    \n",
    "                    # Construct the path to move the file to\n",
    "                    destination_path = os.path.join(original_dir_path, filename)\n",
    "                    \n",
    "                    # Move the file to the \"original\" directory\n",
    "                    shutil.move(file_path, destination_path)\n",
    "            print(f\"Moved .tif files from {subcondition_path} to {original_dir_path}\")\n",
    "\n",
    "\n",
    "def prepare_conditions(data_path):\n",
    "    \"\"\"\n",
    "    Prepare conditions and subconditions, renaming subconditions to 'RepX' where X is the index.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "    \n",
    "    Returns:\n",
    "        conditions (list): List of condition names.\n",
    "        subconditions (list): List of renamed subconditions as 'RepX'.\n",
    "    \"\"\"\n",
    "    # List conditions while ignoring 'output_data'\n",
    "    conditions = natsorted([\n",
    "        f for f in os.listdir(data_path) \n",
    "        if os.path.isdir(os.path.join(data_path, f)) and f != 'output_data'\n",
    "    ])\n",
    "    \n",
    "    # Determine the number of subconditions by counting directories in the first condition\n",
    "    num_subconditions = len([\n",
    "        f for f in os.listdir(os.path.join(data_path, conditions[0])) \n",
    "        if os.path.isdir(os.path.join(data_path, conditions[0], f))\n",
    "    ])\n",
    "    \n",
    "    # Rename subconditions to 'RepX' where X is the index (1-based)\n",
    "    subconditions = [f'Rep{i+1}' for i in range(num_subconditions)]\n",
    "    \n",
    "    return conditions, subconditions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def reorgTiffs_Split_dapi(data_path, conditions, subconditions, file_interval=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions (list): List of conditions.\n",
    "        subconditions (list): List of subconditions.\n",
    "        file_interval (int or None): Option to copy every nth file. If None, this feature is not used.\n",
    "\n",
    "    This function copies 'DAPI' images from the 'original' folder into\n",
    "    the 'DAPI' folder, using the specified interval.\n",
    "    \"\"\"\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            # Construct the path to the 'original' directory within the subcondition\n",
    "            original_dir_path = os.path.join(data_path, condition, subcondition, \"original\")\n",
    "\n",
    "            if not os.path.exists(original_dir_path):\n",
    "                print(f\"Error: The original directory {original_dir_path} does not exist.\")\n",
    "                continue\n",
    "\n",
    "            # Create the directory for the DAPI channel\n",
    "            dapi_dir = os.path.join(data_path, condition, subcondition, f\"dapi-{file_interval}x\")\n",
    "            os.makedirs(dapi_dir, exist_ok=True)\n",
    "\n",
    "            # Check if the expected output is already there\n",
    "            expected_files = [f for f in sorted(os.listdir(original_dir_path))\n",
    "                              if f.lower().endswith(\".tif\") and \"dapi\" in f.lower()]\n",
    "            expected_output_files = expected_files[::file_interval or 1]\n",
    "            already_copied_files = set(os.listdir(dapi_dir))\n",
    "\n",
    "            # If all expected files are already copied, skip this subcondition\n",
    "            if all(file in already_copied_files for file in expected_output_files):\n",
    "                print(f\"Skipping {subcondition} as the expected output is already present.\")\n",
    "                continue\n",
    "\n",
    "            # Separate list for DAPI files\n",
    "            dapi_files = []\n",
    "\n",
    "            # Iterate over all files in the original directory\n",
    "            file_list = sorted(os.listdir(original_dir_path))\n",
    "            for filename in file_list:\n",
    "                # Check if the file is a .tif file and contains 'DAPI' (case insensitive)\n",
    "                if filename.lower().endswith(\".tif\") and \"dapi\" in filename.lower():\n",
    "                    dapi_files.append(filename)\n",
    "\n",
    "            # Copy files based on the file_interval\n",
    "            if file_interval is None:\n",
    "                file_interval = 1  # Copy all files if no interval is set\n",
    "\n",
    "            for idx, filename in enumerate(dapi_files):\n",
    "                if idx % file_interval == 0:\n",
    "                    file_path = os.path.join(original_dir_path, filename)\n",
    "                    shutil.copy(file_path, os.path.join(dapi_dir, filename))\n",
    "\n",
    "            print(f\"Copied every {file_interval}th 'DAPI' file from {original_dir_path} into {dapi_dir}.\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert a single image (helper function for multiprocessing)\n",
    "def process_single_image(file_name, output_dir, brightness_factor, contrast_factor, num_digits, i):\n",
    "    image = Image.open(file_name).convert(\"L\")\n",
    "    image_resized = image.resize((2048, 2048), Image.LANCZOS)\n",
    "\n",
    "    enhancer = ImageEnhance.Brightness(image_resized)\n",
    "    image_brightened = enhancer.enhance(brightness_factor)\n",
    "    enhancer = ImageEnhance.Contrast(image_brightened)\n",
    "    image_contrasted = enhancer.enhance(contrast_factor)\n",
    "\n",
    "    padded_index = str(i + 1).zfill(num_digits)\n",
    "    base_file_name = f'converted_image_{padded_index}.tif'\n",
    "    processed_image_path = os.path.join(output_dir, base_file_name)\n",
    "    image_contrasted.save(processed_image_path, format='TIFF', compression='tiff_lzw')\n",
    "\n",
    "\n",
    "# Convert PIVlab images to the right size using multiprocessing\n",
    "def convert_images(data_path, conditions, subconditions, max_frame, brightness_factor=1, contrast_factor=1, skip_frames=1):\n",
    "    for condition in tqdm(conditions, desc=\"Conditions\", leave=False):\n",
    "        for subcondition in tqdm(subconditions, desc=\"Subconditions\", leave=False):\n",
    "            input_dir = os.path.join(data_path, condition, subcondition, \"piv_movie\")\n",
    "            output_dir = os.path.join(data_path, condition, subcondition, \"piv_movie_converted\")\n",
    "\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            input_files = natsorted(glob.glob(os.path.join(input_dir, '*.jpg')))\n",
    "\n",
    "            if max_frame:\n",
    "                input_files = input_files[:max_frame]\n",
    "\n",
    "            # Apply frame skipping\n",
    "            input_files = input_files[::skip_frames]\n",
    "\n",
    "            output_files = natsorted(glob.glob(os.path.join(output_dir, '*.tif')))\n",
    "            if len(input_files) <= len(output_files):\n",
    "                print(f\"Conversion might already be completed or partial for {output_dir}. Continuing...\")\n",
    "                # Optional: Add logic to check and continue incomplete work.\n",
    "\n",
    "            num_digits = len(str(len(input_files)))\n",
    "\n",
    "            # Use all available cores\n",
    "            with Pool(cpu_count()) as pool:\n",
    "                list(tqdm(pool.starmap(process_single_image, [(file_name, output_dir, brightness_factor, contrast_factor, num_digits, i) for i, file_name in enumerate(input_files)]), total=len(input_files), desc=\"Converting Images\", leave=False))\n",
    "\n",
    "\n",
    "# Helper function to plot autocorrelation\n",
    "def plot_autocorrelation_values(data_path, condition, subcondition, frame_id, lambda_tau, results, fitted_values, intervector_distance_microns):\n",
    "    output_directory_dfs = os.path.join(data_path, condition, subcondition, \"autocorrelation_plots\")\n",
    "    os.makedirs(output_directory_dfs, exist_ok=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    x_values = np.arange(len(results)) * intervector_distance_microns * 1E6\n",
    "\n",
    "    plt.plot(x_values, results, label='Autocorrelation Values', marker='o', linestyle='-', markersize=5)\n",
    "    plt.plot(x_values, fitted_values, label='Fitted Exponential Decay', linestyle='--', color='red')\n",
    "    plt.axvline(x=lambda_tau, color='green', linestyle='-.', label=f'Correlation Length = {lambda_tau:.2f} µm')\n",
    "\n",
    "    plt.xlabel('Scaled Lag (µm)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.title(f'Autocorrelation Function and Fitted Exponential Decay (Frame {frame_id})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    # plt.ylim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = os.path.join(output_directory_dfs, f'autocorrelation_frame_{frame_id}.jpg')\n",
    "    plt.savefig(filename, dpi=200, format='jpg')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Helper function to calculate correlation length\n",
    "def correlation_length(data_frame):\n",
    "    # Reshaping the data frame to a 2D grid and normalizing\n",
    "    v = data_frame.pivot(index='y [m]', columns='x [m]', values=\"velocity magnitude [m/s]\").values\n",
    "    v -= np.mean(v)  # Centering the data\n",
    "\n",
    "    # FFT to find the power spectrum and compute the autocorrelation\n",
    "    fft_v = np.fft.fft2(v)\n",
    "    autocorr = np.fft.ifft2(fft_v * np.conj(fft_v))\n",
    "    autocorr = np.real(autocorr) / np.max(np.real(autocorr))  # Normalize the autocorrelation\n",
    "\n",
    "    # Preparing to extract the autocorrelation values along the diagonal\n",
    "    r_values = min(v.shape) // 2\n",
    "    results = np.zeros(r_values)\n",
    "    for r in range(r_values):\n",
    "        # Properly average over symmetric pairs around the center\n",
    "        autocorrelation_value = (autocorr[r, r] + autocorr[-r, -r]) / 2\n",
    "        results[r] = autocorrelation_value\n",
    "\n",
    "    # Normalize the results to start from 1\n",
    "    results /= results[0]\n",
    "\n",
    "    # Exponential decay fitting to extract the correlation length\n",
    "    def exponential_decay(x, A, B, C):\n",
    "        return A * np.exp(-x / B) + C\n",
    "\n",
    "    # Fit parameters and handling potential issues with initial parameter guesses\n",
    "    try:\n",
    "        params, _ = curve_fit(exponential_decay, np.arange(len(results)), results, p0=(1, 10, 0), maxfev=5000)\n",
    "    except RuntimeError:\n",
    "        # Handle cases where the curve fit does not converge\n",
    "        params = [np.nan, np.nan, np.nan]  # Use NaN to indicate the fit failed\n",
    "\n",
    "    A, B, C = params\n",
    "    fitted_values = exponential_decay(np.arange(r_values), *params)\n",
    "\n",
    "    # Calculate the correlation length\n",
    "    intervector_distance_microns = ((data_frame[\"y [m]\"].max() - data_frame[\"y [m]\"].min()) / v.shape[0])\n",
    "    if B > 0 and A != C:  # Ensure valid values for logarithmic calculation\n",
    "        lambda_tau = -B * np.log((0.3 - C) / A) * intervector_distance_microns\n",
    "    else:\n",
    "        lambda_tau = np.nan  # Return NaN if parameters are not suitable for calculation\n",
    "\n",
    "    return lambda_tau, results, fitted_values, intervector_distance_microns\n",
    "\n",
    "\n",
    "# Load PIV data from PIVlab into dataframes\n",
    "def load_piv_data(data_path, condition, subcondition, min_frame=0, max_frame=None, skip_frames=1):\n",
    "    input_piv_data = os.path.join(data_path, condition, subcondition, \"piv_data\", \"PIVlab_****.txt\")\n",
    "    \n",
    "    # Using a for loop instead of list comprehension\n",
    "    dfs = []\n",
    "    for file in tqdm(sorted(glob.glob(input_piv_data))[min_frame:max_frame:skip_frames], desc=f\"Loading PIV data for {condition} {subcondition}\", leave=False):\n",
    "        df = pd.read_csv(file, skiprows=2).fillna(0).rename(columns={\n",
    "            \"magnitude [m/s]\": \"velocity magnitude [m/s]\",\n",
    "            \"simple shear [1/s]\": \"shear [1/s]\",\n",
    "            \"simple strain [1/s]\": \"strain [1/s]\",\n",
    "            \"Vector type [-]\": \"data type [-]\"\n",
    "        })\n",
    "        dfs.append(df)\n",
    "\n",
    "    return dfs\n",
    "\n",
    "# Generate dataframes from PIV data with time intervals applied\n",
    "def generate_dataframes_from_piv_data(data_path, condition, subcondition, min_frame=0, max_frame=None, skip_frames=1, plot_autocorrelation=True, time_interval=1):\n",
    "    output_directory_dfs = os.path.join(data_path, condition, subcondition, \"dataframes_PIV\")\n",
    "    os.makedirs(output_directory_dfs, exist_ok=True)\n",
    "\n",
    "    # Load PIV data\n",
    "    data_frames = load_piv_data(data_path, condition, subcondition, min_frame, max_frame, skip_frames)\n",
    "\n",
    "    # Calculating mean values with valid vectors only\n",
    "    mean_values = []\n",
    "    for frame_id, data_frame in enumerate(tqdm(data_frames, desc=f\"Generating dataframes for {condition} {subcondition}\", leave=False)):\n",
    "        lambda_tau, results, fitted_values, intervector_distance_microns = correlation_length(data_frame)\n",
    "        if plot_autocorrelation:\n",
    "            plot_autocorrelation_values(data_path, condition, subcondition, frame_id, lambda_tau * 1E6, results, fitted_values, intervector_distance_microns)\n",
    "        data_frame[\"correlation length [m]\"] = lambda_tau\n",
    "        data_frame = data_frame[data_frame[\"data type [-]\"] == 1]\n",
    "        mean_values.append(data_frame.mean(axis=0))\n",
    "\n",
    "    # Creating mean DataFrame\n",
    "    mean_data_frame = pd.DataFrame(mean_values)\n",
    "    mean_data_frame.reset_index(drop=False, inplace=True)\n",
    "    mean_data_frame.rename(columns={'index': 'frame'}, inplace=True)\n",
    "\n",
    "    # Subtract the minimum row value for each column from the entire column for velocity magnitude\n",
    "    mean_data_frame[\"velocity magnitude [m/s]\"] = mean_data_frame[\"velocity magnitude [m/s]\"] - mean_data_frame[\"velocity magnitude [m/s]\"].min()\n",
    "    \n",
    "    # add a column with total distance travelled\n",
    "    mean_data_frame[\"distance [m]\"] = mean_data_frame[\"velocity magnitude [m/s]\"].cumsum() * time_interval\n",
    "    mean_data_frame[\"distance [m]\"] = mean_data_frame[\"distance [m]\"] - mean_data_frame[\"distance [m]\"].min()\n",
    "\n",
    "    # Calculate power and add to DataFrame\n",
    "    volume = 2E-9  # µl --> m^3\n",
    "    viscosity = 1E-3  # mPa*S\n",
    "    mean_data_frame[\"power [W]\"] = volume * viscosity * (mean_data_frame[\"velocity magnitude [m/s]\"]/mean_data_frame[\"correlation length [m]\"])**2\n",
    "\n",
    "    # Scale time appropriately using the provided time_interval\n",
    "    mean_data_frame[\"time (s)\"] = mean_data_frame[\"frame\"] * time_interval\n",
    "    mean_data_frame[\"time (min)\"] = mean_data_frame[\"time (s)\"] / 60\n",
    "    mean_data_frame[\"time (h)\"] = mean_data_frame[\"time (min)\"] / 60\n",
    "\n",
    "    # Creating pivot matrices for each feature\n",
    "    features = data_frames[0].columns[:-1]\n",
    "    pivot_matrices = {feature: [] for feature in features}\n",
    "\n",
    "    for data_frame in data_frames:\n",
    "        temporary_dictionary = {feature: data_frame.pivot(index='y [m]', columns='x [m]', values=feature).values for feature in features}\n",
    "        for feature in features:\n",
    "            pivot_matrices[feature].append(temporary_dictionary[feature])\n",
    "\n",
    "    pivot_data_frame = pd.DataFrame(pivot_matrices)\n",
    "\n",
    "    # Adjusting column names in mean_data_frame\n",
    "    mean_data_frame.columns = [f\"{column}_mean\" if column not in [\"frame\", \"time (s)\", \"time (min)\", \"time (h)\"] else column for column in mean_data_frame.columns]\n",
    "    \n",
    "    # Adding time column to pivot_data_frame\n",
    "    pivot_data_frame[\"frame\"] = mean_data_frame[\"frame\"].values\n",
    "    \n",
    "    # subtract the minimum row value for each column from the entire column in \n",
    "    \n",
    "    # Save DataFrames to CSV\n",
    "    mean_df_output_path = os.path.join(output_directory_dfs, \"mean_values.csv\")\n",
    "    mean_data_frame.to_csv(mean_df_output_path, index=False)\n",
    "\n",
    "    pivot_df_output_path = os.path.join(output_directory_dfs, \"features_matrices.csv\")\n",
    "    pivot_data_frame.to_csv(pivot_df_output_path, index=False)\n",
    "\n",
    "    return mean_data_frame, pivot_data_frame\n",
    "\n",
    "\n",
    "\n",
    "# Plot the PIVlab output as heatmaps\n",
    "def generate_heatmaps_from_dataframes(df, data_path, condition, subcondition, feature_limits, time_interval=3):\n",
    "    for feature, limits in feature_limits.items():\n",
    "        vmin, vmax = limits\n",
    "\n",
    "        for j in tqdm(range(len(df)), desc=f\"Generating heatmaps for {condition} {subcondition} {feature}\", leave=False):\n",
    "            vals = df.iloc[j, df.columns.get_loc(feature)]\n",
    "\n",
    "            output_directory_heatmaps = os.path.join(data_path, condition, subcondition, \"heatmaps_PIV\", f\"{feature.split()[0]}\", f\"{feature.split()[0]}_heatmap_{j}.jpg\")\n",
    "            image_files_pattern = f\"{data_path}/{condition}/{subcondition}/piv_movie_converted/converted_image_****.tif\"\n",
    "            image_files = sorted(glob.glob(image_files_pattern))[j]\n",
    "            image = Image.open(image_files)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(image, cmap=None, extent=[-2762/2, 2762/2, -2762/2, 2762/2]) # piv image\n",
    "            im = plt.imshow(vals, cmap='inferno', origin='upper', alpha=0.7, extent=[-2762/2, 2762/2, -2762/2, 2762/2], vmin=vmin, vmax=vmax) # heatmap\n",
    "            plt.xlabel('x [um]')\n",
    "            plt.ylabel('y [um]')\n",
    "            cbar = plt.colorbar(im)\n",
    "            cbar.set_label(feature)\n",
    "            time = df.iloc[j, -1]\n",
    "            plt.title(f\"PIV - {feature}  ||  time: {int(time * time_interval/60)} min -- {int(time * time_interval/3600)} hours\")\n",
    "\n",
    "            os.makedirs(os.path.dirname(output_directory_heatmaps), exist_ok=True)\n",
    "            plt.savefig(output_directory_heatmaps, format='jpg', dpi=250)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def create_movies_PIV(data_path, condition, subcondition, frame_rate, feature_limits=None, max_frame=None):\n",
    "    plots_dir = f\"{data_path}/{condition}/{subcondition}/heatmaps_PIV/\"\n",
    "    for feature in feature_limits.keys():\n",
    "        feature_name_for_file = feature.split()[0]\n",
    "        heatmap_dir = os.path.join(data_path, condition, subcondition, \"heatmaps_PIV\", f\"{feature.split()[0]}\", f\"{feature.split()[0]}_heatmap_****.jpg\")\n",
    "        image_files = natsorted(glob.glob(heatmap_dir))\n",
    "\n",
    "        if not image_files:\n",
    "            print(f\"No images found for feature {feature_name_for_file}.\")\n",
    "            continue\n",
    "\n",
    "        # Limit the number of files if max_frame is specified\n",
    "        image_files = image_files[:max_frame] if max_frame is not None else image_files\n",
    "\n",
    "        # Get the resolution of the first image (assuming all images are the same size)\n",
    "        first_image = cv2.imread(image_files[0])\n",
    "        video_resolution = (first_image.shape[1], first_image.shape[0])  # Width x Height\n",
    "\n",
    "        # Define the codec and create VideoWriter object\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "        out_path = f'{plots_dir}{feature_name_for_file}.avi'\n",
    "        out = cv2.VideoWriter(out_path, fourcc, frame_rate, video_resolution)\n",
    "\n",
    "        for file in tqdm(image_files, desc=f\"Creating movie for {condition} {subcondition} {feature}\", leave=False):\n",
    "            img = cv2.imread(file)\n",
    "            out.write(img)  # Write the image as is, without resizing\n",
    "\n",
    "        out.release()\n",
    "        print(f\"Video saved to {out_path}\")\n",
    "\n",
    "\n",
    "# Process PIV data for all conditions and subconditions, then average and save results\n",
    "def process_piv_data(data_path, conditions, subconditions, feature_limits, time_intervals, skip_frames, min_frame=0, max_frame=None, plot_autocorrelation=True, frame_rate=120, heatmaps=True):\n",
    "    for i, condition in tqdm(enumerate(conditions), desc=\"Processing PIV data\", total=len(conditions), leave=True):\n",
    "        time_interval = time_intervals[i] * skip_frames\n",
    "        results = []\n",
    "        for subcondition in tqdm(subconditions, desc=f\"Processing subconditions for {condition}\", leave=False):\n",
    "            m, p = generate_dataframes_from_piv_data(data_path, condition, subcondition, min_frame, max_frame, skip_frames, plot_autocorrelation, time_interval)\n",
    "            results.append(m)\n",
    "\n",
    "            if heatmaps == True:\n",
    "                convert_images(data_path, conditions, subconditions, max_frame=None, brightness_factor=1, contrast_factor=1, skip_frames=skip_frames)\n",
    "                generate_heatmaps_from_dataframes(p, data_path, condition, subcondition, feature_limits, time_interval)\n",
    "                create_movies_PIV(data_path, condition, subcondition, frame_rate, feature_limits=feature_limits, max_frame=max_frame)\n",
    "\n",
    "        # Averaging and saving the results for the current condition\n",
    "        save_path = os.path.join(data_path, condition, 'averaged')\n",
    "        average_df = sum(results) / len(results)\n",
    "        \n",
    "        os.makedirs(save_path, exist_ok=True)  # Ensure the directory exists\n",
    "        average_df.to_csv(os.path.join(save_path, f\"{condition}_average.csv\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# Generate PCA from PIVlab output\n",
    "def plot_pca(dfs, data_path, conditions, subconditions, features):\n",
    "    # Perform PCA and Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Get colors from Seaborn's \"colorblind\" color palette\n",
    "    sns.set_palette(\"colorblind\", color_codes=True)\n",
    "    colors = sns.color_palette(\"colorblind\", n_colors=len(conditions))\n",
    "\n",
    "    for group_index, (df, condition, subcondition) in enumerate(zip(dfs, conditions, subconditions)):\n",
    "        pca = PCA(n_components=2)\n",
    "        principalComponents = pca.fit_transform(df.loc[:, features])\n",
    "        principalDf = pd.DataFrame(data=principalComponents, columns=['principal component 1', 'principal component 2'])\n",
    "\n",
    "        # Scaling alpha to increase with respect to the frame index\n",
    "        num_points = principalDf.shape[0]\n",
    "        alphas = np.linspace(0.001, 1, num_points)  # Alpha values linearly spaced from 1 to 0.01\n",
    "        \n",
    "        # Plotting each line segment with increasing alpha\n",
    "        for i in range(1, num_points):\n",
    "            plt.plot(principalDf['principal component 1'][i-1:i+1], principalDf['principal component 2'][i-1:i+1], \n",
    "                     alpha=alphas[i], linestyle='-', linewidth=2, color=colors[group_index])\n",
    "\n",
    "        # Plotting the points\n",
    "        plt.scatter(principalDf['principal component 1'], principalDf['principal component 2'], \n",
    "                    alpha=0.5, label=f'{condition}_{subcondition}', s=10, color=colors[group_index])\n",
    "\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('PCA of PIV Features (All Samples)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    output_dir_pca = os.path.join(data_path, conditions[-1], subconditions[-1], \"plots_PIV\", \"PCA.jpg\")\n",
    "    os.makedirs(os.path.dirname(output_dir_pca), exist_ok=True)\n",
    "    plt.savefig(output_dir_pca, format='jpg', dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def plot_PIV_features(data_path, conditions, subconditions, features_pca, sigma=10, min_frame=0, max_frame=None):\n",
    "    for condition in tqdm(conditions, desc=\"Plotting PIV features\", leave=True):\n",
    "        dfs = []\n",
    "        \n",
    "        for subcondition in subconditions:\n",
    "            # Construct the file path\n",
    "            file_path = os.path.join(data_path, condition, subcondition, \"dataframes_PIV\", \"mean_values.csv\")\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Apply Gaussian filter\n",
    "            df.iloc[:, 1:-3] = df.iloc[:, 1:-3].apply(lambda x: gaussian_filter1d(x, sigma=sigma))\n",
    "\n",
    "            # Rename columns\n",
    "            df = df.rename(columns={\n",
    "                \"data type [-]_mean\": \"work [J]\",\n",
    "                \"correlation length [m]_mean\": \"correlation length [um]\",\n",
    "                \"velocity magnitude [m/s]_mean\": \"velocity magnitude [um/s]\"\n",
    "            })\n",
    "\n",
    "            # Calculate cumulative work\n",
    "            df[\"work [J]\"] = df[\"power [W]_mean\"].cumsum()\n",
    "\n",
    "            # Slice the dataframe if min_frame and max_frame are provided\n",
    "            df = df.iloc[min_frame:max_frame, :]\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "        # Plot PCA\n",
    "        plot_pca(dfs, data_path, [condition] * len(subconditions), subconditions, features_pca)\n",
    "\n",
    "        # Plot individual features\n",
    "        for feature in dfs[0].columns[5:-3]:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            for df, subcondition in zip(dfs, subconditions):\n",
    "                output_directory_plots = os.path.join(data_path, condition, subcondition, \"plots_PIV\")\n",
    "                \n",
    "                # Ensure the directory exists\n",
    "                os.makedirs(output_directory_plots, exist_ok=True)\n",
    "                \n",
    "                plt.plot(df[\"time (h)\"], df[feature], marker='o', linestyle='-', markersize=1, linewidth=1, label=f'{condition}_{subcondition}')\n",
    "                plt.xlabel('Time (hours)')\n",
    "                plt.ylabel(feature)\n",
    "                plt.title(f\"PIV - {feature}\")\n",
    "                plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "                plt.legend()\n",
    "                plt.savefig(os.path.join(output_directory_plots, f\"{feature.split()[0]}_h.jpg\"), format='jpg', dpi=300)\n",
    "                plt.close()\n",
    "\n",
    "                plt.plot(df[\"time (min)\"], df[feature], marker='o', linestyle='-', markersize=1, linewidth=1, label=f'{condition}_{subcondition}')\n",
    "                plt.xlabel('Time (minutes)')\n",
    "                plt.ylabel(feature)\n",
    "                plt.title(f\"PIV - {feature}\")\n",
    "                plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "                plt.legend()\n",
    "                plt.savefig(os.path.join(output_directory_plots, f\"{feature.split()[0]}_min.jpg\"), format='jpg', dpi=300)\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def plot_PIV_features_averaged(data_path, conditions, features_pca, sigma=2, min_frame=0, max_frame=None):\n",
    "    \"\"\"\n",
    "    Generate PIV plots averaged over subconditions and save them in the 'PIV_plots_averaged' folder.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions (list): List of conditions.\n",
    "        features_pca (list): List of features to include in PCA and plotting.\n",
    "        sigma (float): The standard deviation for Gaussian kernel applied for smoothing.\n",
    "        min_frame (int): The minimum frame to include in the analysis.\n",
    "        max_frame (int): The maximum frame to include in the analysis.\n",
    "    \"\"\"\n",
    "    for condition in tqdm(conditions, desc=\"Plotting averaged PIV features\", leave=True):\n",
    "        # Path to the averaged data\n",
    "        averaged_data_path = os.path.join(data_path, condition, \"averaged\")\n",
    "        averaged_df_file = os.path.join(averaged_data_path, f\"{condition}_average.csv\")\n",
    "        \n",
    "        if not os.path.exists(averaged_df_file):\n",
    "            print(f\"Error: Averaged dataframe {averaged_df_file} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        # Load the averaged dataframe\n",
    "        df = pd.read_csv(averaged_df_file)\n",
    "        \n",
    "        # Apply Gaussian smoothing\n",
    "        df.iloc[:, 1:-3] = df.iloc[:, 1:-3].apply(lambda x: gaussian_filter1d(x, sigma=sigma))\n",
    "        \n",
    "        # Rename columns as necessary for consistency in plotting\n",
    "        df = df.rename(columns={\"data type [-]_mean\": \"work [J]\", \n",
    "                                \"correlation length [m]_mean\": \"correlation length [um]\", \n",
    "                                \"velocity magnitude [m/s]_mean\": \"velocity magnitude [um/s]\"})\n",
    "        \n",
    "        df[\"work [J]\"] = df[\"power [W]_mean\"].cumsum()\n",
    "        \n",
    "        # Limit the frames if specified\n",
    "        df = df.iloc[min_frame:max_frame, :]\n",
    "        \n",
    "        # Prepare output directory\n",
    "        output_directory_plots = os.path.join(averaged_data_path, \"PIV_plots_averaged\")\n",
    "        os.makedirs(output_directory_plots, exist_ok=True)\n",
    "        \n",
    "        # PCA and feature plotting\n",
    "        for feature in features_pca:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.plot(df[\"time (h)\"], df[feature], marker='o', linestyle='-', markersize=1, linewidth=1)\n",
    "            plt.xlabel('Time (hours)')\n",
    "            plt.ylabel(feature)\n",
    "            plt.title(f\"Averaged PIV - {feature} over Subconditions\")\n",
    "            plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "            plt.savefig(os.path.join(output_directory_plots, f\"{feature.split()[0]}_h.jpg\"), format='jpg', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            plt.plot(df[\"time (min)\"], df[feature], marker='o', linestyle='-', markersize=1, linewidth=1)\n",
    "            plt.xlabel('Time (minutes)')\n",
    "            plt.ylabel(feature)\n",
    "            plt.title(f\"Averaged PIV - {feature} over Subconditions\")\n",
    "            plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "            plt.savefig(os.path.join(output_directory_plots, f\"{feature.split()[0]}_min.jpg\"), format='jpg', dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_PIV_features_combined(data_path, conditions, features_pca, sigma=2, min_frame=0, max_frame=None):\n",
    "    \"\"\"\n",
    "    Generate combined PIV plots across all conditions and save them in the 'combined_PIV_plots' folder.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions (list): List of conditions.\n",
    "        features_pca (list): List of features to include in PCA and plotting.\n",
    "        sigma (float): The standard deviation for Gaussian kernel applied for smoothing.\n",
    "        min_frame (int): The minimum frame to include in the analysis.\n",
    "        max_frame (int): The maximum frame to include in the analysis.\n",
    "    \"\"\"\n",
    "    # Prepare output directory for combined plots\n",
    "    combined_output_dir = os.path.join(data_path, \"PIV_plots\", \"averaged_conditions\")\n",
    "    os.makedirs(combined_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Dictionary to store data for combined plots\n",
    "    combined_data = {feature: {} for feature in features_pca}\n",
    "    \n",
    "    for condition in tqdm(conditions, desc=\"Collecting PIV data\", leave=True):\n",
    "        # Path to the averaged data\n",
    "        averaged_data_path = os.path.join(data_path, condition, \"averaged\")\n",
    "        averaged_df_file = os.path.join(averaged_data_path, f\"{condition}_average.csv\")\n",
    "        \n",
    "        if not os.path.exists(averaged_df_file):\n",
    "            print(f\"Error: Averaged dataframe {averaged_df_file} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        # Load the averaged dataframe\n",
    "        df = pd.read_csv(averaged_df_file)\n",
    "        \n",
    "        # Apply Gaussian smoothing\n",
    "        df.iloc[:, 1:-3] = df.iloc[:, 1:-3].apply(lambda x: gaussian_filter1d(x, sigma=sigma))\n",
    "        \n",
    "        # Rename columns as necessary for consistency in plotting\n",
    "        df = df.rename(columns={\"data type [-]_mean\": \"work [J]\", \n",
    "                                \"correlation length [m]_mean\": \"correlation length [um]\", \n",
    "                                \"velocity magnitude [m/s]_mean\": \"velocity magnitude [um/s]\"})\n",
    "        \n",
    "        df[\"work [J]\"] = df[\"power [W]_mean\"].cumsum()\n",
    "        \n",
    "        # Limit the frames if specified\n",
    "        df = df.iloc[min_frame:max_frame, :]\n",
    "        \n",
    "        # Store data for combined plots\n",
    "        for feature in features_pca:\n",
    "            combined_data[feature][condition] = (df[\"time (h)\"], df[feature])\n",
    "    \n",
    "    # Generate combined plots\n",
    "    for feature in features_pca:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for condition, (time, values) in combined_data[feature].items():\n",
    "            plt.plot(time, values, marker='o', linestyle='-', markersize=1, linewidth=1, label=condition)\n",
    "        \n",
    "        plt.xlabel('Time (hours)')\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(f\"Combined PIV - {feature} across Conditions\")\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(combined_output_dir, f\"combined_{feature.split()[0]}_h.jpg\"), format='jpg', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for condition, (time, values) in combined_data[feature].items():\n",
    "            time_in_min = time * 60  # Convert hours to minutes for the second plot\n",
    "            plt.plot(time_in_min, values, marker='o', linestyle='-', markersize=1, linewidth=1, label=condition)\n",
    "        \n",
    "        plt.xlabel('Time (minutes)')\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(f\"Combined PIV - {feature} across Conditions\")\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(combined_output_dir, f\"combined_{feature.split()[0]}_min.jpg\"), format='jpg', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_PIV_features_all_conditions_subconditions(data_path, conditions, subconditions, features_pca, sigma=2, min_frame=0, max_frame=None):\n",
    "    \"\"\"\n",
    "    Generate PIV plots that display all conditions and subconditions together on the same plot for each feature.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions (list): List of conditions.\n",
    "        subconditions (list): List of subconditions for each condition.\n",
    "        features_pca (list): List of features to include in plotting.\n",
    "        sigma (float): The standard deviation for Gaussian kernel applied for smoothing.\n",
    "        min_frame (int): The minimum frame to include in the analysis.\n",
    "        max_frame (int): The maximum frame to include in the analysis.\n",
    "    \"\"\"\n",
    "    # Prepare output directory for combined plots\n",
    "    combined_output_dir = os.path.join(data_path, \"PIV_plots\", \"all_conditions_subconditions\")\n",
    "    os.makedirs(combined_output_dir, exist_ok=True)\n",
    "    \n",
    "    for feature in features_pca:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for condition in conditions:\n",
    "            for subcondition in subconditions:\n",
    "                # Path to the subcondition data\n",
    "                subcondition_data_path = os.path.join(data_path, condition, subcondition, \"dataframes_PIV\", \"mean_values.csv\")\n",
    "                \n",
    "                if not os.path.exists(subcondition_data_path):\n",
    "                    print(f\"Error: Data file {subcondition_data_path} does not exist.\")\n",
    "                    continue\n",
    "                \n",
    "                # Load the subcondition dataframe\n",
    "                df = pd.read_csv(subcondition_data_path)\n",
    "                \n",
    "                # Apply Gaussian smoothing\n",
    "                df.iloc[:, 1:-3] = df.iloc[:, 1:-3].apply(lambda x: gaussian_filter1d(x, sigma=sigma))\n",
    "                \n",
    "                # Rename columns for consistency in plotting\n",
    "                df = df.rename(columns={\"data type [-]_mean\": \"work [J]\", \n",
    "                                        \"correlation length [m]_mean\": \"correlation length [um]\", \n",
    "                                        \"velocity magnitude [m/s]_mean\": \"velocity magnitude [um/s]\"})\n",
    "                \n",
    "                df[\"work [J]\"] = df[\"power [W]_mean\"].cumsum()\n",
    "                \n",
    "                # Limit the frames if specified\n",
    "                df = df.iloc[min_frame:max_frame, :]\n",
    "                \n",
    "                # Plot each subcondition on the same figure\n",
    "                plt.plot(df[\"time (h)\"], df[feature], marker='o', linestyle='-', markersize=1, linewidth=1, label=f'{condition} - {subcondition}')\n",
    "        \n",
    "        plt.xlabel('Time (hours)')\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(f\"All Conditions and Subconditions Combined - {feature}\")\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        plt.legend(loc='best', fontsize='small', ncol=2)  # Adjust legend to fit all entries\n",
    "        plt.savefig(os.path.join(combined_output_dir, f\"combined_{feature.split()[0]}_h.jpg\"), format='jpg', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for condition in conditions:\n",
    "            for subcondition in subconditions:\n",
    "                # Load the subcondition dataframe again\n",
    "                df = pd.read_csv(os.path.join(data_path, condition, subcondition, \"dataframes_PIV\", \"mean_values.csv\"))\n",
    "                df.iloc[:, 1:-3] = df.iloc[:, 1:-3].apply(lambda x: gaussian_filter1d(x, sigma=sigma))\n",
    "                df = df.rename(columns={\"data type [-]_mean\": \"work [J]\", \n",
    "                                        \"correlation length [m]_mean\": \"correlation length [um]\", \n",
    "                                        \"velocity magnitude [m/s]_mean\": \"velocity magnitude [um/s]\"})\n",
    "                df[\"work [J]\"] = df[\"power [W]_mean\"].cumsum()\n",
    "                df = df.iloc[min_frame:max_frame, :]\n",
    "                \n",
    "                time_in_min = df[\"time (h)\"] * 60  # Convert hours to minutes for the second plot\n",
    "                plt.plot(time_in_min, df[feature], marker='o', linestyle='-', markersize=1, linewidth=1, label=f'{condition} - {subcondition}')\n",
    "        \n",
    "        plt.xlabel('Time (minutes)')\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(f\"All Conditions and Subconditions Combined - {feature}\")\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        plt.legend(loc='best', fontsize='small', ncol=2)  # Adjust legend to fit all entries\n",
    "        plt.savefig(os.path.join(combined_output_dir, f\"combined_{feature.split()[0]}_min.jpg\"), format='jpg', dpi=300)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions: ['kif3-beads1in10', 'kif3-beads1in100']\n",
      "Subconditions: ['Rep1']\n",
      "Moved .tif files from ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1/kif3-beads1in10/Rep1 to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1/kif3-beads1in10/Rep1/original\n",
      "Moved .tif files from ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1/kif3-beads1in100/Rep1 to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1/kif3-beads1in100/Rep1/original\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "calibration_curve_paths = sorted(glob.glob(\"../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/calibration_curve/***ugml.tif\"))\n",
    "\n",
    "data_path = \"../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/test/3ulTMB-0p5ulDNA_all50nM_/\"\n",
    "conditions, subconditions = prepare_conditions(data_path)\n",
    "\n",
    "print(\"Conditions:\", conditions)\n",
    "print(\"Subconditions:\", subconditions)\n",
    "reorgTiffsToOriginal(data_path, conditions, subconditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied every 3th 'DAPI' file from ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1/kif3-beads1in10/Rep1/original into ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1/kif3-beads1in10/Rep1/dapi-3x.\n",
      "Copied every 3th 'DAPI' file from ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1/kif3-beads1in100/Rep1/original into ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1/kif3-beads1in100/Rep1/dapi-3x.\n"
     ]
    }
   ],
   "source": [
    "reorgTiffs_Split_dapi(data_path, conditions, subconditions, file_interval=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature limits and other parameters\n",
    "v = 2E-7\n",
    "velocity_limits = (0, v)\n",
    "other_limits = (-0.0005, 0.0005)\n",
    "time_interval_list = [6]  # time intervals in seconds between frames for each condition\n",
    "skip_frames = 4 ### CHANGE THIS TO SKIP FRAMES\n",
    " \n",
    "\n",
    "velocity_limits = (None, None)\n",
    "other_limits = (None, None)\n",
    "\n",
    "\n",
    "feature_limits = {\n",
    "    # 'u [m/s]': (-v, v), \n",
    "    # 'v [m/s]': (-v, v), \n",
    "    # 'data type [-]': (None, None),\n",
    "    'velocity magnitude [m/s]': velocity_limits,\n",
    "    'vorticity [1/s]': other_limits,\n",
    "    'divergence [1/s]': other_limits,\n",
    "    # 'dcev [1]': (0, 250),\n",
    "    'shear [1/s]': other_limits,\n",
    "    'strain [1/s]': other_limits,\n",
    "    'vector direction [degrees]': (-180, 180),\n",
    "}\n",
    "\n",
    "\n",
    "# Features for PCA and plotting\n",
    "features_pca = [\n",
    "    \"vorticity [1/s]_mean\",\n",
    "    \"velocity magnitude [um/s]\",\n",
    "    \"divergence [1/s]_mean\",\n",
    "    \"shear [1/s]_mean\",\n",
    "    \"strain [1/s]_mean\",\n",
    "    \"correlation length [um]\", \n",
    "    \"power [W]_mean\",\n",
    "    \"work [J]\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PIV data:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1//kif3/Rep1/heatmaps_PIV/velocity.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1//kif3/Rep1/heatmaps_PIV/vorticity.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1//kif3/Rep1/heatmaps_PIV/divergence.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1//kif3/Rep1/heatmaps_PIV/shear.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1//kif3/Rep1/heatmaps_PIV/strain.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing PIV data: 100%|██████████| 1/1 [04:08<00:00, 248.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/microscope/082624-dapibeads-kif3/4ultxtl-1ulMT-1ulDNA50nM-1ulbeads1in10_1//kif3/Rep1/heatmaps_PIV/vector.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process PIV data\n",
    "process_piv_data(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    feature_limits, \n",
    "    time_interval_list, \n",
    "    min_frame=0, \n",
    "    max_frame=None, \n",
    "    skip_frames=skip_frames, \n",
    "    plot_autocorrelation=False, \n",
    "    frame_rate=1, \n",
    "    heatmaps=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot features and PCA\n",
    "plot_PIV_features(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    features_pca, \n",
    "    min_frame=0, \n",
    "    max_frame=None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot features and PCA\n",
    "plot_PIV_features_averaged(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    features_pca, \n",
    "    min_frame=0, \n",
    "    max_frame=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot features and PCA\n",
    "plot_PIV_features_combined(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    features_pca, \n",
    "    min_frame=0, \n",
    "    max_frame=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot features and PCA\n",
    "plot_PIV_features_all_conditions_subconditions(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    features_pca, \n",
    "    min_frame=0, \n",
    "    max_frame=None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_outputs(data_path, conditions, subconditions, output_dirs=None):\n",
    "    \"\"\"\n",
    "    Deletes all output files and directories for the given conditions and subconditions.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Base directory for PIV data and output.\n",
    "        conditions (list): List of conditions.\n",
    "        subconditions (list): List of subconditions.\n",
    "        output_dirs (list, optional): Specific output directories to delete. If None, delete all known output directories.\n",
    "    \"\"\"\n",
    "    # Default output directories to remove\n",
    "    if output_dirs is None:\n",
    "        output_dirs = [\n",
    "            \"piv_movie_converted\",\n",
    "            \"autocorrelation_plots\",\n",
    "            \"dataframes_PIV\",\n",
    "            \"heatmaps_PIV\",\n",
    "            \"plots_PIV\",\n",
    "            \"averaged\",\n",
    "            os.path.join(\"PIV_plots\", \"averaged_conditions\"),\n",
    "            os.path.join(\"PIV_plots\", \"all_conditions_subconditions\"),\n",
    "            \"combined_PIV_plots\",\n",
    "            \"all_conditions_subconditions_combined_plots\"\n",
    "        ]\n",
    "\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            for output_dir in output_dirs:\n",
    "                dir_path = os.path.join(data_path, condition, subcondition, output_dir)\n",
    "                if os.path.exists(dir_path):\n",
    "                    try:\n",
    "                        shutil.rmtree(dir_path)\n",
    "                        print(f\"Deleted directory: {dir_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error deleting directory {dir_path}: {e}\")\n",
    "\n",
    "        # Remove the averaged directory at the condition level\n",
    "        averaged_dir = os.path.join(data_path, condition, \"averaged\")\n",
    "        if os.path.exists(averaged_dir):\n",
    "            try:\n",
    "                shutil.rmtree(averaged_dir)\n",
    "                print(f\"Deleted directory: {averaged_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting directory {averaged_dir}: {e}\")\n",
    "\n",
    "    # Remove the combined plots directories at the top level\n",
    "    combined_dirs = [\n",
    "        \"combined_PIV_plots\",\n",
    "        os.path.join(\"PIV_plots\", \"averaged_conditions\"),\n",
    "        os.path.join(\"PIV_plots\", \"all_conditions_subconditions\")\n",
    "    ]\n",
    "\n",
    "    for combined_dir in combined_dirs:\n",
    "        combined_dir_path = os.path.join(data_path, combined_dir)\n",
    "        if os.path.exists(combined_dir_path):\n",
    "            try:\n",
    "                shutil.rmtree(combined_dir_path)\n",
    "                print(f\"Deleted directory: {combined_dir_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting directory {combined_dir_path}: {e}\")\n",
    "\n",
    "    # Remove the PIV_plots directory in the data_path\n",
    "    piv_plots_dir = os.path.join(data_path, \"PIV_plots\")\n",
    "    if os.path.exists(piv_plots_dir):\n",
    "        try:\n",
    "            shutil.rmtree(piv_plots_dir)\n",
    "            print(f\"Deleted directory: {piv_plots_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting directory {piv_plots_dir}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "delete_outputs(data_path, conditions, subconditions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
