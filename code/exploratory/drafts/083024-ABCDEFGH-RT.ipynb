{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File management\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import norm\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "\n",
    "# Utilities\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, cpu_count\n",
    "mp.set_start_method('fork', force=True)\n",
    "from ipywidgets import interact, FloatSlider, Layout, interactive\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import cv2\n",
    "from natsort import natsorted\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d  # Import for Gaussian smoothing\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def consolidate_images(base_dir):\n",
    "    # Dynamically list all experimental folders using glob\n",
    "    folders = [f for f in glob.glob(os.path.join(base_dir, '*/')) if os.path.isdir(f)]\n",
    "    folders.sort()  # Optional: sort to ensure consistent processing order\n",
    "\n",
    "    # Derive the new directory name from the common prefix\n",
    "    common_prefix = os.path.commonprefix([os.path.basename(os.path.normpath(f)) for f in folders])\n",
    "    new_dir = os.path.join(base_dir, common_prefix)\n",
    "\n",
    "    # Check if the new directory already exists and has more than one subfolder\n",
    "    if os.path.exists(new_dir) and len([f for f in os.listdir(new_dir) if os.path.isdir(os.path.join(new_dir, f))]) > 1:\n",
    "        print(f\"Consolidation appears to be already done. Directory '{new_dir}' already exists with subfolders.\")\n",
    "        return\n",
    "\n",
    "    # Create the new directory for consolidated images\n",
    "    os.makedirs(new_dir, exist_ok=True)\n",
    "\n",
    "    # Dynamically list all Pos folders from the first experimental folder\n",
    "    first_folder_path = folders[0]\n",
    "    pos_folders = [d for d in os.listdir(first_folder_path) if os.path.isdir(os.path.join(first_folder_path, d))]\n",
    "    pos_folders.sort()  # Optional: sort to ensure consistent processing order\n",
    "\n",
    "    # Create subfolders for each Pos\n",
    "    for pos in pos_folders:\n",
    "        pos_folder_path = os.path.join(new_dir, pos)\n",
    "        os.makedirs(pos_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize counters for Cy5, GFP, DAPI, Brightfield images, metadata, and display_and_comments.txt files\n",
    "    cy5_counter = {pos: 0 for pos in pos_folders}\n",
    "    gfp_counter = {pos: 0 for pos in pos_folders}\n",
    "    dapi_counter = {pos: 0 for pos in pos_folders}\n",
    "    brightfield_counter = {pos: 0 for pos in pos_folders}  # New brightfield counter\n",
    "    metadata_counter = {pos: 0 for pos in pos_folders}\n",
    "    comments_counter = 0\n",
    "\n",
    "    # Function to count images in a folder\n",
    "    def count_images(folder_path):\n",
    "        if not os.path.exists(folder_path):\n",
    "            return 0\n",
    "        return len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "\n",
    "    # Move images, metadata, and display_and_comments.txt, and update counters\n",
    "    for folder in folders:\n",
    "        for pos in pos_folders:\n",
    "            current_pos_folder_path = os.path.join(folder, pos)\n",
    "            \n",
    "            if not os.path.exists(current_pos_folder_path):\n",
    "                print(f\"Warning: {current_pos_folder_path} does not exist.\")\n",
    "                continue\n",
    "            \n",
    "            images = sorted(os.listdir(current_pos_folder_path))\n",
    "            \n",
    "            for image in images:\n",
    "                old_image_path = os.path.join(current_pos_folder_path, image)\n",
    "                image_lower = image.lower()  # Make the image name lowercase\n",
    "                \n",
    "                if 'cy5' in image_lower:\n",
    "                    prefix, ext = os.path.splitext(image)\n",
    "                    parts = prefix.split('_')\n",
    "                    new_image_name = f'img_{cy5_counter[pos]:09d}_{parts[2]}_{parts[3]}{ext}'\n",
    "                    cy5_counter[pos] += 1\n",
    "                elif 'gfp' in image_lower:\n",
    "                    prefix, ext = os.path.splitext(image)\n",
    "                    parts = prefix.split('_')\n",
    "                    new_image_name = f'img_{gfp_counter[pos]:09d}_{parts[2]}_{parts[3]}{ext}'\n",
    "                    gfp_counter[pos] += 1\n",
    "                elif 'dapi' in image_lower:\n",
    "                    prefix, ext = os.path.splitext(image)\n",
    "                    parts = prefix.split('_')\n",
    "                    new_image_name = f'img_{dapi_counter[pos]:09d}_{parts[2]}_{parts[3]}{ext}'\n",
    "                    dapi_counter[pos] += 1\n",
    "                elif 'brightfield' in image_lower:  # New brightfield handling\n",
    "                    prefix, ext = os.path.splitext(image)\n",
    "                    parts = prefix.split('_')\n",
    "                    new_image_name = f'img_{brightfield_counter[pos]:09d}_{parts[2]}_{parts[3]}{ext}'\n",
    "                    brightfield_counter[pos] += 1\n",
    "                elif image == 'metadata.txt':\n",
    "                    # Move and rename metadata.txt to avoid overwriting\n",
    "                    new_image_name = f'metadata_{metadata_counter[pos]:03d}.txt'\n",
    "                    metadata_counter[pos] += 1\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                new_image_path = os.path.join(new_dir, pos, new_image_name)\n",
    "                \n",
    "                try:\n",
    "                    shutil.move(old_image_path, new_image_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error moving {old_image_path} to {new_image_path}: {e}\")\n",
    "\n",
    "            # Move display_and_comments.txt and rename it\n",
    "            comments_file_path = os.path.join(folder, 'display_and_comments.txt')\n",
    "            if os.path.exists(comments_file_path):\n",
    "                new_comments_path = os.path.join(new_dir, f'display_and_comments_{comments_counter:03d}.txt')\n",
    "                comments_counter += 1\n",
    "                try:\n",
    "                    shutil.move(comments_file_path, new_comments_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error moving {comments_file_path} to {new_comments_path}: {e}\")\n",
    "\n",
    "            # If the Pos folder is empty after moving images, delete it\n",
    "            if not os.listdir(current_pos_folder_path):\n",
    "                try:\n",
    "                    os.rmdir(current_pos_folder_path)\n",
    "                    print(f\"Deleted empty folder: {current_pos_folder_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error deleting folder {current_pos_folder_path}: {e}\")\n",
    "\n",
    "        # If the main folder is empty after moving the display_and_comments.txt file, delete it\n",
    "        if not os.listdir(folder):\n",
    "            try:\n",
    "                os.rmdir(folder)\n",
    "                print(f\"Deleted empty folder: {folder}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting folder {folder}: {e}\")\n",
    "\n",
    "    # Check and count images in final folders\n",
    "    print(\"\\nChecking final consolidated folders:\")\n",
    "    for pos in pos_folders:\n",
    "        pos_folder_path = os.path.join(new_dir, pos)\n",
    "        count = count_images(pos_folder_path)\n",
    "        print(f\"Images in final {pos_folder_path}: {count}\")\n",
    "\n",
    "    print(\"Renaming, moving, and cleanup completed.\")\n",
    "\n",
    "\n",
    "\n",
    "def organize_conditions(data_path, conditions_dict):\n",
    "    \"\"\"\n",
    "    Organizes PosX folders into condition folders as specified by conditions_dict.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions_dict (dict): Dictionary where keys are condition names and values are lists of PosX folders.\n",
    "    \"\"\"\n",
    "    for condition, pos_folders in conditions_dict.items():\n",
    "        # Create condition folder if it doesn't exist\n",
    "        condition_path = os.path.join(data_path, condition)\n",
    "        os.makedirs(condition_path, exist_ok=True)\n",
    "        \n",
    "        # Ensure pos_folders is a list, even if only one PosX is provided\n",
    "        if isinstance(pos_folders, str):\n",
    "            pos_folders = [pos_folders]\n",
    "        \n",
    "        # Move PosX folders into the condition folder\n",
    "        for pos_folder in pos_folders:\n",
    "            src_path = os.path.join(data_path, pos_folder)\n",
    "            dest_path = os.path.join(condition_path, pos_folder)\n",
    "            \n",
    "            if os.path.exists(src_path):\n",
    "                shutil.move(src_path, dest_path)\n",
    "            else:\n",
    "                print(f\"Warning: {src_path} does not exist. Skipping.\")\n",
    "\n",
    "def reorgTiffsToOriginal(data_path, conditions, subconditions):\n",
    "    \"\"\"\n",
    "    Renames subconditions as RepX and moves the raw data to the \"original\" folder.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions (list): List of conditions.\n",
    "        subconditions (list): List of subconditions.\n",
    "    \"\"\"\n",
    "    for condition in conditions:\n",
    "        # Get the actual subconditions in the directory\n",
    "        actual_subconditions = [name for name in os.listdir(os.path.join(data_path, condition)) if os.path.isdir(os.path.join(data_path, condition, name))]\n",
    "        \n",
    "        # Ensure subconditions list matches the number of actual subconditions\n",
    "        actual_subconditions.sort()\n",
    "        matched_subconditions = subconditions[:len(actual_subconditions)]\n",
    "        \n",
    "        # Rename the actual subconditions to match the subconditions in your list\n",
    "        for i, actual_subcondition in enumerate(actual_subconditions):\n",
    "            os.rename(os.path.join(data_path, condition, actual_subcondition), os.path.join(data_path, condition, matched_subconditions[i]))\n",
    "        \n",
    "        for subcondition in matched_subconditions:\n",
    "            # Construct the path to the subcondition directory\n",
    "            subcondition_path = os.path.join(data_path, condition, subcondition)\n",
    "            \n",
    "            # Create the path for the \"original\" directory within the subcondition directory\n",
    "            original_dir_path = os.path.join(subcondition_path, \"original\")\n",
    "            \n",
    "            # Always create the \"original\" directory\n",
    "            os.makedirs(original_dir_path, exist_ok=True)\n",
    "            \n",
    "            # Iterate over all files in the subcondition directory\n",
    "            for filename in os.listdir(subcondition_path):\n",
    "                # Check if the file is a .tif file\n",
    "                if filename.endswith(\".tif\"):\n",
    "                    # Construct the full path to the file\n",
    "                    file_path = os.path.join(subcondition_path, filename)\n",
    "                    \n",
    "                    # Construct the path to move the file to\n",
    "                    destination_path = os.path.join(original_dir_path, filename)\n",
    "                    \n",
    "                    # Move the file to the \"original\" directory\n",
    "                    shutil.move(file_path, destination_path)\n",
    "            print(f\"Moved .tif files from {subcondition_path} to {original_dir_path}\")\n",
    "\n",
    "def prepare_conditions(data_path):\n",
    "    \"\"\"\n",
    "    Prepares conditions and subconditions, renaming subconditions to 'RepX'.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "    \n",
    "    Returns:\n",
    "        conditions (list): List of condition names.\n",
    "        subconditions (list): List of renamed subconditions as 'RepX'.\n",
    "    \"\"\"\n",
    "    # List conditions while ignoring 'output_data'\n",
    "    conditions = natsorted([\n",
    "        f for f in os.listdir(data_path) \n",
    "        if os.path.isdir(os.path.join(data_path, f)) and f != 'output_data'\n",
    "    ])\n",
    "    \n",
    "    # Determine the maximum number of subconditions across all conditions\n",
    "    max_num_subconditions = max([\n",
    "        len([\n",
    "            f for f in os.listdir(os.path.join(data_path, condition)) \n",
    "            if os.path.isdir(os.path.join(data_path, condition, f))\n",
    "        ])\n",
    "        for condition in conditions\n",
    "    ])\n",
    "    \n",
    "    # Rename subconditions to 'RepX' where X is the index (1-based)\n",
    "    subconditions = [f'Rep{i+1}' for i in range(max_num_subconditions)]\n",
    "    \n",
    "    return conditions, subconditions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def reorgTiffs_Split_dapi(data_path, conditions, subconditions, file_interval=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions (list): List of conditions.\n",
    "        subconditions (list): List of subconditions.\n",
    "        file_interval (int or None): Option to copy every nth file. If None, this feature is not used.\n",
    "\n",
    "    This function copies 'DAPI' images from the 'original' folder into\n",
    "    the 'DAPI' folder, using the specified interval.\n",
    "    \"\"\"\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            # Construct the path to the 'original' directory within the subcondition\n",
    "            original_dir_path = os.path.join(data_path, condition, subcondition, \"original\")\n",
    "\n",
    "            if not os.path.exists(original_dir_path):\n",
    "                print(f\"Error: The original directory {original_dir_path} does not exist.\")\n",
    "                continue\n",
    "\n",
    "            # Create the directory for the DAPI channel\n",
    "            dapi_dir = os.path.join(data_path, condition, subcondition, f\"dapi-{file_interval}x\")\n",
    "            os.makedirs(dapi_dir, exist_ok=True)\n",
    "\n",
    "            # Check if the expected output is already there\n",
    "            expected_files = [f for f in sorted(os.listdir(original_dir_path))\n",
    "                              if f.lower().endswith(\".tif\") and \"dapi\" in f.lower()]\n",
    "            expected_output_files = expected_files[::file_interval or 1]\n",
    "            already_copied_files = set(os.listdir(dapi_dir))\n",
    "\n",
    "            # If all expected files are already copied, skip this subcondition\n",
    "            if all(file in already_copied_files for file in expected_output_files):\n",
    "                print(f\"Skipping {subcondition} as the expected output is already present.\")\n",
    "                continue\n",
    "\n",
    "            # Separate list for DAPI files\n",
    "            dapi_files = []\n",
    "\n",
    "            # Iterate over all files in the original directory\n",
    "            file_list = sorted(os.listdir(original_dir_path))\n",
    "            for filename in file_list:\n",
    "                # Check if the file is a .tif file and contains 'DAPI' (case insensitive)\n",
    "                if filename.lower().endswith(\".tif\") and \"dapi\" in filename.lower():\n",
    "                    dapi_files.append(filename)\n",
    "\n",
    "            # Copy files based on the file_interval\n",
    "            if file_interval is None:\n",
    "                file_interval = 1  # Copy all files if no interval is set\n",
    "\n",
    "            for idx, filename in enumerate(dapi_files):\n",
    "                if idx % file_interval == 0:\n",
    "                    file_path = os.path.join(original_dir_path, filename)\n",
    "                    shutil.copy(file_path, os.path.join(dapi_dir, filename))\n",
    "\n",
    "            print(f\"Copied every {file_interval}th 'DAPI' file from {original_dir_path} into {dapi_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidation appears to be already done. Directory '../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_' already exists with subfolders.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "base_dir = '../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/'\n",
    "consolidate_images(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/Pos0 does not exist. Skipping.\n",
      "Warning: ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/Pos1 does not exist. Skipping.\n",
      "Warning: ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/Pos2 does not exist. Skipping.\n",
      "Warning: ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/Pos3 does not exist. Skipping.\n",
      "Warning: ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/Pos4 does not exist. Skipping.\n",
      "Warning: ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/Pos5 does not exist. Skipping.\n",
      "Warning: ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/Pos6 does not exist. Skipping.\n",
      "Warning: ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/Pos7 does not exist. Skipping.\n",
      "Conditions: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'negative']\n",
      "Subconditions: ['Rep1']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "data_path = \"../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/\"\n",
    "\n",
    "calibration_curve_paths = sorted(glob.glob(\"../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/calibration_curve/***ugml.tif\"))\n",
    "\n",
    "\n",
    "conditions_dict = {\n",
    "    \"A\": \"Pos0\", \n",
    "    \"B\": \"Pos1\",\n",
    "    \"C\": \"Pos2\",\n",
    "    \"D\": \"Pos3\",\n",
    "    \"E\": \"Pos4\", \n",
    "    \"F\": \"Pos5\",\n",
    "    \"G\": \"Pos6\",\n",
    "    \"H\": \"Pos7\"\n",
    "}\n",
    "\n",
    "# Organize PosX folders into condition folders\n",
    "organize_conditions(data_path, conditions_dict)\n",
    "\n",
    "# Now run the existing functions to reorganize the tiffs and rename the folders\n",
    "conditions, subconditions = prepare_conditions(data_path)\n",
    "time_interval_list = [45] * len(conditions)  # time intervals in seconds between frames for each condition\n",
    "\n",
    "print(\"Conditions:\", conditions)\n",
    "print(\"Subconditions:\", subconditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved .tif files from ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/A/Rep1 to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/A/Rep1/original\n",
      "Moved .tif files from ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/B/Rep1 to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/B/Rep1/original\n",
      "Moved .tif files from ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/C/Rep1 to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/C/Rep1/original\n",
      "Moved .tif files from ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/D/Rep1 to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/D/Rep1/original\n",
      "Moved .tif files from ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/E/Rep1 to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/E/Rep1/original\n",
      "Moved .tif files from ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/F/Rep1 to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/F/Rep1/original\n",
      "Moved .tif files from ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/G/Rep1 to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/G/Rep1/original\n",
      "Moved .tif files from ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/H/Rep1 to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/H/Rep1/original\n",
      "Moved .tif files from ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/negative/Rep1 to ../../../../../../mnt/c/Users/Admin/Thomson Lab Dropbox/David Larios/activedrops/main/083024-ABCDEFGH-RT/3ulTMB-0p5ulDNA_all50nM_/negative/Rep1/original\n"
     ]
    }
   ],
   "source": [
    "reorgTiffsToOriginal(data_path, conditions, subconditions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fluorescence analysis and video creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ensure_output_dir(output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "def calculate_mean_intensity(path):\n",
    "    \"\"\"Calculate mean intensity of an image.\"\"\"\n",
    "    return io.imread(path).mean()\n",
    "\n",
    "def calculate_protein_concentration(mean_intensity, intercept, slope):\n",
    "    \"\"\"Calculate protein concentration in ng/ul and nM.\"\"\"\n",
    "    conc_ng_ul = (mean_intensity - intercept) / slope\n",
    "    return conc_ng_ul\n",
    "\n",
    "def calculate_protein_concentration_nM(conc_ng_ul, mw_kda):\n",
    "    \"\"\"Convert protein concentration from ng/ul to nM.\"\"\"\n",
    "    conc_nM = (conc_ng_ul * 1e-3) / (mw_kda * 1e3) * 1e9\n",
    "    return conc_nM\n",
    "\n",
    "def calculate_number_of_protein_molecules(protein_mass, mw_kda):\n",
    "    \"\"\"Calculate number of protein molecules.\"\"\"\n",
    "    return (protein_mass * 6e14) / (mw_kda * 1e3)\n",
    "\n",
    "def convert_time_units(time_values_s):\n",
    "    \"\"\"Convert time values from seconds to minutes and hours.\"\"\"\n",
    "    time_values_min = time_values_s / 60\n",
    "    time_values_h = time_values_s / 3600\n",
    "    return time_values_s, time_values_min, time_values_h\n",
    "\n",
    "def process_image(args):\n",
    "    image_file, output_directory_path, channel, slope, intercept, vmax, time_interval, i, show_scalebar, min_frame, skip_frames, condition, subcondition = args\n",
    "    # Read the image into a numpy array\n",
    "    intensity_matrix = io.imread(image_file)\n",
    "\n",
    "    if channel == \"cy5\":\n",
    "        # Normalize intensity matrix to range [0, 1] for cy5 channel\n",
    "        matrix_to_plot = intensity_matrix / 1000\n",
    "        label = 'Normalized Fluorescence Intensity'\n",
    "    else:\n",
    "        # Convert intensity values to protein concentration using the calibration curve\n",
    "        matrix_to_plot = calculate_protein_concentration(intensity_matrix, slope, intercept)\n",
    "        matrix_to_plot = matrix_to_plot / 27000 * 1E6\n",
    "        label = 'Protein concentration (nM)'\n",
    "\n",
    "    # Plot the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    im = ax.imshow(matrix_to_plot, cmap='gray', interpolation='nearest', vmin=0, vmax=vmax)\n",
    "\n",
    "    if show_scalebar:\n",
    "        plt.colorbar(im, ax=ax, label=label)\n",
    "    plt.title(f\"Time (min): {(i - min_frame) * time_interval * skip_frames / 60:.2f} \\nTime (h): {(i - min_frame) * time_interval * skip_frames / 3600:.2f} \\n{condition} - {subcondition} - {channel}\", fontsize=20)\n",
    "    plt.xlabel('x [µm]')\n",
    "    plt.ylabel('y [µm]')\n",
    "    plt.grid(True, color='#d3d3d3', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "    # Save the heatmap\n",
    "    heatmap_filename = f\"heatmap_frame_{i}.png\"\n",
    "    heatmap_path = os.path.join(output_directory_path, heatmap_filename)\n",
    "    plt.savefig(heatmap_path, bbox_inches='tight', pad_inches=0.1, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "def fluorescence_heatmap(data_path, conditions, subconditions, channel, time_interval_list, vmax, min_frame, max_frame=None, skip_frames=1, calibration_curve_paths=None, show_scalebar=True, batch_size=100):\n",
    "    \"\"\"\n",
    "    Reads each image as a matrix, creates, and saves a heatmap representing the normalized pixel-wise fluorescence intensity.\n",
    "\n",
    "    Args:\n",
    "    - data_path (str): Base directory where the images are stored.\n",
    "    - conditions (list): List of conditions defining subdirectories within the data path.\n",
    "    - subconditions (list): List of subconditions defining further subdirectories.\n",
    "    - channel (str): Channel specifying the fluorescence ('cy5' or 'gfp').\n",
    "    - time_interval_list (list): List of time intervals in seconds between frames for each condition.\n",
    "    - min_frame (int): Minimum frame number to start processing from.\n",
    "    - max_frame (int): Maximum frame number to stop processing at.\n",
    "    - vmax (float): Maximum value for color scale in the heatmap.\n",
    "    - skip_frames (int): Interval to skip frames (default is 1, meaning process every frame).\n",
    "    - calibration_curve_paths (list): List of file paths for the calibration curve images.\n",
    "    - show_scalebar (bool): Whether to show the color scale bar in the heatmap.\n",
    "    - batch_size (int): Number of images to process in each batch to avoid memory overload.\n",
    "    \"\"\"\n",
    "    output_data_dir = os.path.join(data_path, \"output_data\", \"movies\")\n",
    "    ensure_output_dir(output_data_dir)\n",
    "\n",
    "    for idx, condition in enumerate(conditions):\n",
    "        time_interval = time_interval_list[idx]\n",
    "\n",
    "        for subcondition in subconditions:\n",
    "            # Determine the directory paths based on the channel\n",
    "            input_directory_path = os.path.join(data_path, condition, subcondition, \"original\")\n",
    "            output_directory_path = os.path.join(output_data_dir, f\"{condition}_{subcondition}_heatmaps_{channel}\")\n",
    "\n",
    "            # Create the output directory if it doesn't exist, or clear it if it does\n",
    "            if os.path.exists(output_directory_path):\n",
    "                shutil.rmtree(output_directory_path)\n",
    "            os.makedirs(output_directory_path, exist_ok=True)\n",
    "\n",
    "            # Get all .tif files in the folder\n",
    "            image_files = sorted(glob.glob(os.path.join(input_directory_path, f\"*{channel}*.tif\")))[min_frame:max_frame:skip_frames]\n",
    "\n",
    "            # Setup calibration curve for non-cy5 channels\n",
    "            slope, intercept = None, None\n",
    "            if channel != \"cy5\":\n",
    "                # Calibration curve data and fit\n",
    "                sample_concentration_values = [0, 2, 5, 10, 20, 40, 80, 160, 320]\n",
    "\n",
    "                if calibration_curve_paths is None or len(calibration_curve_paths) != len(sample_concentration_values):\n",
    "                    raise ValueError(f\"Mismatch in lengths: {len(calibration_curve_paths)} calibration images, {len(sample_concentration_values)} sample concentrations\")\n",
    "\n",
    "                mean_intensity_calibration = [calculate_mean_intensity(path) for path in calibration_curve_paths]\n",
    "                slope, intercept = np.polyfit(sample_concentration_values, mean_intensity_calibration, 1)\n",
    "\n",
    "            # Progress bar for the entire subcondition\n",
    "            with tqdm(total=len(image_files), desc=f\"Processing {condition} - {subcondition}\", leave=True, dynamic_ncols=True) as pbar:\n",
    "                # Process images in batches to avoid memory overload\n",
    "                for batch_start in range(0, len(image_files), batch_size):\n",
    "                    batch_end = min(batch_start + batch_size, len(image_files))\n",
    "                    batch_files = image_files[batch_start:batch_end]\n",
    "\n",
    "                    # Prepare arguments for multiprocessing\n",
    "                    args = [(image_file, output_directory_path, channel, slope, intercept, vmax, time_interval, i, show_scalebar, min_frame, skip_frames, condition, subcondition)\n",
    "                            for i, image_file in enumerate(batch_files, start=batch_start + min_frame)]\n",
    "\n",
    "                    with mp.Pool(mp.cpu_count()) as pool:\n",
    "                        for _ in pool.imap(process_image, args):\n",
    "                            pbar.update(1)\n",
    "\n",
    "def prepare_conditions(data_path):\n",
    "    \"\"\"\n",
    "    Prepare conditions and subconditions, renaming subconditions to 'RepX' where X is the index.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "    \n",
    "    Returns:\n",
    "        conditions (list): List of condition names.\n",
    "        subconditions (list): List of renamed subconditions as 'RepX'.\n",
    "    \"\"\"\n",
    "    # List conditions while ignoring 'output_data'\n",
    "    conditions = natsorted([\n",
    "        f for f in os.listdir(data_path) \n",
    "        if os.path.isdir(os.path.join(data_path, f)) and f != 'output_data'\n",
    "    ])\n",
    "    \n",
    "    # Determine the number of subconditions by counting directories in the first condition\n",
    "    num_subconditions = len([\n",
    "        f for f in os.listdir(os.path.join(data_path, conditions[0])) \n",
    "        if os.path.isdir(os.path.join(data_path, conditions[0], f))\n",
    "    ])\n",
    "    \n",
    "    # Rename subconditions to 'RepX' where X is the index (1-based)\n",
    "    subconditions = [f'Rep{i+1}' for i in range(num_subconditions)]\n",
    "    \n",
    "    return conditions, subconditions\n",
    "\n",
    "def process_video_creation(args):\n",
    "    image_files, out_path, frame_rate = args\n",
    "\n",
    "    # Get the resolution of the first image (assuming all images are the same size)\n",
    "    first_image = cv2.imread(image_files[0])\n",
    "    video_resolution = (first_image.shape[1], first_image.shape[0])  # Width x Height\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    out = cv2.VideoWriter(out_path, fourcc, frame_rate, video_resolution)\n",
    "\n",
    "    for file in image_files:\n",
    "        img = cv2.imread(file)\n",
    "        out.write(img)  # Write the image as a frame in the video\n",
    "\n",
    "    out.release()\n",
    "\n",
    "def create_movies(data_path, conditions, subconditions, channel, frame_rate=30, max_frame=None, skip_frames=1, batch_size=100):\n",
    "    \"\"\"\n",
    "    Creates video files from heatmaps stored in the specified directory.\n",
    "\n",
    "    Args:\n",
    "    - data_path (str): Base path where the heatmaps are stored.\n",
    "    - conditions (list): List of conditions defining subdirectories within the data path.\n",
    "    - subconditions (list): List of subconditions defining further subdirectories.\n",
    "    - channel (str): The specific channel being processed ('cy5' or 'gfp').\n",
    "    - frame_rate (int): Frame rate for the output video. Defaults to 30.\n",
    "    - max_frame (int, optional): Maximum number of frames to be included in the video. If None, all frames are included.\n",
    "    - skip_frames (int): Interval to skip frames (default is 1, meaning process every frame).\n",
    "    - batch_size (int): Number of images to process in each batch to avoid memory overload.\n",
    "    \"\"\"\n",
    "    output_data_dir = os.path.join(data_path, \"output_data\", \"movies\")\n",
    "    ensure_output_dir(output_data_dir)\n",
    "\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            images_dir = os.path.join(output_data_dir, f\"{condition}_{subcondition}_heatmaps_{channel}\")\n",
    "            video_filename = f\"{condition}_{subcondition}_{channel}.avi\"\n",
    "            out_path = os.path.join(output_data_dir, video_filename)\n",
    "\n",
    "            # Get all .png files in the folder\n",
    "            image_files = natsorted(glob.glob(os.path.join(images_dir, \"*.png\")))[::skip_frames]\n",
    "            if max_frame is not None:\n",
    "                image_files = image_files[:max_frame]\n",
    "\n",
    "            if len(image_files) == 0:\n",
    "                print(f\"No images found for {condition} - {subcondition} in {channel}.\")\n",
    "                continue\n",
    "\n",
    "            # Calculate the video duration\n",
    "            video_duration = len(image_files) / frame_rate\n",
    "            print(f\"Creating video for {condition} - {subcondition} with duration: {video_duration:.2f} seconds.\")\n",
    "\n",
    "            # Get the resolution of the first image (assuming all images are the same size)\n",
    "            first_image = cv2.imread(image_files[0])\n",
    "            video_resolution = (first_image.shape[1], first_image.shape[0])  # Width x Height\n",
    "\n",
    "            # Define the codec and create the VideoWriter object\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "            out = cv2.VideoWriter(out_path, fourcc, frame_rate, video_resolution)\n",
    "\n",
    "            # Progress bar for the entire subcondition\n",
    "            with tqdm(total=len(image_files), desc=f\"Creating video for {condition} - {subcondition}\", leave=True, dynamic_ncols=True) as pbar:\n",
    "                # Process images in batches to avoid memory overload\n",
    "                for batch_start in range(0, len(image_files), batch_size):\n",
    "                    batch_end = min(batch_start + batch_size, len(image_files))\n",
    "                    batch_files = image_files[batch_start:batch_end]\n",
    "\n",
    "                    for image_file in batch_files:\n",
    "                        img = cv2.imread(image_file)\n",
    "                        out.write(img)  # Write the image as a frame in the video\n",
    "                        pbar.update(1)\n",
    "\n",
    "            # Release the video writer\n",
    "            out.release()\n",
    "\n",
    "def process_frame(args):\n",
    "    frame_index, temp_img_dir, conditions, subconditions, channel, grid_rows, grid_cols, data_path = args\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(grid_cols * 6, grid_rows * 6))\n",
    "    plt.subplots_adjust(hspace=0.1, wspace=0.1)  # Adjust spacing\n",
    "\n",
    "    # Ensure axes is always 2D\n",
    "    if grid_rows == 1 and grid_cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif grid_rows == 1 or grid_cols == 1:\n",
    "        axes = np.array(axes).reshape(grid_rows, grid_cols)\n",
    "\n",
    "    plot_index = 0\n",
    "\n",
    "    # Loop through each condition and subcondition\n",
    "    for col_idx, condition in enumerate(conditions):\n",
    "        for row_idx, subcondition in enumerate(subconditions):\n",
    "            # Determine the image path\n",
    "            images_dir = os.path.join(data_path, \"output_data\", \"movies\", f\"{condition}_{subcondition}_heatmaps_{channel}\")\n",
    "            image_files = natsorted(glob.glob(os.path.join(images_dir, \"*.png\")))\n",
    "\n",
    "            if frame_index < len(image_files):\n",
    "                # Use the available frame\n",
    "                image_path = image_files[frame_index]\n",
    "            else:\n",
    "                # If no more frames, use the last available frame\n",
    "                image_path = image_files[-1]\n",
    "\n",
    "            img = io.imread(image_path)\n",
    "\n",
    "            # Plot the image in the appropriate subplot\n",
    "            ax = axes[row_idx if len(subconditions) > 1 else plot_index // grid_cols,\n",
    "                      col_idx if len(subconditions) > 1 else plot_index % grid_cols]\n",
    "            ax.imshow(img, cmap='gray', vmin=0, vmax=img.max())\n",
    "            ax.axis('off')  # Remove axes\n",
    "\n",
    "            plot_index += 1\n",
    "\n",
    "    # Turn off any unused subplots\n",
    "    for ax in axes.flatten()[plot_index:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Save the combined frame\n",
    "    combined_image_path = os.path.join(temp_img_dir, f\"combined_frame_{frame_index:04d}.png\")\n",
    "    plt.savefig(combined_image_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "def create_combined_heatmap_movie_custom_grid(data_path, conditions, subconditions, channel, grid_rows=None, grid_cols=None, frame_rate=30, batch_size=50):\n",
    "    \"\"\"\n",
    "    Combines heatmaps from different conditions and subconditions into a single video.\n",
    "    Allows specifying the number of grid rows and columns or uses an adaptive layout based on subconditions.\n",
    "\n",
    "    Args:\n",
    "    - data_path (str): Base path where the heatmaps are stored.\n",
    "    - conditions (list): List of conditions defining subdirectories within the data path.\n",
    "    - subconditions (list): List of subconditions defining further subdirectories.\n",
    "    - channel (str): The specific channel being processed ('cy5' or 'gfp').\n",
    "    - grid_rows (int, optional): Number of rows in the grid. If None, calculated adaptively.\n",
    "    - grid_cols (int, optional): Number of columns in the grid. If None, calculated adaptively.\n",
    "    - frame_rate (int): Frame rate for the output video. Defaults to 30.\n",
    "    - batch_size (int): Number of frames to process in each batch to avoid memory overload.\n",
    "    \"\"\"\n",
    "    # Determine grid dimensions if not provided\n",
    "    total_plots = len(conditions) * len(subconditions)\n",
    "    \n",
    "    if grid_rows is None or grid_cols is None:\n",
    "        if len(subconditions) == 1:\n",
    "            grid_cols = int(np.ceil(np.sqrt(total_plots)))\n",
    "            grid_rows = int(np.ceil(total_plots / grid_cols))\n",
    "            while grid_cols * grid_rows >= total_plots:\n",
    "                if (grid_cols - 1) * grid_rows >= total_plots:\n",
    "                    grid_cols -= 1\n",
    "                elif grid_cols * (grid_rows - 1) >= total_plots:\n",
    "                    grid_rows -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            grid_rows = len(subconditions)\n",
    "            grid_cols = len(conditions)\n",
    "    \n",
    "    # Define the output directory for temporary images (now called 'combined_frames' in 'movies' directory)\n",
    "    output_data_dir = os.path.join(data_path, \"output_data\", \"movies\")\n",
    "    combined_frames_dir = os.path.join(output_data_dir, \"combined_frames\")\n",
    "    ensure_output_dir(combined_frames_dir)\n",
    "\n",
    "    # Determine the maximum number of frames based on the longest video\n",
    "    max_num_frames = 0\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            image_dir = os.path.join(data_path, \"output_data\", \"movies\", f\"{condition}_{subcondition}_heatmaps_{channel}\")\n",
    "            num_frames = len(natsorted(glob.glob(os.path.join(image_dir, \"*.png\"))))\n",
    "            if num_frames > max_num_frames:\n",
    "                max_num_frames = num_frames\n",
    "\n",
    "    if max_num_frames == 0:\n",
    "        print(f\"No frames to process. Check if the directories exist and contain images.\")\n",
    "        return\n",
    "\n",
    "    # Calculate and print the video duration\n",
    "    video_duration = max_num_frames / frame_rate\n",
    "    print(f\"Creating video with duration: {video_duration:.2f} seconds.\")\n",
    "\n",
    "    # Progress bar for the entire operation\n",
    "    with tqdm(total=max_num_frames, desc=\"Creating combined frames\", leave=True, dynamic_ncols=True) as pbar:\n",
    "        for batch_start in range(0, max_num_frames, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, max_num_frames)\n",
    "            batch_frames = range(batch_start, batch_end)\n",
    "\n",
    "            args_list = [\n",
    "                (\n",
    "                    frame_index, combined_frames_dir, conditions, subconditions, channel,\n",
    "                    grid_rows, grid_cols, data_path\n",
    "                )\n",
    "                for frame_index in batch_frames\n",
    "            ]\n",
    "            \n",
    "            with mp.Pool(mp.cpu_count()) as pool:\n",
    "                for _ in pool.imap(process_frame, args_list):\n",
    "                    pbar.update(1)\n",
    "\n",
    "    # Compile the images into a video using OpenCV\n",
    "    combined_image_files = natsorted(glob.glob(os.path.join(combined_frames_dir, \"combined_frame_*.png\")))\n",
    "\n",
    "    # Get the resolution of the first image\n",
    "    first_image = cv2.imread(combined_image_files[0])\n",
    "    height, width, layers = first_image.shape\n",
    "    video_resolution = (width, height)\n",
    "\n",
    "    # Define the codec and create a VideoWriter object\n",
    "    output_filename = f\"combined_heatmap_movie_{channel}.avi\"\n",
    "    output_file = os.path.join(output_data_dir, output_filename)\n",
    "    ensure_output_dir(output_data_dir)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    out = cv2.VideoWriter(output_file, fourcc, frame_rate, video_resolution)\n",
    "\n",
    "    for image_file in combined_image_files:\n",
    "        img = cv2.imread(image_file)\n",
    "        out.write(img)  # Write the image as a frame in the video\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Combined video saved to {output_file}\")\n",
    "\n",
    "def delete_temporary_image_directories(data_path, conditions, subconditions):\n",
    "    \"\"\"\n",
    "    Deletes all the temporary directories containing the images used for creating movies, for all channels.\n",
    "\n",
    "    Args:\n",
    "    - data_path (str): Base path where the temporary images are stored.\n",
    "    - conditions (list): List of conditions defining subdirectories within the data path.\n",
    "    - subconditions (list): List of subconditions defining further subdirectories.\n",
    "    \"\"\"\n",
    "    # Define the output directory\n",
    "    output_data_dir = os.path.join(data_path, \"output_data\", \"movies\")\n",
    "    \n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            # Find all channel-specific directories and remove them\n",
    "            temp_dirs = glob.glob(os.path.join(output_data_dir, f\"{condition}_{subcondition}_heatmaps_*\"))\n",
    "            for temp_dir in temp_dirs:\n",
    "                if os.path.exists(temp_dir):\n",
    "                    shutil.rmtree(temp_dir)\n",
    "                    print(f\"Deleted temporary directory: {temp_dir}\")\n",
    "    \n",
    "    # Delete the 'combined_frames' directory used in combined movie creation\n",
    "    combined_frames_dir = os.path.join(output_data_dir, \"combined_frames\")\n",
    "    if os.path.exists(combined_frames_dir):\n",
    "        shutil.rmtree(combined_frames_dir)\n",
    "        print(f\"Deleted temporary images directory: {combined_frames_dir}\")\n",
    "\n",
    "def delete_produced_output_all_channels(output_base_dir, conditions, subconditions):\n",
    "    \"\"\"\n",
    "    Deletes all the produced output including temporary directories and generated files for all channels.\n",
    "    \n",
    "    Args:\n",
    "    - output_base_dir (str): The base directory where output files are stored.\n",
    "    - conditions (list): List of conditions defining subdirectories within the output base directory.\n",
    "    - subconditions (list): List of subconditions defining further subdirectories.\n",
    "    \"\"\"\n",
    "    output_data_dir = os.path.join(output_base_dir, \"output_data\")\n",
    "    \n",
    "    # Delete the main output_data directory if it exists\n",
    "    if os.path.exists(output_data_dir):\n",
    "        shutil.rmtree(output_data_dir)\n",
    "        print(f\"Deleted main output directory: {output_data_dir}\")\n",
    "    \n",
    "    # Loop through each condition and subcondition to delete individual directories\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            # Remove all heatmap directories for each condition and subcondition\n",
    "            channel_dirs = glob.glob(os.path.join(output_data_dir, f\"movies/{condition}_{subcondition}_heatmaps_*\"))\n",
    "            for temp_dir in channel_dirs:\n",
    "                if os.path.exists(temp_dir):\n",
    "                    shutil.rmtree(temp_dir)\n",
    "                    print(f\"Deleted temporary directory: {temp_dir}\")\n",
    "                \n",
    "            # Find and remove all combined movie files\n",
    "            combined_movie_files = glob.glob(os.path.join(output_data_dir, f\"movies/combined_heatmap_movie_*.avi\"))\n",
    "            for combined_movie_file in combined_movie_files:\n",
    "                if os.path.exists(combined_movie_file):\n",
    "                    os.remove(combined_movie_file)\n",
    "                    print(f\"Deleted combined movie file: {combined_movie_file}\")\n",
    "\n",
    "    # Delete the 'combined_frames' directory used for combined movie creation\n",
    "    temp_img_dir = os.path.join(output_data_dir, \"movies/combined_frames\")\n",
    "    if os.path.exists(temp_img_dir):\n",
    "        shutil.rmtree(temp_img_dir)\n",
    "        print(f\"Deleted temporary images directory: {temp_img_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "fluorescence_heatmap(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    channel='cy5', \n",
    "    time_interval_list=time_interval_list, \n",
    "    vmax=24, \n",
    "    skip_frames=1, \n",
    "    calibration_curve_paths=calibration_curve_paths, \n",
    "    show_scalebar=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "create_movies(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    channel='cy5', \n",
    "    frame_rate=120,\n",
    "    skip_frames=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_combined_heatmap_movie_custom_grid(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    channel='cy5', \n",
    "    grid_rows=2, \n",
    "    grid_cols=4, \n",
    "    frame_rate=120,\n",
    "    batch_size=50\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "fluorescence_heatmap(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    channel='GFP', \n",
    "    time_interval_list=time_interval_list, \n",
    "    vmax=500, \n",
    "    skip_frames=1, \n",
    "    calibration_curve_paths=calibration_curve_paths, \n",
    "    show_scalebar=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "create_movies(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    channel='GFP', \n",
    "    frame_rate=120,\n",
    "    skip_frames=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_combined_heatmap_movie_custom_grid(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    channel='GFP', \n",
    "    grid_rows=2, \n",
    "    grid_cols=4, \n",
    "    frame_rate=120,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_temporary_image_directories(data_path, conditions, subconditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_produced_output_all_channels(data_path, conditions, subconditions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fluorescence quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_output_dir(output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "def calculate_mean_intensity(path):\n",
    "    \"\"\"Calculate mean intensity of an image.\"\"\"\n",
    "    return io.imread(path).mean()\n",
    "\n",
    "def subtract_negative_intensity(paths, negative_paths, skip_frames=1):\n",
    "    \"\"\"Subtract the mean intensity of the negative control images from the raw data.\"\"\"\n",
    "    # Apply frame skipping\n",
    "    paths = paths[::skip_frames]\n",
    "    negative_paths = negative_paths[::skip_frames]\n",
    "\n",
    "    # Calculate mean intensities for samples\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        mean_intensity_list = list(tqdm(pool.imap(calculate_mean_intensity, paths), total=len(paths), desc=\"Calculating sample intensities\"))\n",
    "\n",
    "    # Calculate mean intensities for negatives (no progress bar)\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        negative_intensity_list = pool.map(calculate_mean_intensity, negative_paths)\n",
    "\n",
    "    # Subtract negative control intensities from sample intensities\n",
    "    mean_intensity_list = np.array(mean_intensity_list) - np.array(negative_intensity_list)\n",
    "    \n",
    "    return mean_intensity_list\n",
    "\n",
    "def calculate_protein_concentration(mean_intensity, intercept, slope):\n",
    "    \"\"\"Calculate protein concentration in ng/ul.\"\"\"\n",
    "    conc_ng_ul = (mean_intensity - intercept) / slope\n",
    "    return conc_ng_ul\n",
    "\n",
    "def calculate_protein_concentration_nM(conc_ng_ul, mw_kda):\n",
    "    \"\"\"Convert protein concentration from ng/ul to nM.\"\"\"\n",
    "    conc_nM = (conc_ng_ul * 1e-3) / (mw_kda * 1e3) * 1e9\n",
    "    return conc_nM\n",
    "\n",
    "def calculate_number_of_protein_molecules(protein_mass, mw_kda):\n",
    "    \"\"\"Calculate number of protein molecules.\"\"\"\n",
    "    return (protein_mass * 6e14) / (mw_kda * 1e3)\n",
    "\n",
    "def convert_time_units(time_values_s):\n",
    "    \"\"\"Convert time values from seconds to minutes and hours.\"\"\"\n",
    "    time_values_min = time_values_s / 60\n",
    "    time_values_h = time_values_s / 3600\n",
    "    return time_values_s, time_values_min, time_values_h\n",
    "\n",
    "def add_derivative_column(df, time_column, value_column, new_column_name):\n",
    "    \"\"\"Calculate the derivative of a value column with respect to time and add it as a new column.\"\"\"\n",
    "    time_deltas = np.diff(df[time_column], prepend=np.nan)\n",
    "    value_deltas = np.diff(df[value_column], prepend=np.nan)\n",
    "    df[new_column_name] = value_deltas / time_deltas\n",
    "\n",
    "    # Apply Gaussian smoothing with sigma = 2\n",
    "    df[new_column_name] = gaussian_filter1d(df[new_column_name], sigma=2, mode='nearest')\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_translation_rate(df, time_column, protein_molecules_column, protein_length, ribosome_count, droplet_volume_ul):\n",
    "    \"\"\"Calculate the translation rate of individual proteins in amino acids per second.\"\"\"\n",
    "    df['Translation Rate aa_s'] = (df[protein_molecules_column] / ribosome_count) / (df[time_column] * protein_length * droplet_volume_ul * 1e-6)\n",
    "    return df\n",
    "\n",
    "def quantify_tiffiles(data_path, conditions, subconditions, calibration_curve_paths, mw_kda_list, droplet_volume_list, time_interval_s_list, protein_lengths_list, ribosome_count=10**11, skip_frames=1, subtract_negative=True, negative_condition='negative'):\n",
    "    \"\"\"Process images to calculate protein concentration and generate plots, with an option to skip frames.\"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # Sort the calibration curve paths\n",
    "    calibration_curve_paths = sorted(calibration_curve_paths)\n",
    "\n",
    "    # Calibration curve data and fit\n",
    "    sample_concentration_values = [0, 2, 5, 10, 20, 40, 80, 160, 320]\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        mean_intensity_calibration = pool.map(calculate_mean_intensity, calibration_curve_paths)\n",
    "    slope, intercept = np.polyfit(sample_concentration_values, mean_intensity_calibration, 1)\n",
    "\n",
    "    for idx, condition in enumerate(conditions):\n",
    "        mw_kda = mw_kda_list[idx]\n",
    "        droplet_volume = droplet_volume_list[idx]\n",
    "        time_interval_s = time_interval_s_list[idx]\n",
    "        protein_length = protein_lengths_list[idx]\n",
    "\n",
    "        for subcondition in subconditions:\n",
    "            pattern = os.path.join(data_path, condition, subcondition, \"original\", \"*GFP*.tif\")\n",
    "            paths = sorted(glob.glob(pattern))\n",
    "\n",
    "            if not paths:\n",
    "                print(f\"No image files found for condition {condition}, subcondition {subcondition}.\")\n",
    "                continue\n",
    "\n",
    "            # Find the negative control paths\n",
    "            negative_pattern = os.path.join(data_path, negative_condition, subcondition, \"original\", \"*GFP*.tif\")\n",
    "            negative_paths = sorted(glob.glob(negative_pattern))\n",
    "\n",
    "            if subtract_negative and negative_paths:\n",
    "                mean_intensity_list = subtract_negative_intensity(paths, negative_paths, skip_frames)\n",
    "            else:\n",
    "                if not subtract_negative:\n",
    "                    paths = paths[::skip_frames]\n",
    "                with mp.Pool(mp.cpu_count()) as pool:\n",
    "                    mean_intensity_list = list(tqdm(pool.imap(calculate_mean_intensity, paths), total=len(paths), desc=f\"Calculating intensities for {condition} - {subcondition}\"))\n",
    "\n",
    "            protein_concentration_list = [calculate_protein_concentration(intensity, intercept, slope) for intensity in mean_intensity_list]\n",
    "            protein_concentration_nM_list = [calculate_protein_concentration_nM(conc_ng_ul, mw_kda) for conc_ng_ul in protein_concentration_list]\n",
    "\n",
    "            min_intensity = min(mean_intensity_list)\n",
    "            mean_intensity_list = np.array(mean_intensity_list) - min_intensity\n",
    "            protein_concentration_list = np.array(protein_concentration_list) - min(protein_concentration_list)\n",
    "            protein_concentration_nM_list = np.array(protein_concentration_nM_list) - min(protein_concentration_nM_list)\n",
    "\n",
    "            time_values_s = np.arange(len(mean_intensity_list)) * time_interval_s * skip_frames\n",
    "            time_values_s, time_values_min, time_values_h = convert_time_units(time_values_s)\n",
    "            \n",
    "            protein_mass_list = protein_concentration_list * droplet_volume\n",
    "            df = pd.DataFrame({\n",
    "                \"Condition\": condition,\n",
    "                \"Subcondition\": subcondition,\n",
    "                \"Time_s\": time_values_s,\n",
    "                \"Time_min\": time_values_min,\n",
    "                \"Time_h\": time_values_h,\n",
    "                \"Mean Intensity\": mean_intensity_list,\n",
    "                \"Protein Concentration_ng_ul\": protein_concentration_list,\n",
    "                \"Protein Concentration_nM\": protein_concentration_nM_list,\n",
    "            })\n",
    "\n",
    "            df[\"Number of Protein Molecules\"] = [calculate_number_of_protein_molecules(mass, mw_kda) for mass in protein_mass_list]\n",
    "\n",
    "            # Add the derivative (rate of change) of protein molecules over time\n",
    "            df = add_derivative_column(df, \"Time_s\", \"Number of Protein Molecules\", \"Rate of Change of Protein Molecules per Second\")\n",
    "\n",
    "            # Calculate the translation rate in amino acids per second for individual proteins\n",
    "            df = calculate_translation_rate(df, \"Time_s\", \"Number of Protein Molecules\", protein_length, ribosome_count, droplet_volume)\n",
    "\n",
    "            all_data.append(df)\n",
    "\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    mean_df = combined_df.groupby([\"Condition\", \"Time_s\", \"Time_min\", \"Time_h\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    # Add the derivative (rate of change) to the mean dataframe as well\n",
    "    mean_df = add_derivative_column(mean_df, \"Time_s\", \"Number of Protein Molecules\", \"Rate of Change of Protein Molecules per Second\")\n",
    "\n",
    "    # Calculate the translation rate in amino acids per second for the mean dataframe\n",
    "    mean_df = calculate_translation_rate(mean_df, \"Time_s\", \"Number of Protein Molecules\", protein_lengths_list[0], ribosome_count, droplet_volume_list[0])\n",
    "\n",
    "    output_dir = os.path.join(data_path, \"output_data\")\n",
    "    ensure_output_dir(output_dir)\n",
    "\n",
    "    combined_csv_path = os.path.join(output_dir, \"combined_experiment.csv\")\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "    mean_csv_path = os.path.join(output_dir, \"mean_experiment.csv\")\n",
    "    mean_df.to_csv(mean_csv_path, index=False)\n",
    "\n",
    "    plot_results(combined_df, mean_df, output_dir, sample_concentration_values, mean_intensity_calibration, slope, intercept, subtract_negative=subtract_negative, negative_condition=negative_condition)\n",
    "\n",
    "    return combined_csv_path, mean_csv_path\n",
    "\n",
    "\n",
    "\n",
    "def plot_results(df, mean_df, output_dir, sample_concentration_values, mean_intensity_calibration, slope, intercept, subtract_negative=False, negative_condition='negative'):\n",
    "    \"\"\"Generate and save plots for the experimental data.\"\"\"\n",
    "    \n",
    "    calibration_dir = os.path.join(output_dir, \"calibration\")\n",
    "    ensure_output_dir(calibration_dir)\n",
    "    \n",
    "    combined_plots_dir = os.path.join(output_dir, \"combined_plots\")\n",
    "    ensure_output_dir(combined_plots_dir)\n",
    "    \n",
    "    mean_plots_dir = os.path.join(output_dir, \"mean_plots\")\n",
    "    ensure_output_dir(mean_plots_dir)\n",
    "    \n",
    "    combined_log_plots_dir = os.path.join(output_dir, \"combined_plots_log\")\n",
    "    ensure_output_dir(combined_log_plots_dir)\n",
    "    \n",
    "    mean_log_plots_dir = os.path.join(output_dir, \"mean_plots_log\")\n",
    "    ensure_output_dir(mean_log_plots_dir)\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = [12, 8]\n",
    "    plt.rcParams[\"image.cmap\"] = \"viridis\"\n",
    "    dpi_setting = 200\n",
    "    \n",
    "    # Plot calibration curve\n",
    "    plt.figure(dpi=dpi_setting)\n",
    "    plt.scatter(sample_concentration_values, mean_intensity_calibration, label=\"Data Points\")\n",
    "    plt.plot(sample_concentration_values, np.polyval([slope, intercept], sample_concentration_values), color='r', label=f\"Fit: y = {slope:.2f}x + {intercept:.2f}\")\n",
    "    plt.xlabel(\"Protein Concentration ng_ul\")\n",
    "    plt.ylabel(\"Mean Intensity\")\n",
    "    plt.title(\"Calibration Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(calibration_dir, \"calibration_curve.png\"), dpi=dpi_setting)\n",
    "    plt.close()\n",
    "\n",
    "    # Determine time units from dataframe\n",
    "    time_units = [(col, col.replace('_', ' ').title()) for col in df.columns if col.startswith(\"Time_\")]\n",
    "    \n",
    "    # Determine metrics (columns to plot) dynamically, excluding time units and non-numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    metrics = [(col, col.replace('_', ' ').title()) for col in numeric_cols if col not in [col[0] for col in time_units]]\n",
    "\n",
    "    # Plot combined data for each metric\n",
    "    for metric, ylabel in metrics:\n",
    "        for time_unit, xlabel in time_units:\n",
    "            plt.figure(dpi=dpi_setting)\n",
    "            for (condition, subcondition), group in df.groupby([\"Condition\", \"Subcondition\"]):\n",
    "                if subtract_negative and condition == negative_condition:\n",
    "                    continue\n",
    "                plt.plot(group[time_unit], group[metric], label=f\"{condition} - {subcondition}\")\n",
    "            \n",
    "            plt.xlabel(xlabel)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.title(f\"Combined {ylabel} over {xlabel} for All Conditions\")\n",
    "            if metric == 'Translation Rate aa_s':\n",
    "                plt.ylim(0, 1)  # Set y-axis limit for translation rate plot\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(combined_plots_dir, f\"combined_{metric}_plot_{time_unit}.png\"), dpi=dpi_setting)\n",
    "            plt.close()\n",
    "\n",
    "            # Generate log scale version\n",
    "            plt.figure(dpi=dpi_setting)\n",
    "            for (condition, subcondition), group in df.groupby([\"Condition\", \"Subcondition\"]):\n",
    "                if subtract_negative and condition == negative_condition:\n",
    "                    continue\n",
    "                plt.plot(group[time_unit], group[metric], label=f\"{condition} - {subcondition}\")\n",
    "            \n",
    "            plt.xlabel(xlabel)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.title(f\"Combined {ylabel} over {xlabel} for All Conditions (Log Scale)\")\n",
    "            plt.yscale('log')\n",
    "            if metric == 'Translation Rate aa_s':\n",
    "                plt.ylim(1e-3, 1)  # Adjust y-axis limits for log scale if necessary\n",
    "            plt.legend()\n",
    "            plt.grid(True, which=\"both\", ls=\"--\")\n",
    "            plt.savefig(os.path.join(combined_log_plots_dir, f\"combined_{metric}_plot_{time_unit}_log.png\"), dpi=dpi_setting)\n",
    "            plt.close()\n",
    "\n",
    "    # Plot mean data for each metric\n",
    "    for metric, ylabel in metrics:\n",
    "        for time_unit, xlabel in time_units:\n",
    "            plt.figure(dpi=dpi_setting)\n",
    "            for condition, group in mean_df.groupby(\"Condition\"):\n",
    "                if subtract_negative and condition == negative_condition:\n",
    "                    continue\n",
    "                plt.plot(group[time_unit], group[metric], label=f\"{condition}\")\n",
    "                \n",
    "            plt.xlabel(xlabel)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.title(f\"Mean {ylabel} over {xlabel} for All Conditions\")\n",
    "            if metric == 'Translation Rate aa_s':\n",
    "                plt.ylim(0, 1)  # Set y-axis limit for translation rate plot\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(mean_plots_dir, f\"mean_{metric}_plot_{time_unit}.png\"), dpi=dpi_setting)\n",
    "            plt.close()\n",
    "\n",
    "            # Generate log scale version\n",
    "            plt.figure(dpi=dpi_setting)\n",
    "            for condition, group in mean_df.groupby(\"Condition\"):\n",
    "                if subtract_negative and condition == negative_condition:\n",
    "                    continue\n",
    "                plt.plot(group[time_unit], group[metric], label=f\"{condition}\")\n",
    "                \n",
    "            plt.xlabel(xlabel)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.title(f\"Mean {ylabel} over {xlabel} for All Conditions (Log Scale)\")\n",
    "            plt.yscale('log')\n",
    "            if metric == 'Translation Rate aa_s':\n",
    "                plt.ylim(1e-3, 1)  # Adjust y-axis limits for log scale if necessary\n",
    "            plt.legend()\n",
    "            plt.grid(True, which=\"both\", ls=\"--\")\n",
    "            plt.savefig(os.path.join(mean_log_plots_dir, f\"mean_{metric}_plot_{time_unit}_log.png\"), dpi=dpi_setting)\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "mw_kda_list = [100] * len(conditions)\n",
    "droplet_volume_list = [2] * len(conditions)\n",
    "protein_lengths_list = [500] * len(conditions)  # Assuming 500 amino acids per protein\n",
    "\n",
    "quantify_tiffiles(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    calibration_curve_paths, \n",
    "    mw_kda_list, \n",
    "    droplet_volume_list, \n",
    "    time_interval_list, \n",
    "    protein_lengths_list,\n",
    "    ribosome_count=10**10,\n",
    "    skip_frames=1,\n",
    "    subtract_negative=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a single image (helper function for multiprocessing)\n",
    "def process_single_image(file_name, output_dir, brightness_factor, contrast_factor, num_digits, i):\n",
    "    image = Image.open(file_name).convert(\"L\")\n",
    "    image_resized = image.resize((2048, 2048), Image.LANCZOS)\n",
    "\n",
    "    enhancer = ImageEnhance.Brightness(image_resized)\n",
    "    image_brightened = enhancer.enhance(brightness_factor)\n",
    "    enhancer = ImageEnhance.Contrast(image_brightened)\n",
    "    image_contrasted = enhancer.enhance(contrast_factor)\n",
    "\n",
    "    padded_index = str(i + 1).zfill(num_digits)\n",
    "    base_file_name = f'converted_image_{padded_index}.tif'\n",
    "    processed_image_path = os.path.join(output_dir, base_file_name)\n",
    "    image_contrasted.save(processed_image_path, format='TIFF', compression='tiff_lzw')\n",
    "\n",
    "\n",
    "# Convert PIVlab images to the right size using multiprocessing\n",
    "def convert_images(data_path, conditions, subconditions, max_frame, brightness_factor=1, contrast_factor=1, skip_frames=1):\n",
    "    for condition in tqdm(conditions, desc=\"Conditions\", leave=False):\n",
    "        for subcondition in tqdm(subconditions, desc=\"Subconditions\", leave=False):\n",
    "            input_dir = os.path.join(data_path, condition, subcondition, \"piv_movie\")\n",
    "            output_dir = os.path.join(data_path, condition, subcondition, \"piv_movie_converted\")\n",
    "\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            input_files = natsorted(glob.glob(os.path.join(input_dir, '*.jpg')))\n",
    "\n",
    "            if max_frame:\n",
    "                input_files = input_files[:max_frame]\n",
    "\n",
    "            # Apply frame skipping\n",
    "            input_files = input_files[::skip_frames]\n",
    "\n",
    "            output_files = natsorted(glob.glob(os.path.join(output_dir, '*.tif')))\n",
    "            if len(input_files) <= len(output_files):\n",
    "                print(f\"Conversion might already be completed or partial for {output_dir}. Continuing...\")\n",
    "                # Optional: Add logic to check and continue incomplete work.\n",
    "\n",
    "            num_digits = len(str(len(input_files)))\n",
    "\n",
    "            # Use all available cores\n",
    "            with Pool(cpu_count()) as pool:\n",
    "                list(tqdm(pool.starmap(process_single_image, [(file_name, output_dir, brightness_factor, contrast_factor, num_digits, i) for i, file_name in enumerate(input_files)]), total=len(input_files), desc=\"Converting Images\", leave=False))\n",
    "\n",
    "\n",
    "# Helper function to plot autocorrelation\n",
    "def plot_autocorrelation_values(data_path, condition, subcondition, frame_id, lambda_tau, results, fitted_values, intervector_distance_microns):\n",
    "    output_directory_dfs = os.path.join(data_path, condition, subcondition, \"autocorrelation_plots\")\n",
    "    os.makedirs(output_directory_dfs, exist_ok=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    x_values = np.arange(len(results)) * intervector_distance_microns * 1E6\n",
    "\n",
    "    plt.plot(x_values, results, label='Autocorrelation Values', marker='o', linestyle='-', markersize=5)\n",
    "    plt.plot(x_values, fitted_values, label='Fitted Exponential Decay', linestyle='--', color='red')\n",
    "    plt.axvline(x=lambda_tau, color='green', linestyle='-.', label=f'Correlation Length = {lambda_tau:.2f} µm')\n",
    "\n",
    "    plt.xlabel('Scaled Lag (µm)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.title(f'Autocorrelation Function and Fitted Exponential Decay (Frame {frame_id})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    # plt.ylim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = os.path.join(output_directory_dfs, f'autocorrelation_frame_{frame_id}.jpg')\n",
    "    plt.savefig(filename, dpi=200, format='jpg')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Helper function to calculate correlation length\n",
    "def correlation_length(data_frame):\n",
    "    # Reshaping the data frame to a 2D grid and normalizing\n",
    "    v = data_frame.pivot(index='y [m]', columns='x [m]', values=\"velocity magnitude [m/s]\").values\n",
    "    v -= np.mean(v)  # Centering the data\n",
    "\n",
    "    # FFT to find the power spectrum and compute the autocorrelation\n",
    "    fft_v = np.fft.fft2(v)\n",
    "    autocorr = np.fft.ifft2(fft_v * np.conj(fft_v))\n",
    "    autocorr = np.real(autocorr) / np.max(np.real(autocorr))  # Normalize the autocorrelation\n",
    "\n",
    "    # Preparing to extract the autocorrelation values along the diagonal\n",
    "    r_values = min(v.shape) // 2\n",
    "    results = np.zeros(r_values)\n",
    "    for r in range(r_values):\n",
    "        # Properly average over symmetric pairs around the center\n",
    "        autocorrelation_value = (autocorr[r, r] + autocorr[-r, -r]) / 2\n",
    "        results[r] = autocorrelation_value\n",
    "\n",
    "    # Normalize the results to start from 1\n",
    "    results /= results[0]\n",
    "\n",
    "    # Exponential decay fitting to extract the correlation length\n",
    "    def exponential_decay(x, A, B, C):\n",
    "        return A * np.exp(-x / B) + C\n",
    "\n",
    "    # Fit parameters and handling potential issues with initial parameter guesses\n",
    "    try:\n",
    "        params, _ = curve_fit(exponential_decay, np.arange(len(results)), results, p0=(1, 10, 0), maxfev=5000)\n",
    "    except RuntimeError:\n",
    "        # Handle cases where the curve fit does not converge\n",
    "        params = [np.nan, np.nan, np.nan]  # Use NaN to indicate the fit failed\n",
    "\n",
    "    A, B, C = params\n",
    "    fitted_values = exponential_decay(np.arange(r_values), *params)\n",
    "\n",
    "    # Calculate the correlation length\n",
    "    intervector_distance_microns = ((data_frame[\"y [m]\"].max() - data_frame[\"y [m]\"].min()) / v.shape[0])\n",
    "    if B > 0 and A != C:  # Ensure valid values for logarithmic calculation\n",
    "        lambda_tau = -B * np.log((0.3 - C) / A) * intervector_distance_microns\n",
    "    else:\n",
    "        lambda_tau = np.nan  # Return NaN if parameters are not suitable for calculation\n",
    "\n",
    "    return lambda_tau, results, fitted_values, intervector_distance_microns\n",
    "\n",
    "\n",
    "# Load PIV data from PIVlab into dataframes\n",
    "def load_piv_data(data_path, condition, subcondition, min_frame=0, max_frame=None, skip_frames=1):\n",
    "    input_piv_data = os.path.join(data_path, condition, subcondition, \"piv_data\", \"PIVlab_****.txt\")\n",
    "    \n",
    "    # Using a for loop instead of list comprehension\n",
    "    dfs = []\n",
    "    for file in tqdm(sorted(glob.glob(input_piv_data))[min_frame:max_frame:skip_frames], desc=f\"Loading PIV data for {condition} {subcondition}\", leave=False):\n",
    "        df = pd.read_csv(file, skiprows=2).fillna(0).rename(columns={\n",
    "            \"magnitude [m/s]\": \"velocity magnitude [m/s]\",\n",
    "            \"simple shear [1/s]\": \"shear [1/s]\",\n",
    "            \"simple strain [1/s]\": \"strain [1/s]\",\n",
    "            \"Vector type [-]\": \"data type [-]\"\n",
    "        })\n",
    "        dfs.append(df)\n",
    "\n",
    "    return dfs\n",
    "\n",
    "# Generate dataframes from PIV data with time intervals applied\n",
    "def generate_dataframes_from_piv_data(data_path, condition, subcondition, min_frame=0, max_frame=None, skip_frames=1, plot_autocorrelation=True, time_interval=1):\n",
    "    output_directory_dfs = os.path.join(data_path, condition, subcondition, \"dataframes_PIV\")\n",
    "    os.makedirs(output_directory_dfs, exist_ok=True)\n",
    "\n",
    "    # Load PIV data\n",
    "    data_frames = load_piv_data(data_path, condition, subcondition, min_frame, max_frame, skip_frames)\n",
    "\n",
    "    # Calculating mean values with valid vectors only\n",
    "    mean_values = []\n",
    "    for frame_id, data_frame in enumerate(tqdm(data_frames, desc=f\"Generating dataframes for {condition} {subcondition}\", leave=False)):\n",
    "        lambda_tau, results, fitted_values, intervector_distance_microns = correlation_length(data_frame)\n",
    "        if plot_autocorrelation:\n",
    "            plot_autocorrelation_values(data_path, condition, subcondition, frame_id, lambda_tau * 1E6, results, fitted_values, intervector_distance_microns)\n",
    "        data_frame[\"correlation length [m]\"] = lambda_tau\n",
    "        data_frame = data_frame[data_frame[\"data type [-]\"] == 1]\n",
    "        mean_values.append(data_frame.mean(axis=0))\n",
    "\n",
    "    # Creating mean DataFrame\n",
    "    mean_data_frame = pd.DataFrame(mean_values)\n",
    "    mean_data_frame.reset_index(drop=False, inplace=True)\n",
    "    mean_data_frame.rename(columns={'index': 'frame'}, inplace=True)\n",
    "\n",
    "    # Subtract the minimum row value for each column from the entire column for velocity magnitude\n",
    "    mean_data_frame[\"velocity magnitude [m/s]\"] = mean_data_frame[\"velocity magnitude [m/s]\"] - mean_data_frame[\"velocity magnitude [m/s]\"].min()\n",
    "    \n",
    "    # add a column with total distance travelled\n",
    "    mean_data_frame[\"distance [m]\"] = mean_data_frame[\"velocity magnitude [m/s]\"].cumsum() * time_interval\n",
    "    mean_data_frame[\"distance [m]\"] = mean_data_frame[\"distance [m]\"] - mean_data_frame[\"distance [m]\"].min()\n",
    "\n",
    "    # Calculate power and add to DataFrame\n",
    "    volume = 2E-9  # µl --> m^3\n",
    "    viscosity = 1E-3  # mPa*S\n",
    "    mean_data_frame[\"power [W]\"] = volume * viscosity * (mean_data_frame[\"velocity magnitude [m/s]\"]/mean_data_frame[\"correlation length [m]\"])**2\n",
    "\n",
    "    # Scale time appropriately using the provided time_interval\n",
    "    mean_data_frame[\"time (s)\"] = mean_data_frame[\"frame\"] * time_interval\n",
    "    mean_data_frame[\"time (min)\"] = mean_data_frame[\"time (s)\"] / 60\n",
    "    mean_data_frame[\"time (h)\"] = mean_data_frame[\"time (min)\"] / 60\n",
    "\n",
    "    # Creating pivot matrices for each feature\n",
    "    features = data_frames[0].columns[:-1]\n",
    "    pivot_matrices = {feature: [] for feature in features}\n",
    "\n",
    "    for data_frame in data_frames:\n",
    "        temporary_dictionary = {feature: data_frame.pivot(index='y [m]', columns='x [m]', values=feature).values for feature in features}\n",
    "        for feature in features:\n",
    "            pivot_matrices[feature].append(temporary_dictionary[feature])\n",
    "\n",
    "    pivot_data_frame = pd.DataFrame(pivot_matrices)\n",
    "\n",
    "    # Adjusting column names in mean_data_frame\n",
    "    mean_data_frame.columns = [f\"{column}_mean\" if column not in [\"frame\", \"time (s)\", \"time (min)\", \"time (h)\"] else column for column in mean_data_frame.columns]\n",
    "    \n",
    "    # Adding time column to pivot_data_frame\n",
    "    pivot_data_frame[\"frame\"] = mean_data_frame[\"frame\"].values\n",
    "    \n",
    "    # subtract the minimum row value for each column from the entire column in \n",
    "    \n",
    "    # Save DataFrames to CSV\n",
    "    mean_df_output_path = os.path.join(output_directory_dfs, \"mean_values.csv\")\n",
    "    mean_data_frame.to_csv(mean_df_output_path, index=False)\n",
    "\n",
    "    pivot_df_output_path = os.path.join(output_directory_dfs, \"features_matrices.csv\")\n",
    "    pivot_data_frame.to_csv(pivot_df_output_path, index=False)\n",
    "\n",
    "    return mean_data_frame, pivot_data_frame\n",
    "\n",
    "\n",
    "\n",
    "# Plot the PIVlab output as heatmaps\n",
    "def generate_heatmaps_from_dataframes(df, data_path, condition, subcondition, feature_limits, time_interval=3):\n",
    "    for feature, limits in feature_limits.items():\n",
    "        vmin, vmax = limits\n",
    "\n",
    "        for j in tqdm(range(len(df)), desc=f\"Generating heatmaps for {condition} {subcondition} {feature}\", leave=False):\n",
    "            vals = df.iloc[j, df.columns.get_loc(feature)]\n",
    "\n",
    "            output_directory_heatmaps = os.path.join(data_path, condition, subcondition, \"heatmaps_PIV\", f\"{feature.split()[0]}\", f\"{feature.split()[0]}_heatmap_{j}.jpg\")\n",
    "            image_files_pattern = f\"{data_path}/{condition}/{subcondition}/piv_movie_converted/converted_image_****.tif\"\n",
    "            image_files = sorted(glob.glob(image_files_pattern))[j]\n",
    "            image = Image.open(image_files)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(image, cmap=None, extent=[-2762/2, 2762/2, -2762/2, 2762/2]) # piv image\n",
    "            im = plt.imshow(vals, cmap='inferno', origin='upper', alpha=0.7, extent=[-2762/2, 2762/2, -2762/2, 2762/2], vmin=vmin, vmax=vmax) # heatmap\n",
    "            plt.xlabel('x [um]')\n",
    "            plt.ylabel('y [um]')\n",
    "            cbar = plt.colorbar(im)\n",
    "            cbar.set_label(feature)\n",
    "            time = df.iloc[j, -1]\n",
    "            plt.title(f\"PIV - {feature}  ||  time: {int(time * time_interval/60)} min -- {int(time * time_interval/3600)} hours\")\n",
    "\n",
    "            os.makedirs(os.path.dirname(output_directory_heatmaps), exist_ok=True)\n",
    "            plt.savefig(output_directory_heatmaps, format='jpg', dpi=200)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def create_movies_PIV(data_path, condition, subcondition, frame_rate, feature_limits=None, max_frame=None):\n",
    "    plots_dir = f\"{data_path}/{condition}/{subcondition}/heatmaps_PIV/\"\n",
    "    for feature in feature_limits.keys():\n",
    "        feature_name_for_file = feature.split()[0]\n",
    "        heatmap_dir = os.path.join(data_path, condition, subcondition, \"heatmaps_PIV\", f\"{feature.split()[0]}\", f\"{feature.split()[0]}_heatmap_****.jpg\")\n",
    "        image_files = natsorted(glob.glob(heatmap_dir))\n",
    "\n",
    "        if not image_files:\n",
    "            print(f\"No images found for feature {feature_name_for_file}.\")\n",
    "            continue\n",
    "\n",
    "        # Limit the number of files if max_frame is specified\n",
    "        image_files = image_files[:max_frame] if max_frame is not None else image_files\n",
    "\n",
    "        # Get the resolution of the first image (assuming all images are the same size)\n",
    "        first_image = cv2.imread(image_files[0])\n",
    "        video_resolution = (first_image.shape[1], first_image.shape[0])  # Width x Height\n",
    "\n",
    "        # Define the codec and create VideoWriter object\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "        out_path = f'{plots_dir}{feature_name_for_file}.avi'\n",
    "        out = cv2.VideoWriter(out_path, fourcc, frame_rate, video_resolution)\n",
    "\n",
    "        for file in tqdm(image_files, desc=f\"Creating movie for {condition} {subcondition} {feature}\", leave=False):\n",
    "            img = cv2.imread(file)\n",
    "            out.write(img)  # Write the image as is, without resizing\n",
    "\n",
    "        out.release()\n",
    "        print(f\"Video saved to {out_path}\")\n",
    "\n",
    "\n",
    "# Process PIV data for all conditions and subconditions, then average and save results\n",
    "def process_piv_data(data_path, conditions, subconditions, feature_limits, time_intervals, skip_frames, min_frame=0, max_frame=None, plot_autocorrelation=True, frame_rate=120, heatmaps=True):\n",
    "    for i, condition in tqdm(enumerate(conditions), desc=\"Processing PIV data\", total=len(conditions), leave=True):\n",
    "        time_interval = time_intervals[i] * skip_frames\n",
    "        results = []\n",
    "        for subcondition in tqdm(subconditions, desc=f\"Processing subconditions for {condition}\", leave=False):\n",
    "            m, p = generate_dataframes_from_piv_data(data_path, condition, subcondition, min_frame, max_frame, skip_frames, plot_autocorrelation, time_interval)\n",
    "            results.append(m)\n",
    "\n",
    "            if heatmaps == True:\n",
    "                convert_images(data_path, conditions, subconditions, max_frame=None, brightness_factor=1, contrast_factor=1, skip_frames=skip_frames)\n",
    "                generate_heatmaps_from_dataframes(p, data_path, condition, subcondition, feature_limits, time_interval)\n",
    "                create_movies_PIV(data_path, condition, subcondition, frame_rate, feature_limits=feature_limits, max_frame=max_frame)\n",
    "\n",
    "        # Averaging and saving the results for the current condition\n",
    "        save_path = os.path.join(data_path, condition, 'averaged')\n",
    "        average_df = sum(results) / len(results)\n",
    "        \n",
    "        os.makedirs(save_path, exist_ok=True)  # Ensure the directory exists\n",
    "        average_df.to_csv(os.path.join(save_path, f\"{condition}_average.csv\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# Generate PCA from PIVlab output\n",
    "def plot_pca(dfs, data_path, conditions, subconditions, features):\n",
    "    # Perform PCA and Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Get colors from Seaborn's \"colorblind\" color palette\n",
    "    sns.set_palette(\"colorblind\", color_codes=True)\n",
    "    colors = sns.color_palette(\"colorblind\", n_colors=len(conditions))\n",
    "\n",
    "    for group_index, (df, condition, subcondition) in enumerate(zip(dfs, conditions, subconditions)):\n",
    "        pca = PCA(n_components=2)\n",
    "        principalComponents = pca.fit_transform(df.loc[:, features])\n",
    "        principalDf = pd.DataFrame(data=principalComponents, columns=['principal component 1', 'principal component 2'])\n",
    "\n",
    "        # Scaling alpha to increase with respect to the frame index\n",
    "        num_points = principalDf.shape[0]\n",
    "        alphas = np.linspace(0.001, 1, num_points)  # Alpha values linearly spaced from 1 to 0.01\n",
    "        \n",
    "        # Plotting each line segment with increasing alpha\n",
    "        for i in range(1, num_points):\n",
    "            plt.plot(principalDf['principal component 1'][i-1:i+1], principalDf['principal component 2'][i-1:i+1], \n",
    "                     alpha=alphas[i], linestyle='-', linewidth=2, color=colors[group_index])\n",
    "\n",
    "        # Plotting the points\n",
    "        plt.scatter(principalDf['principal component 1'], principalDf['principal component 2'], \n",
    "                    alpha=0.5, label=f'{condition}_{subcondition}', s=10, color=colors[group_index])\n",
    "\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('PCA of PIV Features (All Samples)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    output_dir_pca = os.path.join(data_path, conditions[-1], subconditions[-1], \"plots_PIV\", \"PCA.jpg\")\n",
    "    os.makedirs(os.path.dirname(output_dir_pca), exist_ok=True)\n",
    "    plt.savefig(output_dir_pca, format='jpg', dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def plot_PIV_features(data_path, conditions, subconditions, features_pca, sigma=10, min_frame=0, max_frame=None):\n",
    "    for condition in tqdm(conditions, desc=\"Plotting PIV features\", leave=True):\n",
    "        dfs = []\n",
    "        \n",
    "        for subcondition in subconditions:\n",
    "            # Construct the file path\n",
    "            file_path = os.path.join(data_path, condition, subcondition, \"dataframes_PIV\", \"mean_values.csv\")\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Apply Gaussian filter\n",
    "            df.iloc[:, 1:-3] = df.iloc[:, 1:-3].apply(lambda x: gaussian_filter1d(x, sigma=sigma))\n",
    "\n",
    "            # Rename columns\n",
    "            df = df.rename(columns={\n",
    "                \"data type [-]_mean\": \"work [J]\",\n",
    "                \"correlation length [m]_mean\": \"correlation length [um]\",\n",
    "                \"velocity magnitude [m/s]_mean\": \"velocity magnitude [um/s]\"\n",
    "            })\n",
    "\n",
    "            # Calculate cumulative work\n",
    "            df[\"work [J]\"] = df[\"power [W]_mean\"].cumsum()\n",
    "\n",
    "            # Slice the dataframe if min_frame and max_frame are provided\n",
    "            df = df.iloc[min_frame:max_frame, :]\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "        # Plot PCA\n",
    "        plot_pca(dfs, data_path, [condition] * len(subconditions), subconditions, features_pca)\n",
    "\n",
    "        # Plot individual features\n",
    "        for feature in dfs[0].columns[5:-3]:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            for df, subcondition in zip(dfs, subconditions):\n",
    "                output_directory_plots = os.path.join(data_path, condition, subcondition, \"plots_PIV\")\n",
    "                \n",
    "                # Ensure the directory exists\n",
    "                os.makedirs(output_directory_plots, exist_ok=True)\n",
    "                \n",
    "                plt.plot(df[\"time (h)\"], df[feature], marker='o', linestyle='-', markersize=1, linewidth=1, label=f'{condition}_{subcondition}')\n",
    "                plt.xlabel('Time (hours)')\n",
    "                plt.ylabel(feature)\n",
    "                plt.title(f\"PIV - {feature}\")\n",
    "                plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "                plt.legend()\n",
    "                plt.savefig(os.path.join(output_directory_plots, f\"{feature.split()[0]}_h.jpg\"), format='jpg', dpi=200)\n",
    "                plt.close()\n",
    "\n",
    "                plt.plot(df[\"time (min)\"], df[feature], marker='o', linestyle='-', markersize=1, linewidth=1, label=f'{condition}_{subcondition}')\n",
    "                plt.xlabel('Time (minutes)')\n",
    "                plt.ylabel(feature)\n",
    "                plt.title(f\"PIV - {feature}\")\n",
    "                plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "                plt.legend()\n",
    "                plt.savefig(os.path.join(output_directory_plots, f\"{feature.split()[0]}_min.jpg\"), format='jpg', dpi=200)\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def plot_PIV_features_averaged(data_path, conditions, features_pca, sigma=2, min_frame=0, max_frame=None):\n",
    "    \"\"\"\n",
    "    Generate PIV plots averaged over subconditions and save them in the 'PIV_plots_averaged' folder.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions (list): List of conditions.\n",
    "        features_pca (list): List of features to include in PCA and plotting.\n",
    "        sigma (float): The standard deviation for Gaussian kernel applied for smoothing.\n",
    "        min_frame (int): The minimum frame to include in the analysis.\n",
    "        max_frame (int): The maximum frame to include in the analysis.\n",
    "    \"\"\"\n",
    "    for condition in tqdm(conditions, desc=\"Plotting averaged PIV features\", leave=True):\n",
    "        # Path to the averaged data\n",
    "        averaged_data_path = os.path.join(data_path, condition, \"averaged\")\n",
    "        averaged_df_file = os.path.join(averaged_data_path, f\"{condition}_average.csv\")\n",
    "        \n",
    "        if not os.path.exists(averaged_df_file):\n",
    "            print(f\"Error: Averaged dataframe {averaged_df_file} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        # Load the averaged dataframe\n",
    "        df = pd.read_csv(averaged_df_file)\n",
    "        \n",
    "        # Apply Gaussian smoothing\n",
    "        df.iloc[:, 1:-3] = df.iloc[:, 1:-3].apply(lambda x: gaussian_filter1d(x, sigma=sigma))\n",
    "        \n",
    "        # Rename columns as necessary for consistency in plotting\n",
    "        df = df.rename(columns={\"data type [-]_mean\": \"work [J]\", \n",
    "                                \"correlation length [m]_mean\": \"correlation length [um]\", \n",
    "                                \"velocity magnitude [m/s]_mean\": \"velocity magnitude [um/s]\"})\n",
    "        \n",
    "        df[\"work [J]\"] = df[\"power [W]_mean\"].cumsum()\n",
    "        \n",
    "        # Limit the frames if specified\n",
    "        df = df.iloc[min_frame:max_frame, :]\n",
    "        \n",
    "        # Prepare output directory\n",
    "        output_directory_plots = os.path.join(averaged_data_path, \"PIV_plots_averaged\")\n",
    "        os.makedirs(output_directory_plots, exist_ok=True)\n",
    "        \n",
    "        # PCA and feature plotting\n",
    "        for feature in features_pca:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.plot(df[\"time (h)\"], df[feature], marker='o', linestyle='-', markersize=1, linewidth=1)\n",
    "            plt.xlabel('Time (hours)')\n",
    "            plt.ylabel(feature)\n",
    "            plt.title(f\"Averaged PIV - {feature} over Subconditions\")\n",
    "            plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "            plt.savefig(os.path.join(output_directory_plots, f\"{feature.split()[0]}_h.jpg\"), format='jpg', dpi=200)\n",
    "            plt.close()\n",
    "            \n",
    "            plt.plot(df[\"time (min)\"], df[feature], marker='o', linestyle='-', markersize=1, linewidth=1)\n",
    "            plt.xlabel('Time (minutes)')\n",
    "            plt.ylabel(feature)\n",
    "            plt.title(f\"Averaged PIV - {feature} over Subconditions\")\n",
    "            plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "            plt.savefig(os.path.join(output_directory_plots, f\"{feature.split()[0]}_min.jpg\"), format='jpg', dpi=200)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_PIV_features_combined(data_path, conditions, features_pca, sigma=2, min_frame=0, max_frame=None):\n",
    "    \"\"\"\n",
    "    Generate combined PIV plots across all conditions and save them in the 'combined_PIV_plots' folder.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions (list): List of conditions.\n",
    "        features_pca (list): List of features to include in PCA and plotting.\n",
    "        sigma (float): The standard deviation for Gaussian kernel applied for smoothing.\n",
    "        min_frame (int): The minimum frame to include in the analysis.\n",
    "        max_frame (int): The maximum frame to include in the analysis.\n",
    "    \"\"\"\n",
    "    # Prepare output directory for combined plots\n",
    "    combined_output_dir = os.path.join(data_path, \"PIV_plots\", \"averaged_conditions\")\n",
    "    os.makedirs(combined_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Dictionary to store data for combined plots\n",
    "    combined_data = {feature: {} for feature in features_pca}\n",
    "    \n",
    "    for condition in tqdm(conditions, desc=\"Collecting PIV data\", leave=True):\n",
    "        # Path to the averaged data\n",
    "        averaged_data_path = os.path.join(data_path, condition, \"averaged\")\n",
    "        averaged_df_file = os.path.join(averaged_data_path, f\"{condition}_average.csv\")\n",
    "        \n",
    "        if not os.path.exists(averaged_df_file):\n",
    "            print(f\"Error: Averaged dataframe {averaged_df_file} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        # Load the averaged dataframe\n",
    "        df = pd.read_csv(averaged_df_file)\n",
    "        \n",
    "        # Apply Gaussian smoothing\n",
    "        df.iloc[:, 1:-3] = df.iloc[:, 1:-3].apply(lambda x: gaussian_filter1d(x, sigma=sigma))\n",
    "        \n",
    "        # Rename columns as necessary for consistency in plotting\n",
    "        df = df.rename(columns={\"data type [-]_mean\": \"work [J]\", \n",
    "                                \"correlation length [m]_mean\": \"correlation length [um]\", \n",
    "                                \"velocity magnitude [m/s]_mean\": \"velocity magnitude [um/s]\"})\n",
    "        \n",
    "        df[\"work [J]\"] = df[\"power [W]_mean\"].cumsum()\n",
    "        \n",
    "        # Limit the frames if specified\n",
    "        df = df.iloc[min_frame:max_frame, :]\n",
    "        \n",
    "        # Store data for combined plots\n",
    "        for feature in features_pca:\n",
    "            combined_data[feature][condition] = (df[\"time (h)\"], df[feature])\n",
    "    \n",
    "    # Generate combined plots\n",
    "    for feature in features_pca:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for condition, (time, values) in combined_data[feature].items():\n",
    "            plt.plot(time, values, marker='o', linestyle='-', markersize=1, linewidth=1, label=condition)\n",
    "        \n",
    "        plt.xlabel('Time (hours)')\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(f\"Combined PIV - {feature} across Conditions\")\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(combined_output_dir, f\"combined_{feature.split()[0]}_h.jpg\"), format='jpg', dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for condition, (time, values) in combined_data[feature].items():\n",
    "            time_in_min = time * 60  # Convert hours to minutes for the second plot\n",
    "            plt.plot(time_in_min, values, marker='o', linestyle='-', markersize=1, linewidth=1, label=condition)\n",
    "        \n",
    "        plt.xlabel('Time (minutes)')\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(f\"Combined PIV - {feature} across Conditions\")\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(combined_output_dir, f\"combined_{feature.split()[0]}_min.jpg\"), format='jpg', dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_PIV_features_all_conditions_subconditions(data_path, conditions, subconditions, features_pca, sigma=2, min_frame=0, max_frame=None):\n",
    "    \"\"\"\n",
    "    Generate PIV plots that display all conditions and subconditions together on the same plot for each feature.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions (list): List of conditions.\n",
    "        subconditions (list): List of subconditions for each condition.\n",
    "        features_pca (list): List of features to include in plotting.\n",
    "        sigma (float): The standard deviation for Gaussian kernel applied for smoothing.\n",
    "        min_frame (int): The minimum frame to include in the analysis.\n",
    "        max_frame (int): The maximum frame to include in the analysis.\n",
    "    \"\"\"\n",
    "    # Prepare output directory for combined plots\n",
    "    combined_output_dir = os.path.join(data_path, \"PIV_plots\", \"all_conditions_subconditions\")\n",
    "    os.makedirs(combined_output_dir, exist_ok=True)\n",
    "    \n",
    "    for feature in features_pca:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for condition in conditions:\n",
    "            for subcondition in subconditions:\n",
    "                # Path to the subcondition data\n",
    "                subcondition_data_path = os.path.join(data_path, condition, subcondition, \"dataframes_PIV\", \"mean_values.csv\")\n",
    "                \n",
    "                if not os.path.exists(subcondition_data_path):\n",
    "                    print(f\"Error: Data file {subcondition_data_path} does not exist.\")\n",
    "                    continue\n",
    "                \n",
    "                # Load the subcondition dataframe\n",
    "                df = pd.read_csv(subcondition_data_path)\n",
    "                \n",
    "                # Apply Gaussian smoothing\n",
    "                df.iloc[:, 1:-3] = df.iloc[:, 1:-3].apply(lambda x: gaussian_filter1d(x, sigma=sigma))\n",
    "                \n",
    "                # Rename columns for consistency in plotting\n",
    "                df = df.rename(columns={\"data type [-]_mean\": \"work [J]\", \n",
    "                                        \"correlation length [m]_mean\": \"correlation length [um]\", \n",
    "                                        \"velocity magnitude [m/s]_mean\": \"velocity magnitude [um/s]\"})\n",
    "                \n",
    "                df[\"work [J]\"] = df[\"power [W]_mean\"].cumsum()\n",
    "                \n",
    "                # Limit the frames if specified\n",
    "                df = df.iloc[min_frame:max_frame, :]\n",
    "                \n",
    "                # Plot each subcondition on the same figure\n",
    "                plt.plot(df[\"time (h)\"], df[feature], marker='o', linestyle='-', markersize=1, linewidth=1, label=f'{condition} - {subcondition}')\n",
    "        \n",
    "        plt.xlabel('Time (hours)')\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(f\"All Conditions and Subconditions Combined - {feature}\")\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        plt.legend(loc='best', fontsize='small', ncol=2)  # Adjust legend to fit all entries\n",
    "        plt.savefig(os.path.join(combined_output_dir, f\"combined_{feature.split()[0]}_h.jpg\"), format='jpg', dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for condition in conditions:\n",
    "            for subcondition in subconditions:\n",
    "                # Load the subcondition dataframe again\n",
    "                df = pd.read_csv(os.path.join(data_path, condition, subcondition, \"dataframes_PIV\", \"mean_values.csv\"))\n",
    "                df.iloc[:, 1:-3] = df.iloc[:, 1:-3].apply(lambda x: gaussian_filter1d(x, sigma=sigma))\n",
    "                df = df.rename(columns={\"data type [-]_mean\": \"work [J]\", \n",
    "                                        \"correlation length [m]_mean\": \"correlation length [um]\", \n",
    "                                        \"velocity magnitude [m/s]_mean\": \"velocity magnitude [um/s]\"})\n",
    "                df[\"work [J]\"] = df[\"power [W]_mean\"].cumsum()\n",
    "                df = df.iloc[min_frame:max_frame, :]\n",
    "                \n",
    "                time_in_min = df[\"time (h)\"] * 60  # Convert hours to minutes for the second plot\n",
    "                plt.plot(time_in_min, df[feature], marker='o', linestyle='-', markersize=1, linewidth=1, label=f'{condition} - {subcondition}')\n",
    "        \n",
    "        plt.xlabel('Time (minutes)')\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(f\"All Conditions and Subconditions Combined - {feature}\")\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        plt.legend(loc='best', fontsize='small', ncol=2)  # Adjust legend to fit all entries\n",
    "        plt.savefig(os.path.join(combined_output_dir, f\"combined_{feature.split()[0]}_min.jpg\"), format='jpg', dpi=200)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorgTiffs_Split_dapi(data_path, ['G'], subconditions, file_interval=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['C', 'D', 'H']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature limits and other parameters\n",
    "v = 2E-7\n",
    "velocity_limits = (0, v)\n",
    "other_limits = (-0.0005, 0.0005)\n",
    "time_interval_list = [45, 45, 45]  # time intervals in seconds between frames for each condition\n",
    "skip_frames = 1 ### CHANGE THIS TO SKIP FRAMES\n",
    "\n",
    "\n",
    "velocity_limits = (None, None)\n",
    "other_limits = (None, None)\n",
    "\n",
    "\n",
    "feature_limits = {\n",
    "    # 'u [m/s]': (-v, v), \n",
    "    # 'v [m/s]': (-v, v), \n",
    "    # 'data type [-]': (None, None),\n",
    "    'velocity magnitude [m/s]': velocity_limits,\n",
    "    'vorticity [1/s]': other_limits,\n",
    "    'divergence [1/s]': other_limits,\n",
    "    # 'dcev [1]': (0, 250),\n",
    "    'shear [1/s]': other_limits,\n",
    "    'strain [1/s]': other_limits,\n",
    "    'vector direction [degrees]': (-180, 180),\n",
    "}\n",
    "\n",
    "\n",
    "# Features for PCA and plotting\n",
    "features_pca = [\n",
    "    \"vorticity [1/s]_mean\",\n",
    "    \"velocity magnitude [um/s]\",\n",
    "    \"divergence [1/s]_mean\",\n",
    "    \"shear [1/s]_mean\",\n",
    "    \"strain [1/s]_mean\",\n",
    "    \"correlation length [um]\", \n",
    "    \"power [W]_mean\",\n",
    "    \"work [J]\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PIV data:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing PIV data:  33%|███▎      | 1/3 [00:35<01:10, 35.32s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing PIV data:  67%|██████▋   | 2/3 [01:09<00:34, 34.62s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing PIV data: 100%|██████████| 3/3 [01:19<00:00, 26.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process PIV data\n",
    "process_piv_data(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    feature_limits, \n",
    "    time_interval_list, \n",
    "    min_frame=0, \n",
    "    max_frame=None, \n",
    "    skip_frames=skip_frames, \n",
    "    plot_autocorrelation=False, \n",
    "    frame_rate=1, \n",
    "    heatmaps=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting PIV features: 100%|██████████| 3/3 [00:10<00:00,  3.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# Plot features and PCA\n",
    "plot_PIV_features(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    features_pca, \n",
    "    min_frame=0, \n",
    "    max_frame=None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting averaged PIV features: 100%|██████████| 3/3 [00:05<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# Plot features and PCA\n",
    "plot_PIV_features_averaged(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    features_pca, \n",
    "    min_frame=0, \n",
    "    max_frame=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting PIV data:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting PIV data: 100%|██████████| 3/3 [00:00<00:00, 56.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Plot features and PCA\n",
    "plot_PIV_features_combined(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    features_pca, \n",
    "    min_frame=0, \n",
    "    max_frame=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot features and PCA\n",
    "plot_PIV_features_all_conditions_subconditions(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    features_pca, \n",
    "    min_frame=0, \n",
    "    max_frame=None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
