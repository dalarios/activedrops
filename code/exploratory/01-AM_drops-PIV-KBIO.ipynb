{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('../../activedrops')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import colorcet as cc\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "\n",
    "import pivdrops\n",
    "pivdrops.set_plotting_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of ActiveDROPS PIV data\n",
    "\n",
    "ActiveDROPS imaging is carried out in two channels corresponding to microtubules (Cy5) and fluorescent beads (YFP). For PIV analysis we take only the beads channel and process the images (stored in .tiff files) with Matlab PIVlab. We choose an interrogation window of 128 pixels (~175.34 um), and step sizes of 64 and 32 pixels (87.67 um and 43.84 um, respectively). This means we will have a grid of 128 x 128 velocity vectors pointing the direction of the flow. The imaging interval varies across samples, but for this first example we took one frame every three seconds.\n",
    "\n",
    "In this notebook we show the pipeline used to analyze ActiveDROPS data. PIVlab produces a .txt file per frame, which contains our grid of vectors with velocity data, as well as other features like divergence, vorticity, etc. We first need code that takes our .txt file and gives us pandas dataframe to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"../../data/k401bio-250nM-piv/piv_data/PIVlab_****.txt\"\n",
    "files = sorted(glob.glob(link))\n",
    "\n",
    "df = pd.read_csv(files[0], skiprows=2).fillna(0)\n",
    "\n",
    "df['x [um]'] = df['x [m]'] * 1E6\n",
    "df['y [um]'] = df['y [m]'] * 1E6\n",
    "df['magnitude [um/s]'] = df['magnitude [m/s]'] * 1E6\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, from this table we already have a lot of features we can visualize. For example, the velocity magnitude of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_heatmap(df, feature):\n",
    "\n",
    "    vals = df.pivot(index='y [um]', columns='x [um]', values=feature).values\n",
    "\n",
    "    # Plot the magnitude of the velocity\n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "    im = plt.imshow(vals, cmap='viridis', origin='lower', extent=[-2762/2, 2762/2, -2762/2, 2762/2], vmin=vals.min(), vmax=vals.max())\n",
    "    plt.xlabel('x [um]')\n",
    "    plt.ylabel('y [um]')\n",
    "    cbar = plt.colorbar(im)\n",
    "    cbar.set_label(feature)\n",
    "    plt.title('Velocity magnitude')\n",
    "    plt.show()\n",
    "\n",
    "drop_heatmap(df, \"vorticity [1/s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial autocorrelation\n",
    "\n",
    "Before moving forward with plotting more features, we need to know the relevant scales of the flow, which we can later use to calculate the measured energy of the system over time. Let's calculate the spatial autocorrelation of the velocity data to illustrate the lengths at which vectors are correlated. To do so, we need to follow a few steps.\n",
    "\n",
    "First we need to apply a Fourier Transform to the velocity magnitude of our data, so we convert it from the spatial to the frequency domain. What does that mean? Let's illustrate it with an example:\n",
    "\n",
    "The Fourier transform is a mathematical operation that transforms a time-domain signal into its frequency-domain representation. The formula includes the term $e^{-i \\omega t}$ (where $i$ is the imaginary unit, $ \\omega $ is the angular frequency, and $ t $ is time) because it represents a complex exponential function which can be used to model sinusoids.\n",
    "\n",
    "The use of $ e^{-i \\omega t} $ comes from Euler's formula, which states that:\n",
    "\n",
    "$$\n",
    "e^{i\\theta} = \\cos(\\theta) + i\\sin(\\theta)\n",
    "$$\n",
    "\n",
    "From which we can derive Euler's identity when the angle is half a circunference ($\\pi$):\n",
    "\n",
    "$$\n",
    "e^{i\\pi} = -1\n",
    "$$\n",
    "\n",
    "Euler's formula says that spinning a point around a circle in the complex plane is like a combination of sliding back and forth along two perpendicular lines, one for cosine in the real plane and one for sine in the imaginary plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot Euler's formula with a variable radius\n",
    "def plot_euler(theta, radius):\n",
    "    # Calculate the point on the circle\n",
    "    x = radius * np.cos(theta)\n",
    "    y = radius * np.sin(theta)\n",
    "    \n",
    "    # Set up the figure and axis\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    \n",
    "    # Draw the unit circle scaled by the radius\n",
    "    circle = plt.Circle((0, 0), radius, fill=True, alpha=0.2)\n",
    "    ax.add_artist(circle)\n",
    "    \n",
    "    # Plot the real and imaginary parts\n",
    "    ax.plot([0, x], [0, y], 'r')\n",
    "    ax.plot(x, y, 'ro')\n",
    "    \n",
    "    # Annotations\n",
    "    complex_number = f'{x:.2f} + {y:.2f}i'\n",
    "    ax.annotate(complex_number, xy=(x, y), xytext=(x+0.1, y),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "    \n",
    "    # Fixed limits for the plot\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Grid, legend, and titles\n",
    "    ax.grid(True)\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    ax.axvline(0, color='black', linewidth=0.5)\n",
    "    plt.title(\"Visualization of Euler's Formula\")\n",
    "    plt.xlabel('Real part')\n",
    "    plt.ylabel('Imaginary part')\n",
    "    plt.show()\n",
    "\n",
    "# Create the sliders\n",
    "theta_slider = FloatSlider(min=0, max=2*np.pi, step=0.01, value=np.pi, description='Theta')\n",
    "radius_slider = FloatSlider(min=0.1, max=2, step=0.1, value=1, description='Radius')\n",
    "\n",
    "# Show the interactive plot\n",
    "interact(plot_euler, theta=theta_slider, radius=radius_slider);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The general form of the continuous Fourier transform of a time-domain function $ f(t) $ into a frequency-domain function $ F(\\omega) $ is:\n",
    "\n",
    "$$\n",
    "F(\\omega) = \\int_{-\\infty}^{\\infty} f(t) e^{-i \\omega t} \\, dt\n",
    "$$\n",
    "\n",
    "The negative sign in the exponential ($ e^{-i \\omega t} $) indicates that the transform is analyzing the signal in terms of sinusoids that rotate in the counterclockwise direction, which is the standard positive direction of rotation in the complex plane. This is a convention that ensures when you apply the inverse Fourier transform, you get back your original time-domain signal.\n",
    "\n",
    "\n",
    "In this equation:\n",
    "- $ F(\\omega) $ is the Fourier transform of $ f(t) $.\n",
    "- $ \\omega $ is the angular frequency, related to the frequency $ f $ by $ \\omega = 2\\pi f $.\n",
    "- $ t $ is time.\n",
    "- The integral sums (integrates) the function $ f(t) $ multiplied by a rotating complex exponential across all time.\n",
    "\n",
    "The Fourier transform breaks down $ f(t) $ into its constituent frequencies, and $ F(\\omega) $ tells you the amplitude and phase of each of those frequencies. The magnitude $ |F(\\omega)| $ gives the amplitude spectrum, and the argument $ \\arg(F(\\omega)) $ gives the phase spectrum of the function $ f(t) $.\n",
    "\n",
    "\n",
    "Since we have discrete values, we'll use the Discrete Fourier Transform, but the difference is just computational:\n",
    "\n",
    "$$\n",
    "X(k) = \\sum_{n=0}^{N-1} x(n) \\cdot e^{-i 2 \\pi k n / N}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ X(k) $ is the output of the DFT at frequency $ k $.\n",
    "- $ x(n) $ is the input data at point $ n $.\n",
    "- $ N $ is the total number of points in the data.\n",
    "- The exponential term $ e^{-i 2 \\pi k n / N} $ represents the complex sinusoids (comprising both sine and cosine terms) at frequency $ k $.\n",
    "\n",
    "The operation is performed for each frequency $ k $ from 0 to $ N-1 $, covering the whole spectrum from the lowest to the highest frequency that the grid can represent.\n",
    "\n",
    "From our data, we can do fast Fourier transform with numpy and visualize the energy spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = df.pivot(index='y [um]', columns='x [um]', values=\"magnitude [um/s]\").values\n",
    "\n",
    "v_ft = np.fft.fftshift(np.fft.fft2(v))\n",
    "\n",
    "# Compute the radial energy spectrum (averaging over circular shells in k-space)\n",
    "kx = np.fft.fftshift(np.fft.fftfreq(v.shape[0]))\n",
    "ky = np.fft.fftshift(np.fft.fftfreq(v.shape[1]))\n",
    "kx, ky = np.meshgrid(kx, ky)\n",
    "k = np.sqrt(kx**2 + ky**2)\n",
    "\n",
    "# Define the bins for k (radial wavenumbers)\n",
    "k_bins = np.linspace(0, 0.5, v.shape[0]//2)\n",
    "k_bin_centers = 0.5 * (k_bins[:-1] + k_bins[1:])\n",
    "\n",
    "E_u = np.histogram(k.ravel(), bins=k_bins, weights=(np.abs(v_ft)**2).ravel())[0]\n",
    "\n",
    "# Plotting the average energy spectrum\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.loglog(k_bin_centers, E_u)\n",
    "plt.xlabel('Wavenumber')\n",
    "plt.ylabel('Energy Density')\n",
    "plt.title('Energy Spectrum')\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Autocorrelation Function:\n",
    "\n",
    "The spatial autocorrelation function quantifies how similar the velocities are across different spatial lags in the vector field. If we have a velocity vector field defined as $\\mathbf{v}(x,y)$, where $\\mathbf{v}$ is the velocity at position $(x,y)$, the spatial autocorrelation function $R(\\Delta x, \\Delta y)$ for spatial lags $\\Delta x$ and $\\Delta y$ is given by:\n",
    "\n",
    "$$\n",
    "R(\\Delta x, \\Delta y) = \\sum_{x,y} \\mathbf{v}(x,y) \\cdot \\mathbf{v}(x + \\Delta x, y + \\Delta y)\n",
    "$$\n",
    "\n",
    "This function computes the dot product of velocity vectors at the original and shifted positions, thus incorporating both the magnitude and direction of the velocities into the correlation.\n",
    "\n",
    "### Wiener-Khinchin Theorem for Spatial Data:\n",
    "\n",
    "The Wiener-Khinchin theorem states that the Fourier transform of the autocorrelation function of a stationary process is equal to the power spectral density (PSD) of that process. For spatial data, this relationship allows us to compute the PSD of the velocity field from the spatial autocorrelation function, and vice versa. The theorem for spatial data is expressed as:\n",
    "\n",
    "$$\n",
    "S(\\mathbf{k}) = \\mathcal{F}\\{ R(\\Delta x, \\Delta y) \\}\n",
    "$$\n",
    "\n",
    "where $S(\\mathbf{k})$ represents the spatial power spectral density as a function of the spatial frequency vector $\\mathbf{k}$, and $\\mathcal{F}$ denotes the Fourier transform.\n",
    "\n",
    "Conversely, we can obtain the spatial autocorrelation function from the PSD using the inverse Fourier transform:\n",
    "\n",
    "$$\n",
    "R(\\Delta x, \\Delta y) = \\mathcal{F}^{-1}\\{ S(\\mathbf{k}) \\}\n",
    "$$\n",
    "\n",
    "### Computing the Spatial Autocorrelation Function:\n",
    "\n",
    "To compute the spatial autocorrelation function for a single frame of PIV data:\n",
    "\n",
    "1. **Calculate the Velocity Magnitude**: First, compute the magnitude of the velocity vector at each point in the 2D space.\n",
    "\n",
    "$$\n",
    "\\text{magnitude} = \\sqrt{u^2 + v^2}\n",
    "$$\n",
    "\n",
    "where $u$ and $v$ are the $x$ and $y$ components of the velocity vector, respectively.\n",
    "\n",
    "2. **Compute the 2D Fourier Transform**: Then, apply a 2D Fourier transform to the magnitude of the velocity vectors to move from the spatial domain to the frequency domain.\n",
    "\n",
    "$$\n",
    "F(\\mathbf{k}) = \\mathcal{F}\\{ \\text{magnitude} \\}\n",
    "$$\n",
    "\n",
    "3. **Calculate the Power Spectral Density**: Multiply the Fourier-transformed velocity magnitude by its complex conjugate to get the power spectral density.\n",
    "\n",
    "$$\n",
    "S(\\mathbf{k}) = F(\\mathbf{k}) \\cdot F^*(\\mathbf{k})\n",
    "$$\n",
    "\n",
    "4. **Inverse Fourier Transform**: Finally, apply the inverse 2D Fourier transform to the PSD to obtain the spatial autocorrelation function.\n",
    "\n",
    "$$\n",
    "R(\\Delta x, \\Delta y) = \\mathcal{F}^{-1}\\{ S(\\mathbf{k}) \\}\n",
    "$$\n",
    "\n",
    "This spatial autocorrelation function will describe how velocity magnitudes are correlated with themselves over spatial shifts in the PIV data. It helps to identify patterns and coherent structures within the flow. For example, here we can visualize the spatial autocorrelation of the entire first frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = df.pivot(index='y [um]', columns='x [um]', values=\"magnitude [um/s]\").values\n",
    "\n",
    "# Calculate the autocorrelation function with fourier transform\n",
    "full_product = np.fft.fft2(v) * np.conj(np.fft.fft2(v))\n",
    "\n",
    "# get the real part of the inverse fourier transform (autocorrlation function)\n",
    "inverse = np.real(np.fft.ifft2(full_product))\n",
    "\n",
    "# normalize the autocorrelation function\n",
    "normalized_inverse = inverse / inverse[0, 0]\n",
    "\n",
    "# shift the autocorrelation function so that the center is at the center of the image\n",
    "shifted_inverse =  np.fft.fftshift(normalized_inverse)\n",
    "\n",
    "# plot inverse fourier transform of the product of the fourier transform of the magnitude of the velocity\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "im = plt.imshow(shifted_inverse, cmap='viridis', origin='lower', extent=[-2762/2, 2762/2, -2762/2, 2762/2])\n",
    "plt.xlabel('x [um]')\n",
    "plt.ylabel('y [um]')\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Autocorrelation')\n",
    "plt.title('Autocorrelation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the spatial autocorrelation with a space lag $r$ as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{{inverse}}[r, r] + \\text{{inverse}}[-r, -r]}{\\text{{shape}}[0] \\times \\text{{shape}}[1]}\n",
    "$$\n",
    "\n",
    "Notice that the maximum number of time lags $r$ are 127 because we have that number of points per axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of r values\n",
    "r_values = v.shape[0]//2\n",
    "\n",
    "# Initialize an array to store the autocorrelation results\n",
    "results = np.zeros(r_values)\n",
    "\n",
    "# Compute the autocorrelation for each r value\n",
    "for r in range(r_values):\n",
    "    # Compute the autocorrelation value for the current r value\n",
    "    autocorrelation_value = (inverse[r, r] + inverse[-r, -r]) / (v.shape[0] * v.shape[1])\n",
    "    # Store the autocorrelation value in the results array\n",
    "    results[r] = autocorrelation_value\n",
    "\n",
    "# Normalize the results array\n",
    "results = results / results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the spatial correlation function, a correlation length is discerned, representing the extent over which two points retain significant correlation. This length is derived from the velocity autocorrelation, $A_{\\text{vel}}(r,\\tau)$, fitted to an exponential decay model:\n",
    "$$\n",
    "A_{\\text{vel}}(r,\\tau) = Ae^{-\\frac{\\tau}{B}} + C.\n",
    "$$\n",
    "The correlation length, symbolized by $\\lambda(\\tau)$, is then given by:\n",
    "$$\n",
    "\\lambda(\\tau) = -B \\times \\log \\left( \\frac{0.3-C}{A} \\right).\n",
    "$$\n",
    "Temporal averages of this length, represented by $\\langle \\lambda(\\tau) \\rangle$, furnish an overarching correlation length for the entire experimental dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember that the PIVlab output is a 127x127 grid of vectors, each representing a chunk of distance. How much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervector_distance_microns = (df[\"y [m]\"].max() - df[\"y [m]\"].min()) * 1000000 / v.shape[0]\n",
    "intervector_distance_microns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real distance between vectors, and therefore spatial lags $r$, is 21.75 microns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the exponential decay model\n",
    "def exponential_decay(tau, A, B, C):\n",
    "    return A * np.exp(-tau/B) + C\n",
    "\n",
    "# Fit the results to the exponential decay model\n",
    "params, _ = curve_fit(exponential_decay, np.arange(len(results)), results, maxfev=5000)\n",
    "A, B, C = params\n",
    "fitted_values = exponential_decay(np.arange(r_values), A, B, C)\n",
    "\n",
    "# Compute the correlation length\n",
    "lambda_tau = -B * np.log((0.3 - C) / A) * intervector_distance_microns\n",
    "print(f\"The correlation length is {lambda_tau}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(results)) * intervector_distance_microns\n",
    "\n",
    "# Plot the autocorrelation function\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, results, 'o', label='Autocorrelation')\n",
    "plt.plot(x, fitted_values , label='Exponential fit')\n",
    "plt.axvline(lambda_tau, color='black', linestyle='--', label=f'Correlation length = {np.round(lambda_tau, 2)}')\n",
    "plt.xlabel('r (spatial lags)')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.title('Autocorrelation')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half a millimiter! Now let's calculate the power (joules per second), given by the formula:\n",
    "\n",
    "$$\n",
    "P_{\\text{flow}} \\approx V_0 \\mu \\left( \\frac{\\left< \\vec{v}(r,t) \\right>}{ L } \\right)^2 \n",
    "$$\n",
    "\n",
    "Where:\n",
    "- V0 is volume\n",
    "- µ is the viscosity of water (1mPa*S)\n",
    "- $\\left< \\vec{v}(r,t) \\right>$ is mean velocity \n",
    "- L is correlation length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0 = 2 # µl\n",
    "µ = 1 # mPa*S\n",
    "\n",
    "# Calculate power\n",
    "df[\"Power (W)\"] = v0 * µ * (df[\"magnitude [m/s]\"].mean()/lambda_tau)**2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have plotted a single frame of data and analyzed the correlation length to estimate power. Now let's build a function that takes a frame and gives us an updated dataframe w\n",
    "\n",
    "The goal here is to build a set of functions to analye this and all the future activedrops data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_piv(file, volume=2):\n",
    "\n",
    "    df = pd.read_csv(file, skiprows=2).fillna(0)\n",
    "\n",
    "    df['x [um]'] = df['x [m]'] * 1E6\n",
    "    df['y [um]'] = df['y [m]'] * 1E6\n",
    "    df['u [um/s]'] = df['u [m/s]'] * 1E6\n",
    "    df['v [um/s]'] = df['v [m/s]'] * 1E6\n",
    "    df['magnitude [um/s]'] = df['magnitude [m/s]'] * 1E6\n",
    "\n",
    "    # Obtain square grid of velocity magnitudes\n",
    "    v = df.pivot(index='y [um]', columns='x [um]', values=\"magnitude [um/s]\").values\n",
    "\n",
    "    # Calculate the autocorrelation function with fourier transform\n",
    "    full_product = np.fft.fft2(v) * np.conj(np.fft.fft2(v))\n",
    "\n",
    "    # get the real part of the inverse fourier transform (autocorrlation function)\n",
    "    inverse = np.real(np.fft.ifft2(full_product))\n",
    "\n",
    "    # normalize the autocorrelation function\n",
    "    normalized_inverse = inverse / inverse[0, 0]\n",
    "\n",
    "    # Define the number of r values\n",
    "    r_values = v.shape[0]//2\n",
    "\n",
    "    # Initialize an array to store the autocorrelation results\n",
    "    results = np.zeros(r_values)\n",
    "\n",
    "    # Compute the autocorrelation for each r value\n",
    "    for r in range(r_values):\n",
    "        # Compute the autocorrelation value for the current r value\n",
    "        autocorrelation_value = (inverse[r, r] + inverse[-r, -r]) / (v.shape[0] * v.shape[1])\n",
    "        # Store the autocorrelation value in the results array\n",
    "        results[r] = autocorrelation_value\n",
    "\n",
    "    # Normalize the results array\n",
    "    results = results / results[0]\n",
    "\n",
    "    # Define the exponential decay model\n",
    "    def exponential_decay(tau, A, B, C):\n",
    "        return A * np.exp(-tau/B) + C\n",
    "\n",
    "    # Fit the results to the exponential decay model\n",
    "    params, _ = curve_fit(exponential_decay, np.arange(len(results)), results, maxfev=5000)\n",
    "    A, B, C = params\n",
    "    fitted_values = exponential_decay(np.arange(r_values), A, B, C)\n",
    "\n",
    "    # Obtain the true distance between spatial lags/vectors \n",
    "    intervector_distance_microns = (df[\"y [um]\"].max() - df[\"y [um]\"].min()) / v.shape[0]\n",
    "\n",
    "    # Compute the correlation length (lambda tau)\n",
    "    lambda_tau = -B * np.log((0.3 - C) / A) * intervector_distance_microns\n",
    "\n",
    "    # Add correlation length to dataframe\n",
    "    df[\"correlation length (µm)\"] = lambda_tau\n",
    "\n",
    "    # Calculate power and add to dataframe. Remember this is with m/s, not microns!\n",
    "    v0 = volume # µl\n",
    "    µ = 1 # mPa*S\n",
    "    df[\"Power (W)\"] = v0 * µ * (df[\"magnitude [m/s]\"].mean()/lambda_tau)**2\n",
    "\n",
    "    # Add mean velocity\n",
    "    df[\"mean velocity [um/s]\"] = df[\"magnitude [um/s]\"].mean()\n",
    "\n",
    "    # Add file name\n",
    "    df[\"file name\"] = os.path.basename(file).split('.')[0]\n",
    "\n",
    "    # Simply reorganize\n",
    "    df = pd.concat([df.iloc[:, 12:], df.iloc[:, 4:12]], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "link = \"../../data/k401bio-250nM-piv/piv_data/PIVlab_****.txt\"\n",
    "files = sorted(glob.glob(link))\n",
    "df = df_piv(files[50])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piv_heatmap(df, feature, vmax=10, output_dir=None):\n",
    "\n",
    "    vals = df.pivot(index='y [um]', columns='x [um]', values=feature).values\n",
    "\n",
    "    # Plot the magnitude of the velocity\n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "    im = plt.imshow(vals, cmap='viridis', origin='lower', extent=[-2762/2, 2762/2, -2762/2, 2762/2], vmin=0, vmax=vmax)\n",
    "    plt.xlabel('x [um]')\n",
    "    plt.ylabel('y [um]')\n",
    "    cbar = plt.colorbar(im)\n",
    "    cbar.set_label(feature)\n",
    "    plt.title(f'ActiveDROPS PIV - {df[\"file name\"][0]}')\n",
    "    \n",
    "    if output_dir:\n",
    "        plt.savefig(output_dir, format='jpg', dpi=250)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "output_dir = \"../../data/k401bio-250nM-piv/plots/heatmap.jpg\"\n",
    "piv_heatmap(df, \"magnitude [um/s]\", vmax=10, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have functions to process and plot single frames. Now let's go on to write a function that stores every dataframe per movie frame in a list, and generates plots from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting utilities\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import colorcet as cc\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "def df_piv(file, volume=2):\n",
    "    \"\"\"\n",
    "    Processes a PIV data file and computes various parameters.\n",
    "\n",
    "    Args:\n",
    "    - file (str): Path to the PIV data file.\n",
    "    - volume (float): The volume in microliters for power calculation.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Processed DataFrame with additional computed columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read and preprocess the dataframe\n",
    "    df = pd.read_csv(file, skiprows=2).fillna(0)\n",
    "    # Convert measurements to micrometers and micrometers per second\n",
    "    df['x [um]'] = df['x [m]'] * 1E6\n",
    "    df['y [um]'] = df['y [m]'] * 1E6\n",
    "    df['u [um/s]'] = df['u [m/s]'] * 1E6\n",
    "    df['v [um/s]'] = df['v [m/s]'] * 1E6\n",
    "    df['magnitude [um/s]'] = df['magnitude [m/s]'] * 1E6\n",
    "\n",
    "    # Obtain square grid of velocity magnitudes\n",
    "    v = df.pivot(index='y [um]', columns='x [um]', values=\"magnitude [um/s]\").values\n",
    "\n",
    "    # Calculate the autocorrelation function with Fourier transform\n",
    "    full_product = np.fft.fft2(v) * np.conj(np.fft.fft2(v))\n",
    "    inverse = np.real(np.fft.ifft2(full_product))  # Real part of the inverse Fourier transform\n",
    "    normalized_inverse = inverse / inverse[0, 0]   # Normalize the autocorrelation function\n",
    "\n",
    "    # Define the number of r values and initialize an array for the results\n",
    "    r_values = v.shape[0] // 2\n",
    "    results = np.zeros(r_values)\n",
    "\n",
    "    # Compute the autocorrelation for each r value\n",
    "    for r in range(r_values):\n",
    "        autocorrelation_value = (inverse[r, r] + inverse[-r, -r]) / (v.shape[0] * v.shape[1])\n",
    "        results[r] = autocorrelation_value\n",
    "\n",
    "    # Normalize the results array\n",
    "    results = results / results[0]\n",
    "\n",
    "    # Fit the results to an exponential decay model\n",
    "    def exponential_decay(tau, A, B, C):\n",
    "        return A * np.exp(-tau / B) + C\n",
    "\n",
    "    params, _ = curve_fit(exponential_decay, np.arange(len(results)), results, maxfev=5000)\n",
    "    A, B, C = params\n",
    "    fitted_values = exponential_decay(np.arange(r_values), A, B, C)\n",
    "\n",
    "    # Compute correlation length and other parameters\n",
    "    intervector_distance_microns = (df[\"y [um]\"].max() - df[\"y [um]\"].min()) / v.shape[0]\n",
    "    lambda_tau = -B * np.log((0.3 - C) / A) * intervector_distance_microns\n",
    "    df[\"correlation length (µm)\"] = lambda_tau\n",
    "\n",
    "    v0 = volume  # µl\n",
    "    µ = 1        # mPa*S\n",
    "    df[\"Power (W)\"] = v0 * µ * (df[\"magnitude [m/s]\"].mean() / lambda_tau) ** 2\n",
    "    df[\"mean velocity [um/s]\"] = df[\"magnitude [um/s]\"].mean()\n",
    "    df[\"file name\"] = os.path.basename(file).split('.')[0]\n",
    "\n",
    "    # Reorganize DataFrame\n",
    "    df = pd.concat([df.iloc[:, 12:], df.iloc[:, 4:12]], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def piv_heatmap(df, feature, vmin, vmax, output_dir=None):\n",
    "    \"\"\"\n",
    "    Generates and saves/renders a heatmap of a specified feature from the PIV data.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): DataFrame containing PIV data.\n",
    "    - feature (str): Column name for the feature to plot.\n",
    "    - vmin (float): Minimum value for colormap scaling.\n",
    "    - vmax (float): Maximum value for colormap scaling.\n",
    "    - output_dir (str, optional): Directory to save the plot. If None, the plot is shown.\n",
    "    \"\"\"\n",
    "\n",
    "    vals = df.pivot(index='y [um]', columns='x [um]', values=feature).values\n",
    "\n",
    "    # Plot setup\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    im = plt.imshow(vals, cmap='viridis', origin='lower', extent=[-2762/2, 2762/2, -2762/2, 2762/2], vmin=vmin, vmax=vmax)\n",
    "    plt.xlabel('x [um]')\n",
    "    plt.ylabel('y [um]')\n",
    "    cbar = plt.colorbar(im)\n",
    "    cbar.set_label(feature)\n",
    "    plt.title(f'ActiveDROPS PIV - {df[\"file name\"][0]}')\n",
    "    \n",
    "    # Save or show the plot\n",
    "    if output_dir:\n",
    "        os.makedirs(os.path.dirname(output_dir), exist_ok=True)\n",
    "        plt.savefig(output_dir, format='jpg', dpi=250)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def process_piv_files(path, max_frame=10):\n",
    "    \"\"\"\n",
    "    Processes multiple PIV files from a given directory.\n",
    "\n",
    "    Args:\n",
    "    - path (str): Path pattern to the PIV files.\n",
    "\n",
    "    Returns:\n",
    "    - List[DataFrame]: List of processed DataFrames.\n",
    "    \"\"\"\n",
    "    files = sorted(glob.glob(path))\n",
    "    dataframes = [df_piv(file) for file in files[:max_frame]]\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def generate_heatmaps(dataframes, feature, output_dir_base=None, vmin=None, vmax=None):\n",
    "    \"\"\"\n",
    "    Generates heatmaps for a list of DataFrames and a specified feature.\n",
    "\n",
    "    Args:\n",
    "    - dataframes (List[DataFrame]): List of DataFrame objects to plot.\n",
    "    - feature (str): The feature to plot.\n",
    "    - output_dir_base (str, optional): Base directory to save heatmaps. If None, heatmaps are displayed.\n",
    "    - vmin (float, optional): Minimum value for colormap scaling. If None, computed from data.\n",
    "    - vmax (float, optional): Maximum value for colormap scaling. If None, computed from data.\n",
    "    \"\"\"\n",
    "    # Removing units and special characters from the feature name for folder creation\n",
    "    feature_name_for_folder = ''.join(filter(str.isalnum, feature.split('[')[0])).strip()\n",
    "\n",
    "    # Calculate vmin and vmax if not provided\n",
    "    if vmin is None:\n",
    "        vmin = min(df[feature].min() for df in dataframes)\n",
    "    if vmax is None:\n",
    "        vmax = max(df[feature].max() for df in dataframes)\n",
    "\n",
    "    for i, df in enumerate(dataframes):\n",
    "        output_dir = None\n",
    "        if output_dir_base:\n",
    "            # Creating a specific folder for the feature\n",
    "            feature_folder = os.path.join(output_dir_base, feature_name_for_folder)\n",
    "            os.makedirs(feature_folder, exist_ok=True)\n",
    "            output_dir = os.path.join(feature_folder, f\"heatmap_{i}.jpg\")\n",
    "        piv_heatmap(df, feature, vmin=vmin, vmax=vmax, output_dir=output_dir)\n",
    "\n",
    "# Example usage\n",
    "input_dir = \"../../data/k401bio-250nM-piv/piv_data/PIVlab_****.txt\"\n",
    "dataframes = process_piv_files(input_dir, max_frame=10)\n",
    "\n",
    "output_dir = \"../../data/k401bio-250nM-piv/plots/\"\n",
    "generate_heatmaps(dataframes, 'simple strain [1/s]', output_dir_base=output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataframes[0].columns[9:]:\n",
    "    generate_heatmaps(dataframes, i, output_dir_base=output_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! Now we can process all the PIV output! There a couple interesting plots we're missing, namely velocity and power over time. Let's proceed to obtain that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piv_time_series(dataframes, time_interval_min=0.05):\n",
    "\n",
    "    df_ts = pd.DataFrame(columns=['file name', 'Power (W)', 'mean velocity [um/s]'])\n",
    "\n",
    "    for df in dataframes:\n",
    "        file_name = df.loc[0, 'file name']\n",
    "        power = df.loc[0, 'Power (W)']\n",
    "        velocity = df.loc[0, 'mean velocity [um/s]']\n",
    "        # Make sure the column names match those in new_df\n",
    "        new_row = pd.DataFrame({'file name': [file_name], 'Power (W)': [power], 'mean velocity [um/s]': [velocity]})\n",
    "        df_ts = pd.concat([df_ts, new_row], ignore_index=True)\n",
    "\n",
    "    df_ts = df_ts.reset_index(drop=False)\n",
    "    df_ts = df_ts.rename(columns={'index': 'time (min)'})\n",
    "    df_ts['time (min)'] = df_ts['time (min)'] * time_interval_min\n",
    "    return df_ts\n",
    "\n",
    "df_ts = piv_time_series(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "def plot_time_series(df_ts, feature, sigma=0):\n",
    "\n",
    "    if feature == 'velocity':\n",
    "        y = gaussian_filter(df_ts['mean velocity [um/s]'], sigma=sigma)\n",
    "    if feature == 'power':\n",
    "        y = gaussian_filter(df_ts['Power (W)'], sigma=sigma)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    ax.plot(df_ts['time (min)'], y, label='K401-biotin')\n",
    "    ax.fill_between(df_ts['time (min)'], y, alpha=0.2)\n",
    "\n",
    "    ax.set_xlabel('time (min)')\n",
    "    ax.set_ylabel('Velocity (µm/s)')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_time_series(df_ts, 'power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now we have a whole set of functions to process our PIV data! Let's make these functions a package called `pivdrops`. Here is the implementation:\n",
    "\n",
    "#### 1. Import relevant pachages.\n",
    "\n",
    "The pivdrops package contains most of the imports we need for this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import relevant libraries\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('../../activedrops')\n",
    "\n",
    "## Including ourselves\n",
    "import pivdrops\n",
    "pivdrops.set_plotting_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Process PIV files \n",
    "\n",
    "The function takes an input as follows, which is the output of PIVlab. Each .txt frame in the directory correspont to a movie frame, and our function turns it into an useful pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the PIV files\n",
    "input_dir = \"../../data/k401bio-250nM-piv/piv_data/PIVlab_****.txt\"\n",
    "output_dir = \"../../data/k401bio-250nM-piv/plots/\"\n",
    "\n",
    "dataframes = pivdrops.process_piv_files(input_dir, max_frame=None)\n",
    "\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Plot vector fields (velocity & derivatives)\n",
    "\n",
    "Thhis function takes all dataframes and turns the features into movies (created in your output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pivdrops.generate_heatmaps(dataframes, i, output_dir_base=output_dir) for i in dataframes[0].columns[10:]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Generate a single dataframe with velocity and power\n",
    "\n",
    "We can also multiply these two by time to get distance and work respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pivdrops.piv_time_series(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot global dynamics\n",
    "\n",
    "Called global because come from mean values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of feature names to plot\n",
    "features_to_plot = ['velocity', 'power', 'distance', 'work']\n",
    "\n",
    "for feature in features_to_plot:\n",
    "    pivdrops.plot_time_series(df, feature, output_dir=output_dir, sigma=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a super function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageEnhance, ImageOps  # Added ImageOps here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append('../../activedrops')\n",
    "import pivdrops\n",
    "pivdrops.set_plotting_style()\n",
    "from natsort import natsorted  # Import for natural sorting\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "from PIL import Image, ImageEnhance, ImageOps  # Added ImageOps here\n",
    "\n",
    "\n",
    "def sorted_alphanumeric(data):\n",
    "    \"\"\"\n",
    "    Helper function to sort data in human-readable alphanumeric order.\n",
    "    \"\"\"\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(data, key=alphanum_key)\n",
    "\n",
    "def make_movies_from_features(base_directory, fps):\n",
    "    # Find all subdirectories in the base directory\n",
    "    subdirectories = [d for d in os.listdir(base_directory) if os.path.isdir(os.path.join(base_directory, d))]\n",
    "\n",
    "    for feature in subdirectories:\n",
    "        feature_directory = os.path.join(base_directory, feature)\n",
    "        output_filename = os.path.join(base_directory, f\"{feature}.avi\")\n",
    "\n",
    "        # Get all the .jpg files from the feature directory\n",
    "        images = [img for img in os.listdir(feature_directory) if img.endswith(\".jpg\")]\n",
    "\n",
    "        # Skip if no images are found\n",
    "        if not images:\n",
    "            print(f\"No images found in {feature_directory}, skipping movie creation.\")\n",
    "            continue\n",
    "\n",
    "        # Sort the images in alphanumeric order\n",
    "        images = sorted_alphanumeric(images)\n",
    "\n",
    "        # Read the first image to get the width and height\n",
    "        frame = cv2.imread(os.path.join(feature_directory, images[0]))\n",
    "        height, width, layers = frame.shape\n",
    "\n",
    "        # Define the codec and create VideoWriter object\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(output_filename, fourcc, fps, (width, height))\n",
    "\n",
    "        # Loop through all the images and add them to the video\n",
    "        for image in images:\n",
    "            frame = cv2.imread(os.path.join(feature_directory, image))\n",
    "            out.write(frame)\n",
    "\n",
    "        # Release everything when the job is finished\n",
    "        out.release()\n",
    "\n",
    "def save_dataframes(dataframes, output_dir, prefix='df'):\n",
    "    \"\"\"Saves a list of DataFrames to the specified directory.\"\"\"\n",
    "    dataframes_dir = os.path.join(output_dir, 'dataframes')\n",
    "    os.makedirs(dataframes_dir, exist_ok=True)\n",
    "\n",
    "    for i, df in enumerate(dataframes):\n",
    "        df_path = os.path.join(dataframes_dir, f'{prefix}_{i}.csv')\n",
    "        df.to_csv(df_path, index=False)\n",
    "\n",
    "def load_dataframes(output_dir, prefix='df'):\n",
    "    \"\"\"Loads DataFrames from the specified directory.\"\"\"\n",
    "    dataframes_dir = os.path.join(output_dir, 'dataframes')\n",
    "    if not os.path.exists(dataframes_dir):\n",
    "        return None\n",
    "\n",
    "    df_files = sorted(glob.glob(os.path.join(dataframes_dir, f'{prefix}_*.csv')))\n",
    "    if not df_files:\n",
    "        return None\n",
    "\n",
    "    return [pd.read_csv(df_file) for df_file in df_files]\n",
    "\n",
    "def convert_images(input_dir, output_dir, max_frame=None, brightness_factor=1, contrast_factor=1):\n",
    "    \"\"\"Converts and adjusts images from input_dir and saves them in output_dir.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    input_files = natsorted(glob.glob(os.path.join(input_dir, '*.jpg')))\n",
    "\n",
    "    if max_frame is not None:\n",
    "        input_files = input_files[:max_frame]\n",
    "\n",
    "    output_files = natsorted(glob.glob(os.path.join(output_dir, '*.tif')))\n",
    "\n",
    "    if len(input_files) == len(output_files):\n",
    "        print(f\"Conversion already completed for {output_dir}. Skipping...\")\n",
    "        return\n",
    "\n",
    "    num_digits = len(str(len(input_files)))\n",
    "\n",
    "    for i, file_name in enumerate(input_files):\n",
    "        image = Image.open(file_name).convert(\"L\")\n",
    "        image_resized = image.resize((2048, 2048), Image.LANCZOS)\n",
    "\n",
    "        # Adjust brightness\n",
    "        enhancer = ImageEnhance.Brightness(image_resized)\n",
    "        image_brightened = enhancer.enhance(brightness_factor)\n",
    "\n",
    "        # Adjust contrast\n",
    "        enhancer = ImageEnhance.Contrast(image_brightened)\n",
    "        image_contrasted = enhancer.enhance(contrast_factor)\n",
    "\n",
    "        padded_index = str(i + 1).zfill(num_digits)\n",
    "        base_file_name = f'converted_image_{padded_index}.tif'\n",
    "        processed_image_path = os.path.join(output_dir, base_file_name)\n",
    "        image_contrasted.save(processed_image_path, format='TIFF', compression='tiff_lzw')\n",
    "\n",
    "def combine_timeseries_dataframes(base_data_dir, conditions, subconditions):\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            file_path = os.path.join(base_data_dir, condition, subcondition, 'plots', 'dataframes', 'timeseries_df_0.csv')\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['Condition'] = f'{condition} {subcondition}'\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def process_piv_workflow(conditions, subconditions, base_data_dir, max_frame=None, volume_ul=2, brightness_factor=0.8, contrast_factor=0.8):\n",
    "    \"\"\"Main workflow to process PIV data.\"\"\"\n",
    "\n",
    "    feature_limits = {\n",
    "        'magnitude [um/s]': (0, 10),\n",
    "        'vorticity [1/s]': (-0.03, 0.03),\n",
    "        'divergence [1/s]': (-0.03, 0.03),\n",
    "        # 'dcev [1]': (0, 250),\n",
    "        'simple shear [1/s]': (-0.03, 0.03),\n",
    "        'simple strain [1/s]': (-0.03, 0.03),\n",
    "        'vector direction [degrees]': (-180, 180),\n",
    "    }\n",
    "    \n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            # Define directories\n",
    "            input_image_dir = os.path.join(base_data_dir, condition, subcondition, \"piv_movie\")\n",
    "            output_image_dir = os.path.join(base_data_dir, condition, subcondition, \"piv_movie_8bit_2048x2048\")\n",
    "            output_dir = os.path.join(base_data_dir, condition, subcondition, \"plots\")\n",
    "\n",
    "            # Convert images if not already done\n",
    "            convert_images(input_image_dir, output_image_dir, max_frame, brightness_factor, contrast_factor)\n",
    "\n",
    "            # Process PIV files if not already done\n",
    "            dataframes_path = os.path.join(output_dir, 'dataframes', 'df_0.csv')\n",
    "            if not os.path.exists(dataframes_path):\n",
    "                input_dir = os.path.join(base_data_dir, condition, subcondition, \"piv_data\", \"PIVlab_****.txt\")\n",
    "                dataframes = pivdrops.process_piv_files(input_dir, volume=volume_ul, max_frame=max_frame)\n",
    "                # save_dataframes(dataframes, output_dir)\n",
    "            else:\n",
    "                dataframes = [pd.read_csv(dataframes_path)]\n",
    "\n",
    "            # Process time series data if not already done\n",
    "            timeseries_df_path = os.path.join(output_dir, 'dataframes', 'timeseries_df_0.csv')\n",
    "            if not os.path.exists(timeseries_df_path):\n",
    "                df = pivdrops.piv_time_series(dataframes, time_interval_seconds=3)\n",
    "                save_dataframes([df], output_dir, prefix='timeseries_df')\n",
    "            else:\n",
    "                df = pd.read_csv(timeseries_df_path)\n",
    "\n",
    "            # Plot time series for each feature\n",
    "            combined_df = combine_timeseries_dataframes(base_data_dir, conditions, subconditions)\n",
    "\n",
    "            # Plot combined time series data\n",
    "            for feature in ['velocity', 'power', 'distance', 'work']:\n",
    "                pivdrops.plot_combined_time_series(combined_df, feature, sigma=1, output_dir=output_dir)  # Specify output_dir if needed\n",
    "\n",
    "            # Generate heatmaps for each feature\n",
    "            for feature, (vmin, vmax) in feature_limits.items():\n",
    "                pivdrops.generate_heatmaps(dataframes, feature, vmin=vmin, vmax=vmax, output_dir_base=output_dir, image_path=output_image_dir)\n",
    "\n",
    "            # Make movies\n",
    "            make_movies_from_features(output_dir, fps=120)\n",
    "\n",
    "                \n",
    "\n",
    "# Example usage\n",
    "conditions = ['k401']\n",
    "subconditions = ['rep1']\n",
    "base_data_dir = \"../../data/02-ActiveDROPSfig2-K401_Kif3/\"\n",
    "process_piv_workflow(conditions, subconditions, base_data_dir, max_frame=None, volume_ul=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../activedrops')\n",
    "import pivdrops\n",
    "pivdrops.set_plotting_style()\n",
    "\n",
    "velocity_limits = (0, 10)\n",
    "other_limits = (-0.01, 0.01)\n",
    "\n",
    "# velocity_limits = (0, None)\n",
    "# other_limits = (None, None)\n",
    "\n",
    "feature_limits = {\n",
    "    'magnitude [um/s]': velocity_limits,\n",
    "    'vorticity [1/s]': other_limits,\n",
    "    'divergence [1/s]': other_limits,\n",
    "    'dcev [1]': (0, 250),\n",
    "    'simple shear [1/s]': other_limits,\n",
    "    'simple strain [1/s]': other_limits,\n",
    "    'vector direction [degrees]': (-180, 180),\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "conditions = ['250nM']\n",
    "subconditions = ['2ul']\n",
    "data_path = \"../../data/01-k401-biotin_strep/\"\n",
    "pivdrops.process_piv_data(data_path, max_frame=None, conditions=conditions, subconditions=subconditions, time_interval_seconds=3, feature_limits=feature_limits, frame_rate=120)\n",
    "pivdrops.plot_combined_timeseries(conditions, subconditions, data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
