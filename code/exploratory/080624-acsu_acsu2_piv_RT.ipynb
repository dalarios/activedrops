{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File management\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import norm\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "\n",
    "# Utilities\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, cpu_count\n",
    "mp.set_start_method('fork', force=True)\n",
    "from ipywidgets import interact, FloatSlider, Layout, interactive\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import cv2\n",
    "from natsort import natsorted\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "def reorgTiffsToOriginal(data_path, conditions, subconditions):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions (list): List of conditions.\n",
    "        subconditions (list): List of subconditions.\n",
    "        \n",
    "    This function renames the subconditions as PosX and moves the raw data to the \"original\" folder.\n",
    "    \"\"\"\n",
    "    for condition in conditions:\n",
    "        # Get the actual subconditions in the directory\n",
    "        actual_subconditions = [name for name in os.listdir(os.path.join(data_path, condition)) if os.path.isdir(os.path.join(data_path, condition, name))]\n",
    "        \n",
    "        # Rename the actual subconditions to match the subconditions in your list\n",
    "        for i, actual_subcondition in enumerate(sorted(actual_subconditions)):\n",
    "            os.rename(os.path.join(data_path, condition, actual_subcondition), os.path.join(data_path, condition, subconditions[i]))\n",
    "        \n",
    "        for subcondition in subconditions:\n",
    "            # Construct the path to the subcondition directory\n",
    "            subcondition_path = os.path.join(data_path, condition, subcondition)\n",
    "            \n",
    "            # Create the path for the \"original\" directory within the subcondition directory\n",
    "            original_dir_path = os.path.join(subcondition_path, \"original\")\n",
    "            \n",
    "            # Always create the \"original\" directory\n",
    "            os.makedirs(original_dir_path, exist_ok=True)\n",
    "            \n",
    "            # Iterate over all files in the subcondition directory\n",
    "            for filename in os.listdir(subcondition_path):\n",
    "                # Check if the file is a .tif file\n",
    "                if filename.endswith(\".tif\"):\n",
    "                    # Construct the full path to the file\n",
    "                    file_path = os.path.join(subcondition_path, filename)\n",
    "                    \n",
    "                    # Construct the path to move the file to\n",
    "                    destination_path = os.path.join(original_dir_path, filename)\n",
    "                    \n",
    "                    # Move the file to the \"original\" directory\n",
    "                    shutil.move(file_path, destination_path)\n",
    "            print(f\"Moved .tif files from {subcondition_path} to {original_dir_path}\")\n",
    "\n",
    "\n",
    "def prepare_conditions(data_path, num_reps):\n",
    "    # List conditions while ignoring 'output_data'\n",
    "    conditions = natsorted([\n",
    "        f for f in os.listdir(data_path) \n",
    "        if os.path.isdir(os.path.join(data_path, f)) and f != 'output_data'\n",
    "    ])\n",
    "    \n",
    "    # Generate subconditions list based on num_reps\n",
    "    subconditions = [f\"Rep{x}\" for x in range(1, num_reps + 1)]\n",
    "    \n",
    "    return conditions, subconditions\n",
    "\n",
    "\n",
    "def reorgTiffs_Split_cy5_gfp(data_path, conditions, subconditions, file_interval=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_path (str): Path to the data directory.\n",
    "        conditions (list): List of conditions.\n",
    "        subconditions (list): List of subconditions.\n",
    "        file_interval (int or None): Option to copy every nth file. If None, this feature is not used.\n",
    "\n",
    "    This function copies 'cy5' and 'gfp' images from the 'original' folder into\n",
    "    their respective folders ('cy5' and 'gfp'), using the specified interval.\n",
    "    \"\"\"\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            # Construct the path to the 'original' directory within the subcondition\n",
    "            original_dir_path = os.path.join(data_path, condition, subcondition, \"original\")\n",
    "\n",
    "            if not os.path.exists(original_dir_path):\n",
    "                print(f\"Error: The original directory {original_dir_path} does not exist.\")\n",
    "                continue\n",
    "\n",
    "            # Create separate directories for channels\n",
    "            gfp_dir = os.path.join(data_path, condition, subcondition, \"gfp\")\n",
    "            cy5_dir = os.path.join(data_path, condition, subcondition, \"cy5\")\n",
    "            os.makedirs(gfp_dir, exist_ok=True)\n",
    "            os.makedirs(cy5_dir, exist_ok=True)\n",
    "\n",
    "            # Separate lists for cy5 and gfp files\n",
    "            cy5_files = []\n",
    "            gfp_files = []\n",
    "\n",
    "            # Iterate over all files in the original directory\n",
    "            file_list = sorted(os.listdir(original_dir_path))\n",
    "            for filename in file_list:\n",
    "                # Check if the file is a .tif file\n",
    "                if filename.endswith(\".tif\"):\n",
    "                    if \"Cy5\" in filename:\n",
    "                        cy5_files.append(filename)\n",
    "                    elif \"GFP\" in filename:\n",
    "                        gfp_files.append(filename)\n",
    "\n",
    "            # Copy files based on the file_interval for each channel\n",
    "            if file_interval is None:\n",
    "                file_interval = 1  # Copy all files if no interval is set\n",
    "\n",
    "            for idx, filename in enumerate(cy5_files):\n",
    "                if idx % file_interval == 0:\n",
    "                    file_path = os.path.join(original_dir_path, filename)\n",
    "                    shutil.copy(file_path, os.path.join(cy5_dir, filename))\n",
    "\n",
    "            for idx, filename in enumerate(gfp_files):\n",
    "                if idx % file_interval == 0:\n",
    "                    file_path = os.path.join(original_dir_path, filename)\n",
    "                    shutil.copy(file_path, os.path.join(gfp_dir, filename))\n",
    "\n",
    "            print(f\"Copied every {file_interval}th 'cy5' and 'gfp' file from {original_dir_path} into separate folders.\")\n",
    "\n",
    "\n",
    "# Convert a single image (helper function for multiprocessing)\n",
    "def process_single_image(file_name, output_dir, brightness_factor, contrast_factor, num_digits, i):\n",
    "    image = Image.open(file_name).convert(\"L\")\n",
    "    image_resized = image.resize((2048, 2048), Image.LANCZOS)\n",
    "\n",
    "    enhancer = ImageEnhance.Brightness(image_resized)\n",
    "    image_brightened = enhancer.enhance(brightness_factor)\n",
    "    enhancer = ImageEnhance.Contrast(image_brightened)\n",
    "    image_contrasted = enhancer.enhance(contrast_factor)\n",
    "\n",
    "    padded_index = str(i + 1).zfill(num_digits)\n",
    "    base_file_name = f'converted_image_{padded_index}.tif'\n",
    "    processed_image_path = os.path.join(output_dir, base_file_name)\n",
    "    image_contrasted.save(processed_image_path, format='TIFF', compression='tiff_lzw')\n",
    "\n",
    "\n",
    "# Convert PIVlab images to the right size using multiprocessing\n",
    "def convert_images(data_path, conditions, subconditions, max_frame, brightness_factor=1, contrast_factor=1, skip_frames=1):\n",
    "    for condition in tqdm(conditions, desc=\"Conditions\", leave=False):\n",
    "        for subcondition in tqdm(subconditions, desc=\"Subconditions\", leave=False):\n",
    "            input_dir = os.path.join(data_path, condition, subcondition, \"piv_movie\")\n",
    "            output_dir = os.path.join(data_path, condition, subcondition, \"piv_movie_converted\")\n",
    "\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            input_files = natsorted(glob.glob(os.path.join(input_dir, '*.jpg')))\n",
    "\n",
    "            if max_frame:\n",
    "                input_files = input_files[:max_frame]\n",
    "\n",
    "            # Apply frame skipping\n",
    "            input_files = input_files[::skip_frames]\n",
    "\n",
    "            output_files = natsorted(glob.glob(os.path.join(output_dir, '*.tif')))\n",
    "            if len(input_files) <= len(output_files):\n",
    "                print(f\"Conversion might already be completed or partial for {output_dir}. Continuing...\")\n",
    "                # Optional: Add logic to check and continue incomplete work.\n",
    "\n",
    "            num_digits = len(str(len(input_files)))\n",
    "\n",
    "            # Use all available cores\n",
    "            with Pool(cpu_count()) as pool:\n",
    "                list(tqdm(pool.starmap(process_single_image, [(file_name, output_dir, brightness_factor, contrast_factor, num_digits, i) for i, file_name in enumerate(input_files)]), total=len(input_files), desc=\"Converting Images\", leave=False))\n",
    "\n",
    "\n",
    "# Helper function to plot autocorrelation\n",
    "def plot_autocorrelation_values(data_path, condition, subcondition, frame_id, lambda_tau, results, fitted_values, intervector_distance_microns):\n",
    "    output_directory_dfs = os.path.join(data_path, condition, subcondition, \"autocorrelation_plots\")\n",
    "    os.makedirs(output_directory_dfs, exist_ok=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    x_values = np.arange(len(results)) * intervector_distance_microns * 1E6\n",
    "\n",
    "    plt.plot(x_values, results, label='Autocorrelation Values', marker='o', linestyle='-', markersize=5)\n",
    "    plt.plot(x_values, fitted_values, label='Fitted Exponential Decay', linestyle='--', color='red')\n",
    "    plt.axvline(x=lambda_tau, color='green', linestyle='-.', label=f'Correlation Length = {lambda_tau:.2f} µm')\n",
    "\n",
    "    plt.xlabel('Scaled Lag (µm)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.title(f'Autocorrelation Function and Fitted Exponential Decay (Frame {frame_id})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    # plt.ylim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = os.path.join(output_directory_dfs, f'autocorrelation_frame_{frame_id}.jpg')\n",
    "    plt.savefig(filename, dpi=200, format='jpg')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Helper function to calculate correlation length\n",
    "def correlation_length(data_frame):\n",
    "    # Reshaping the data frame to a 2D grid and normalizing\n",
    "    v = data_frame.pivot(index='y [m]', columns='x [m]', values=\"velocity magnitude [m/s]\").values\n",
    "    v -= np.mean(v)  # Centering the data\n",
    "\n",
    "    # FFT to find the power spectrum and compute the autocorrelation\n",
    "    fft_v = np.fft.fft2(v)\n",
    "    autocorr = np.fft.ifft2(fft_v * np.conj(fft_v))\n",
    "    autocorr = np.real(autocorr) / np.max(np.real(autocorr))  # Normalize the autocorrelation\n",
    "\n",
    "    # Preparing to extract the autocorrelation values along the diagonal\n",
    "    r_values = min(v.shape) // 2\n",
    "    results = np.zeros(r_values)\n",
    "    for r in range(r_values):\n",
    "        # Properly average over symmetric pairs around the center\n",
    "        autocorrelation_value = (autocorr[r, r] + autocorr[-r, -r]) / 2\n",
    "        results[r] = autocorrelation_value\n",
    "\n",
    "    # Normalize the results to start from 1\n",
    "    results /= results[0]\n",
    "\n",
    "    # Exponential decay fitting to extract the correlation length\n",
    "    def exponential_decay(x, A, B, C):\n",
    "        return A * np.exp(-x / B) + C\n",
    "\n",
    "    # Fit parameters and handling potential issues with initial parameter guesses\n",
    "    try:\n",
    "        params, _ = curve_fit(exponential_decay, np.arange(len(results)), results, p0=(1, 10, 0), maxfev=5000)\n",
    "    except RuntimeError:\n",
    "        # Handle cases where the curve fit does not converge\n",
    "        params = [np.nan, np.nan, np.nan]  # Use NaN to indicate the fit failed\n",
    "\n",
    "    A, B, C = params\n",
    "    fitted_values = exponential_decay(np.arange(r_values), *params)\n",
    "\n",
    "    # Calculate the correlation length\n",
    "    intervector_distance_microns = ((data_frame[\"y [m]\"].max() - data_frame[\"y [m]\"].min()) / v.shape[0])\n",
    "    if B > 0 and A != C:  # Ensure valid values for logarithmic calculation\n",
    "        lambda_tau = -B * np.log((0.3 - C) / A) * intervector_distance_microns\n",
    "    else:\n",
    "        lambda_tau = np.nan  # Return NaN if parameters are not suitable for calculation\n",
    "\n",
    "    return lambda_tau, results, fitted_values, intervector_distance_microns\n",
    "\n",
    "\n",
    "# Load PIV data from PIVlab into dataframes\n",
    "def load_piv_data(data_path, condition, subcondition, min_frame=0, max_frame=None, skip_frames=1):\n",
    "    input_piv_data = os.path.join(data_path, condition, subcondition, \"piv_data\", \"PIVlab_****.txt\")\n",
    "    \n",
    "    # Using a for loop instead of list comprehension\n",
    "    dfs = []\n",
    "    for file in tqdm(sorted(glob.glob(input_piv_data))[min_frame:max_frame:skip_frames], desc=f\"Loading PIV data for {condition} {subcondition}\", leave=False):\n",
    "        df = pd.read_csv(file, skiprows=2).fillna(0).rename(columns={\n",
    "            \"magnitude [m/s]\": \"velocity magnitude [m/s]\",\n",
    "            \"simple shear [1/s]\": \"shear [1/s]\",\n",
    "            \"simple strain [1/s]\": \"strain [1/s]\",\n",
    "            \"Vector type [-]\": \"data type [-]\"\n",
    "        })\n",
    "        dfs.append(df)\n",
    "\n",
    "    return dfs\n",
    "\n",
    "# Generate dataframes from PIV data with time intervals applied\n",
    "def generate_dataframes_from_piv_data(data_path, condition, subcondition, min_frame=0, max_frame=None, skip_frames=1, plot_autocorrelation=True, time_interval=1):\n",
    "    output_directory_dfs = os.path.join(data_path, condition, subcondition, \"dataframes_PIV\")\n",
    "    os.makedirs(output_directory_dfs, exist_ok=True)\n",
    "\n",
    "    # Load PIV data\n",
    "    data_frames = load_piv_data(data_path, condition, subcondition, min_frame, max_frame, skip_frames)\n",
    "\n",
    "    # Calculating mean values with valid vectors only\n",
    "    mean_values = []\n",
    "    for frame_id, data_frame in enumerate(tqdm(data_frames, desc=f\"Generating dataframes for {condition} {subcondition}\", leave=False)):\n",
    "        lambda_tau, results, fitted_values, intervector_distance_microns = correlation_length(data_frame)\n",
    "        if plot_autocorrelation:\n",
    "            plot_autocorrelation_values(data_path, condition, subcondition, frame_id, lambda_tau * 1E6, results, fitted_values, intervector_distance_microns)\n",
    "        data_frame[\"correlation length [m]\"] = lambda_tau\n",
    "        data_frame = data_frame[data_frame[\"data type [-]\"] == 1]\n",
    "        mean_values.append(data_frame.mean(axis=0))\n",
    "\n",
    "    # Creating mean DataFrame\n",
    "    mean_data_frame = pd.DataFrame(mean_values)\n",
    "    mean_data_frame.reset_index(drop=False, inplace=True)\n",
    "    mean_data_frame.rename(columns={'index': 'frame'}, inplace=True)\n",
    "\n",
    "    # Calculate power and add to DataFrame\n",
    "    volume = 2.5E-9  # µl --> m^3\n",
    "    viscosity = 1E-3  # mPa*S\n",
    "    mean_data_frame[\"power [W]\"] = volume * viscosity * (mean_data_frame[\"velocity magnitude [m/s]\"]/mean_data_frame[\"correlation length [m]\"])**2\n",
    "\n",
    "    # Scale time appropriately using the provided time_interval\n",
    "    mean_data_frame[\"time (s)\"] = mean_data_frame[\"frame\"] * time_interval\n",
    "    mean_data_frame[\"time (min)\"] = mean_data_frame[\"time (s)\"] / 60\n",
    "    mean_data_frame[\"time (h)\"] = mean_data_frame[\"time (min)\"] / 60\n",
    "\n",
    "    # Creating pivot matrices for each feature\n",
    "    features = data_frames[0].columns[:-1]\n",
    "    pivot_matrices = {feature: [] for feature in features}\n",
    "\n",
    "    for data_frame in data_frames:\n",
    "        temporary_dictionary = {feature: data_frame.pivot(index='y [m]', columns='x [m]', values=feature).values for feature in features}\n",
    "        for feature in features:\n",
    "            pivot_matrices[feature].append(temporary_dictionary[feature])\n",
    "\n",
    "    pivot_data_frame = pd.DataFrame(pivot_matrices)\n",
    "\n",
    "    # Adjusting column names in mean_data_frame\n",
    "    mean_data_frame.columns = [f\"{column}_mean\" if column not in [\"frame\", \"time (s)\", \"time (min)\", \"time (h)\"] else column for column in mean_data_frame.columns]\n",
    "    \n",
    "    # Adding time column to pivot_data_frame\n",
    "    pivot_data_frame[\"frame\"] = mean_data_frame[\"frame\"].values\n",
    "    \n",
    "    # Save DataFrames to CSV\n",
    "    mean_df_output_path = os.path.join(output_directory_dfs, \"mean_values.csv\")\n",
    "    mean_data_frame.to_csv(mean_df_output_path, index=False)\n",
    "\n",
    "    pivot_df_output_path = os.path.join(output_directory_dfs, \"features_matrices.csv\")\n",
    "    pivot_data_frame.to_csv(pivot_df_output_path, index=False)\n",
    "\n",
    "    return mean_data_frame, pivot_data_frame\n",
    "\n",
    "\n",
    "\n",
    "# Plot the PIVlab output as heatmaps\n",
    "def generate_heatmaps_from_dataframes(df, data_path, condition, subcondition, feature_limits, time_interval=3):\n",
    "    for feature, limits in feature_limits.items():\n",
    "        vmin, vmax = limits\n",
    "\n",
    "        for j in tqdm(range(len(df)), desc=f\"Generating heatmaps for {condition} {subcondition} {feature}\", leave=False):\n",
    "            vals = df.iloc[j, df.columns.get_loc(feature)]\n",
    "\n",
    "            output_directory_heatmaps = os.path.join(data_path, condition, subcondition, \"heatmaps_PIV\", f\"{feature.split()[0]}\", f\"{feature.split()[0]}_heatmap_{j}.jpg\")\n",
    "            image_files_pattern = f\"{data_path}/{condition}/{subcondition}/piv_movie_converted/converted_image_****.tif\"\n",
    "            image_files = sorted(glob.glob(image_files_pattern))[j]\n",
    "            image = Image.open(image_files)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(image, cmap=None, extent=[-2762/2, 2762/2, -2762/2, 2762/2]) # piv image\n",
    "            im = plt.imshow(vals, cmap='inferno', origin='upper', alpha=0.7, extent=[-2762/2, 2762/2, -2762/2, 2762/2], vmin=vmin, vmax=vmax) # heatmap\n",
    "            plt.xlabel('x [um]')\n",
    "            plt.ylabel('y [um]')\n",
    "            cbar = plt.colorbar(im)\n",
    "            cbar.set_label(feature)\n",
    "            time = df.iloc[j, -1]\n",
    "            plt.title(f\"PIV - {feature}  ||  time: {int(time * time_interval/60)} min -- {int(time * time_interval/3600)} hours\")\n",
    "\n",
    "            os.makedirs(os.path.dirname(output_directory_heatmaps), exist_ok=True)\n",
    "            plt.savefig(output_directory_heatmaps, format='jpg', dpi=250)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def create_movies_PIV(data_path, condition, subcondition, frame_rate, feature_limits=None, max_frame=None):\n",
    "    plots_dir = f\"{data_path}/{condition}/{subcondition}/heatmaps_PIV/\"\n",
    "    for feature in feature_limits.keys():\n",
    "        feature_name_for_file = feature.split()[0]\n",
    "        heatmap_dir = os.path.join(data_path, condition, subcondition, \"heatmaps_PIV\", f\"{feature.split()[0]}\", f\"{feature.split()[0]}_heatmap_****.jpg\")\n",
    "        image_files = natsorted(glob.glob(heatmap_dir))\n",
    "\n",
    "        if not image_files:\n",
    "            print(f\"No images found for feature {feature_name_for_file}.\")\n",
    "            continue\n",
    "\n",
    "        # Limit the number of files if max_frame is specified\n",
    "        image_files = image_files[:max_frame] if max_frame is not None else image_files\n",
    "\n",
    "        # Get the resolution of the first image (assuming all images are the same size)\n",
    "        first_image = cv2.imread(image_files[0])\n",
    "        video_resolution = (first_image.shape[1], first_image.shape[0])  # Width x Height\n",
    "\n",
    "        # Define the codec and create VideoWriter object\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "        out_path = f'{plots_dir}{feature_name_for_file}.avi'\n",
    "        out = cv2.VideoWriter(out_path, fourcc, frame_rate, video_resolution)\n",
    "\n",
    "        for file in tqdm(image_files, desc=f\"Creating movie for {condition} {subcondition} {feature}\", leave=False):\n",
    "            img = cv2.imread(file)\n",
    "            out.write(img)  # Write the image as is, without resizing\n",
    "\n",
    "        out.release()\n",
    "        print(f\"Video saved to {out_path}\")\n",
    "\n",
    "\n",
    "# Process PIV data for all conditions and subconditions, then average and save results\n",
    "def process_piv_data(data_path, conditions, subconditions, feature_limits, time_intervals, skip_frames, min_frame=0, max_frame=None, plot_autocorrelation=True, frame_rate=120, heatmaps=True):\n",
    "    for i, condition in tqdm(enumerate(conditions), desc=\"Processing PIV data\", total=len(conditions), leave=True):\n",
    "        time_interval = time_intervals[i] * skip_frames\n",
    "        results = []\n",
    "        for subcondition in tqdm(subconditions, desc=f\"Processing subconditions for {condition}\", leave=False):\n",
    "            m, p = generate_dataframes_from_piv_data(data_path, condition, subcondition, min_frame, max_frame, skip_frames, plot_autocorrelation, time_interval)\n",
    "            results.append(m)\n",
    "\n",
    "            if heatmaps == True:\n",
    "                generate_heatmaps_from_dataframes(p, data_path, condition, subcondition, feature_limits, time_interval)\n",
    "                create_movies_PIV(data_path, condition, subcondition, frame_rate, feature_limits=feature_limits, max_frame=max_frame)\n",
    "\n",
    "        # Averaging and saving the results for the current condition\n",
    "        save_path = os.path.join(data_path, condition, 'averaged')\n",
    "        average_df = sum(results) / len(results)\n",
    "        \n",
    "        os.makedirs(save_path, exist_ok=True)  # Ensure the directory exists\n",
    "        average_df.to_csv(os.path.join(save_path, f\"{condition}_average.csv\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# Generate PCA from PIVlab output\n",
    "def plot_pca(dfs, data_paths, conditions, subconditions, features):\n",
    "    # Perform PCA and Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Get colors from Seaborn's \"colorblind\" color palette\n",
    "    sns.set_palette(\"colorblind\", color_codes=True)\n",
    "    colors = sns.color_palette(\"colorblind\", n_colors=len(data_paths))\n",
    "\n",
    "    for group_index, (df, data_path, condition, subcondition) in enumerate(zip(dfs, data_paths, conditions, subconditions)):\n",
    "        pca = PCA(n_components=2)\n",
    "        principalComponents = pca.fit_transform(df.loc[:, features])\n",
    "        principalDf = pd.DataFrame(data=principalComponents, columns=['principal component 1', 'principal component 2'])\n",
    "\n",
    "        # Scaling alpha to increase with respect to the frame index\n",
    "        num_points = principalDf.shape[0]\n",
    "        alphas = np.linspace(0.001, 1, num_points)  # Alpha values linearly spaced from 1 to 0.01\n",
    "        \n",
    "        # Plotting each line segment with increasing alpha\n",
    "        for i in range(1, num_points):\n",
    "            plt.plot(principalDf['principal component 1'][i-1:i+1], principalDf['principal component 2'][i-1:i+1], \n",
    "                     alpha=alphas[i], linestyle='-', linewidth=2, color=colors[group_index])\n",
    "\n",
    "        # Plotting the points\n",
    "        plt.scatter(principalDf['principal component 1'], principalDf['principal component 2'], \n",
    "                    alpha=0.5, label=f'{condition}_{subcondition}', s=10, color=colors[group_index])\n",
    "\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('PCA of PIV Features (All Samples)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    output_dir_pca = os.path.join(data_paths[-1], conditions[-1], subconditions[-1], \"plots_PIV\", \"PCA.jpg\")\n",
    "    os.makedirs(os.path.dirname(output_dir_pca), exist_ok=True)\n",
    "    plt.savefig(output_dir_pca, format='jpg', dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_features(data_paths, conditions, subconditions, features, time_intervals, sigma=2, min_frame=None, max_frame=None):\n",
    "    dfs = []\n",
    "\n",
    "    for data_path, condition, subcondition, time_interval in zip(data_paths, conditions, subconditions, time_intervals):\n",
    "        file_path = os.path.join(data_path, condition, subcondition, \"dataframes_PIV\", \"mean_values.csv\")\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        df.iloc[:, :-3] = df.iloc[:, :-3].apply(lambda x: gaussian_filter1d(x, sigma=sigma))\n",
    "\n",
    "\n",
    "        df = df.rename(columns={\"data type [-]_mean\": \"work [J]\", \"correlation length [m]_mean\": \"correlation length [um]\", \"velocity magnitude [m/s]_mean\": \"velocity magnitude [um/s]\"})\n",
    "        df[\"work [J]\"] = df[\"power [W]_mean\"].cumsum()\n",
    "        df[\"correlation length [um]\"] = df[\"correlation length [um]\"] * 1e6\n",
    "        df[\"velocity magnitude [um/s]\"] = df[\"velocity magnitude [um/s]\"] * 1e6\n",
    "\n",
    "        # make \"power [W]_mean\" the first column\n",
    "        cols = list(df.columns)\n",
    "        # cols = [cols[-1]] + cols[:-1]\n",
    "        df = df[cols]\n",
    "\n",
    "        df = df.iloc[min_frame:max_frame, :]\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    plot_pca(dfs, data_paths, conditions, subconditions, features)\n",
    "\n",
    "    for feature in dfs[0].columns[:]:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for df, data_path, condition, subcondition, time_interval in zip(dfs, data_paths, conditions, subconditions, time_intervals):\n",
    "            output_directory_plots = os.path.join(data_path, condition, subcondition, \"plots_PIV\", f\"{feature.split()[0]}_plot.jpg\")\n",
    "            os.makedirs(os.path.dirname(output_directory_plots), exist_ok=True)\n",
    "            plt.plot(df[\"time (h)\"], df[feature], marker='o', linestyle='-', markersize=1, linewidth=1, label=f'{condition}_{subcondition}')\n",
    "\n",
    "        plt.xlabel('Time (hours)')\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(f\"PIV - {feature}\")\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        plt.legend()\n",
    "        plt.savefig(output_directory_plots, format='jpg', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Plot features and PCA averaged over subconditions\n",
    "def plot_features_averages(data_paths, conditions, subconditions, features, time_intervals, sigma=2, min_frame=None, max_frame=None):\n",
    "    dfs = []\n",
    "\n",
    "    for data_path, condition, subcondition, time_interval in zip(data_paths, conditions, subconditions, time_intervals):\n",
    "        file_path = os.path.join(data_path, condition, subcondition, f\"{condition}_average.csv\")\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        df.iloc[:, :-3] = df.iloc[:, :-3].apply(lambda x: gaussian_filter1d(x, sigma=sigma))\n",
    "\n",
    "\n",
    "        df = df.rename(columns={\"data type [-]_mean\": \"work [J]\", \"correlation length [m]_mean\": \"correlation length [um]\", \"velocity magnitude [m/s]_mean\": \"velocity magnitude [um/s]\"})\n",
    "        df[\"work [J]\"] = df[\"power [W]_mean\"].cumsum()\n",
    "        df[\"correlation length [um]\"] = df[\"correlation length [um]\"] * 1e6\n",
    "        df[\"velocity magnitude [um/s]\"] = df[\"velocity magnitude [um/s]\"] * 1e6\n",
    "\n",
    "        df = df.iloc[min_frame:max_frame, :]\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    plot_pca(dfs, data_paths, conditions, subconditions, features)\n",
    "\n",
    "    for feature in dfs[0].columns[:]:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for df, data_path, condition, subcondition, time_interval in zip(dfs, data_paths, conditions, subconditions, time_intervals):\n",
    "            output_directory_plots = os.path.join(data_path, condition, subcondition, \"plots_PIV\", f\"{feature.split()[0]}_plot.jpg\")\n",
    "            os.makedirs(os.path.dirname(output_directory_plots), exist_ok=True)\n",
    "            plt.plot(df[\"time (h)\"], df[feature], marker='o', linestyle='-', markersize=1, linewidth=1, label=f'{condition}_{subcondition}')\n",
    "\n",
    "        plt.xlabel('Time (hours)')\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(f\"PIV - {feature}\")\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        plt.legend()\n",
    "        plt.savefig(output_directory_plots, format='jpg', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_PIV_features(data_path, conditions, subconditions, features_pca, time_intervals, sigma=10, min_frame=0, max_frame=None, averages=True):\n",
    "    # Plot features for individual subconditions\n",
    "    for condition in tqdm(conditions, desc=\"Plotting PIV features\", leave=True):\n",
    "        data_paths = [data_path] * len(subconditions)\n",
    "        condition_list = [condition] * len(subconditions)\n",
    "        plot_features(\n",
    "            data_paths,\n",
    "            condition_list,\n",
    "            subconditions,\n",
    "            features_pca,\n",
    "            time_intervals=[time_intervals[conditions.index(condition)]] * len(subconditions),\n",
    "            sigma=sigma,\n",
    "            min_frame=min_frame,\n",
    "            max_frame=max_frame,\n",
    "        )\n",
    "\n",
    "    # Plot features for all subconditions together\n",
    "    data_paths = [data_path] * len(conditions) * len(subconditions)\n",
    "    condition_list = [condition for condition in conditions for _ in range(len(subconditions))]\n",
    "    subcondition_list = subconditions * len(conditions)\n",
    "    time_interval_list = [time_interval for time_interval in time_intervals for _ in range(len(subconditions))]\n",
    "\n",
    "    plot_features(\n",
    "        data_paths,\n",
    "        condition_list,\n",
    "        subcondition_list,\n",
    "        features_pca,\n",
    "        time_intervals=time_interval_list,\n",
    "        sigma=sigma,\n",
    "        min_frame=min_frame,\n",
    "        max_frame=max_frame,\n",
    "    )\n",
    "\n",
    "    if averages == True:\n",
    "        # Plot features for averaged subconditions\n",
    "        data_paths = [data_path] * len(conditions)\n",
    "        subcondition_list = ['averaged'] * len(conditions)\n",
    "        time_interval_list = time_intervals\n",
    "\n",
    "        plot_features_averages(\n",
    "            data_paths,\n",
    "            conditions,\n",
    "            subcondition_list,\n",
    "            features_pca,\n",
    "            time_intervals=time_interval_list,\n",
    "            sigma=sigma,\n",
    "            min_frame=min_frame,\n",
    "            max_frame=max_frame,\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions: ['AcSu', 'AcSu2']\n",
      "Subconditions: ['Rep1']\n",
      "Moved .tif files from ../../data/080624-acsu_acsu2_piv_RT/2txtl_0p5mt_1dna_1/AcSu/Rep1 to ../../data/080624-acsu_acsu2_piv_RT/2txtl_0p5mt_1dna_1/AcSu/Rep1/original\n",
      "Moved .tif files from ../../data/080624-acsu_acsu2_piv_RT/2txtl_0p5mt_1dna_1/AcSu2/Rep1 to ../../data/080624-acsu_acsu2_piv_RT/2txtl_0p5mt_1dna_1/AcSu2/Rep1/original\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "calibration_curve_paths = sorted(glob.glob(\"../../data/calibration_curve/***ugml.tif\"))\n",
    "\n",
    "data_path = \"../../data/080624-acsu_acsu2_piv_RT/2txtl_0p5mt_1dna_1/\"\n",
    "conditions, subconditions = prepare_conditions(data_path, 1)\n",
    "\n",
    "print(\"Conditions:\", conditions)\n",
    "print(\"Subconditions:\", subconditions)\n",
    "reorgTiffsToOriginal(data_path, conditions, subconditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied every 2th 'cy5' and 'gfp' file from ../../data/080624-acsu_acsu2_piv_RT/2txtl_0p5mt_1dna_1/AcSu/Rep1/original into separate folders.\n",
      "Copied every 2th 'cy5' and 'gfp' file from ../../data/080624-acsu_acsu2_piv_RT/2txtl_0p5mt_1dna_1/AcSu2/Rep1/original into separate folders.\n"
     ]
    }
   ],
   "source": [
    "reorgTiffs_Split_cy5_gfp(data_path, conditions, subconditions, file_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature limits and other parameters\n",
    "v = 1E-6\n",
    "velocity_limits = (0, v)\n",
    "other_limits = (-0.0005, 0.0005)\n",
    "\n",
    "\n",
    "# velocity_limits = (None, None)\n",
    "# other_limits = (None, None)\n",
    "\n",
    "\n",
    "feature_limits = {\n",
    "    'u [m/s]': (-v, v), \n",
    "    'v [m/s]': (-v, v), \n",
    "    'data type [-]': (None, None),\n",
    "    'velocity magnitude [m/s]': velocity_limits,\n",
    "    'vorticity [1/s]': other_limits,\n",
    "    'divergence [1/s]': other_limits,\n",
    "    'dcev [1]': (0, 250),\n",
    "    'shear [1/s]': other_limits,\n",
    "    'strain [1/s]': other_limits,\n",
    "    'vector direction [degrees]': (-180, 180),\n",
    "}\n",
    "\n",
    "skip_frames = 16 ### CHANGE THIS TO SKIP FRAMES\n",
    "\n",
    "\n",
    "# Convert images to the right size\n",
    "convert_images(data_path, conditions, subconditions, max_frame=None, brightness_factor=1, contrast_factor=1, skip_frames=skip_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time interval list adjusted for skip_frames\n",
    "time_interval_list = [3, 60]  # time intervals in seconds between frames for each condition\n",
    "\n",
    "# Process PIV data\n",
    "process_piv_data(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    feature_limits, \n",
    "    time_interval_list, \n",
    "    min_frame=0, \n",
    "    max_frame=None, \n",
    "    skip_frames=skip_frames, \n",
    "    plot_autocorrelation=True, \n",
    "    frame_rate=1, \n",
    "    heatmaps=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for PCA and plotting\n",
    "features_pca = [\n",
    "    \"vorticity [1/s]_mean\",\n",
    "    \"velocity magnitude [um/s]\",\n",
    "    \"divergence [1/s]_mean\",\n",
    "    \"shear [1/s]_mean\",\n",
    "    \"strain [1/s]_mean\",\n",
    "    \"correlation length [um]\", \n",
    "    \"power [W]_mean\",\n",
    "    \"work [J]\",\n",
    "]\n",
    "\n",
    "# Plot features and PCA\n",
    "plot_PIV_features(\n",
    "    data_path, \n",
    "    conditions, \n",
    "    subconditions, \n",
    "    features_pca, \n",
    "    time_interval_list, \n",
    "    min_frame=0, \n",
    "    max_frame=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def delete_outputs(data_path, conditions, subconditions, output_dirs=None):\n",
    "    \"\"\"\n",
    "    Deletes all output files and directories for the given conditions and subconditions.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Base directory for PIV data and output.\n",
    "        conditions (list): List of conditions.\n",
    "        subconditions (list): List of subconditions.\n",
    "        output_dirs (list, optional): Specific output directories to delete. If None, delete all known output directories.\n",
    "    \"\"\"\n",
    "    # Default output directories to remove\n",
    "    if output_dirs is None:\n",
    "        output_dirs = [\n",
    "            \"piv_movie_converted\",\n",
    "            \"autocorrelation_plots\",\n",
    "            \"dataframes_PIV\",\n",
    "            \"heatmaps_PIV\",\n",
    "            \"plots_PIV\",\n",
    "            \"averaged\",\n",
    "        ]\n",
    "\n",
    "    for condition in conditions:\n",
    "        for subcondition in subconditions:\n",
    "            for output_dir in output_dirs:\n",
    "                dir_path = os.path.join(data_path, condition, subcondition, output_dir)\n",
    "                if os.path.exists(dir_path):\n",
    "                    try:\n",
    "                        shutil.rmtree(dir_path)\n",
    "                        print(f\"Deleted directory: {dir_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error deleting directory {dir_path}: {e}\")\n",
    "\n",
    "        # Remove the averaged directory at the condition level\n",
    "        averaged_dir = os.path.join(data_path, condition, \"averaged\")\n",
    "        if os.path.exists(averaged_dir):\n",
    "            try:\n",
    "                shutil.rmtree(averaged_dir)\n",
    "                print(f\"Deleted directory: {averaged_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting directory {averaged_dir}: {e}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "delete_outputs(data_path, conditions, subconditions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
